{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# data processing packages\n",
    "import numpy as np   \n",
    "import pandas as pd \n",
    "from pandas import *\n",
    "from numpy import *\n",
    "\n",
    "from scipy import stats # look at scipy\n",
    "from scipy import linalg\n",
    "from scipy import *\n",
    " \n",
    "%matplotlib inline    \n",
    "import matplotlib as mplt\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.style.use('ggplot')\n",
    "import itertools\n",
    "\n",
    "import random\n",
    "\n",
    "# machine leanring packages\n",
    "\n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation, TimeDistributedDense, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "# RMSprop, Adadelta\n",
    "from keras.regularizers import l2, activity_l2\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6938, 186) (771, 186) (6938, 1) (771, 1)\n",
      "(6938, 186) (6938, 1)\n",
      "(771, 186) (771, 1)\n"
     ]
    }
   ],
   "source": [
    "# ------------data prepro-----------------------\n",
    "\n",
    "files_list=[\"../dataset/dataset_demo/xtrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/xtest.csv\",\\\n",
    "            \"../dataset/dataset_demo/ytrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/ytest.csv\"]\n",
    "\n",
    "xtrain_df=pd.read_csv( files_list[0] ,sep=',', header=None)\n",
    "xtest_df=pd.read_csv( files_list[1] ,sep=',', header=None)\n",
    "ytrain_df=pd.read_csv( files_list[2] ,sep=',', header=None)\n",
    "ytest_df=pd.read_csv( files_list[3] ,sep=',', header=None)\n",
    "\n",
    "print xtrain_df.shape, xtest_df.shape, ytrain_df.shape, ytest_df.shape\n",
    "\n",
    "\n",
    "# get training and testing data prepared\n",
    "\n",
    "# cxtest = conti_normalization_test_dta(cxtest, cxtrain)\n",
    "# cxtrain = conti_normalization_train_dta(cxtrain)\n",
    "\n",
    "print xtrain_df.shape, ytrain_df.shape\n",
    "print xtest_df.shape,  ytest_df.shape\n",
    "\n",
    "# cxtrain = cxtrain.as_matrix() \n",
    "xtrain = xtrain_df.as_matrix()\n",
    "xtest  = xtest_df.as_matrix()\n",
    "\n",
    "ytrain = ytrain_df.as_matrix()\n",
    "ytest  = ytest_df.as_matrix()\n",
    "\n",
    "# ytest = np_utils.to_categorical( ytest_df.as_matrix() )\n",
    "# ytrain =np_utils.to_categorical( ytrain_df.as_matrix() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crossv_rf_num_estimatior(maxnum, X,Y):\n",
    "    score_list=[]\n",
    "    tmpscore=0\n",
    "    tmpnum=0\n",
    "    \n",
    "    tmpy = Y.reshape( (len(Y),) )\n",
    "    \n",
    "    for i in range(1,maxnum+1,10):\n",
    "        clf = RandomForestClassifier(n_estimators=i,max_depth=3)\n",
    "        scores = cross_val_score(clf, X, tmpy)\n",
    "        score_list.append(scores)\n",
    "    \n",
    "        if scores.mean()>tmpscore:\n",
    "            tmpscore=scores.mean()\n",
    "            tmpnum=i\n",
    "    return tmpnum, score_list\n",
    "\n",
    "def crossv_gbt_num_estimatior(maxnum,X,Y):\n",
    "    score_list=[]\n",
    "    tmpscore=0\n",
    "    tmpnum=0\n",
    "    \n",
    "    tmpy = Y.reshape( (len(Y),) )\n",
    "    \n",
    "    for i in range(1,maxnum+1,10):\n",
    "        clf = GradientBoostingClassifier(n_estimators=i,learning_rate=0.1)\n",
    "        scores = cross_val_score(clf, X, tmpy)\n",
    "        score_list.append(scores)\n",
    "    \n",
    "        if scores.mean()>tmpscore:\n",
    "            tmpscore=scores.mean()\n",
    "            tmpnum=i\n",
    "    return tmpnum, score_list\n",
    "\n",
    "def rf_num_estimatior(maxnum, X,Y, xtest, ytest):\n",
    "    \n",
    "    tmpy = Y.reshape( (len(Y),) )\n",
    "    score = []\n",
    "    for i in range(10,maxnum+1,10):\n",
    "        clf = RandomForestClassifier(n_estimators=i,max_depth=3)\n",
    "        \n",
    "        clf.fit( X, tmpy )\n",
    "        score.append( clf.score(xtest, ytest))\n",
    "            \n",
    "    return score\n",
    "\n",
    "def gbt_num_estimatior(maxnum,X,Y, xtest, ytest):\n",
    "    \n",
    "    tmpy = Y.reshape( (len(Y),) )\n",
    "    score = []\n",
    "    for i in range(10,maxnum+1,10):\n",
    "        clf = GradientBoostingClassifier(n_estimators=i,learning_rate=0.1)\n",
    "        \n",
    "        clf.fit( X, tmpy )\n",
    "        score.append( clf.score(xtest, ytest))\n",
    "    \n",
    "    return score\n",
    "\n",
    "def xgt_num_estimator( maxdepth, rounds ,X,Y, xtest, ytest):\n",
    "    \n",
    "    score = []\n",
    "    xg_train = xgb.DMatrix(xtrain, label = ytrain)\n",
    "    xg_test  = xgb.DMatrix(xtest,  label = ytest)\n",
    "\n",
    "# setup parameters for xgboost\n",
    "    param = {}\n",
    "# use softmax multi-class classification\n",
    "    param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 0\n",
    "    param['silent'] = 1\n",
    "    param['nthread'] = 8\n",
    "    param['num_class'] = 8\n",
    "    \n",
    "    for i in range(1, maxdepth):\n",
    "        for num_round in range(1, rounds):\n",
    "            \n",
    "            param['max_depth'] = i\n",
    "            bst = xgb.train(param, xg_train, num_round )\n",
    "            \n",
    "            pred = bst.predict( xg_test )\n",
    "            tmp_accur =  sum( int(pred[i]) == ytest[i] \\\n",
    "                             for i in range(len(ytest))) / float(len(ytest)) \n",
    "            \n",
    "            score.append( (i,num_round,tmp_accur) )\n",
    "            \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def result_plot( accu_list, feature_name, maxnum, fig_title, filename ):\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    figsize=( 15.4,7)\n",
    "    font_size=15\n",
    "    fig.set_size_inches( figsize )\n",
    "    matplotlib.rcParams.update({'font.size': font_size})\n",
    "\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0.2,0.35])\n",
    "    \n",
    "    x= range(1,maxnum+1,10)\n",
    "    \n",
    "    for i in range(len(accu_list)):\n",
    "        \n",
    "        tmp=[]\n",
    "        for j in range(len(accu_list[i])):\n",
    "            tmp.append( accu_list[i][j].mean() )\n",
    "        \n",
    "        plt.plot( x, tmp ,label=feature_name[i] )\n",
    "\n",
    "    plt.title(fig_title)\n",
    "    plt.ylabel('Accuray on validation datasets')\n",
    "    plt.xlabel('# of tress or iterations')\n",
    "    # plt.legend( loc='upper left',fontsize=12 )\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "#     fig.savefig('./results/'+filename+'.jpg', format='jpg', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest best validation accuracy: 0.306095979248\n"
     ]
    }
   ],
   "source": [
    "rf_accu = rf_num_estimatior(150,xtrain,ytrain,xtest,ytest)\n",
    "print \"Random forest best validation accuracy:\", max(rf_accu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbt_accu = gbt_num_estimatior(150, xtrain,ytrain, xtest, ytest)\n",
    "print \"Gradient boosted tree best validation accuracy:\", max(gbt_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gradient boosted tree\n",
    "0.315175097276"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGT best validation accuracy: (3, 8, array([ 0.32944228]))\n"
     ]
    }
   ],
   "source": [
    "xgt_accu = xgt_num_estimator(10, 20, xtrain,ytrain, xtest, ytest)\n",
    "print \"XGT best validation accuracy:\", max(xgt_accu, key = lambda item:item[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.613722\ttest-merror:0.721141\n",
      "[1]\ttrain-merror:0.579274\ttest-merror:0.709468\n",
      "[2]\ttrain-merror:0.561545\ttest-merror:0.709468\n",
      "[3]\ttrain-merror:0.545835\ttest-merror:0.692607\n",
      "[4]\ttrain-merror:0.536033\ttest-merror:0.687419\n",
      "[5]\ttrain-merror:0.531421\ttest-merror:0.684825\n",
      "[6]\ttrain-merror:0.518737\ttest-merror:0.677043\n",
      "[7]\ttrain-merror:0.515711\ttest-merror:0.67834\n",
      "[8]\ttrain-merror:0.509225\ttest-merror:0.688716\n",
      "[9]\ttrain-merror:0.501009\ttest-merror:0.693904\n",
      "[10]\ttrain-merror:0.496541\ttest-merror:0.684825\n",
      "[11]\ttrain-merror:0.492793\ttest-merror:0.67834\n",
      "[12]\ttrain-merror:0.487172\ttest-merror:0.688716\n",
      "[13]\ttrain-merror:0.479533\ttest-merror:0.688716\n",
      "[14]\ttrain-merror:0.472326\ttest-merror:0.683528\n",
      "[15]\ttrain-merror:0.466129\ttest-merror:0.690013\n",
      "[16]\ttrain-merror:0.460651\ttest-merror:0.692607\n",
      "[17]\ttrain-merror:0.454742\ttest-merror:0.692607\n",
      "[18]\ttrain-merror:0.452292\ttest-merror:0.69131\n",
      "[19]\ttrain-merror:0.445662\ttest-merror:0.696498\n",
      "[20]\ttrain-merror:0.442491\ttest-merror:0.70428\n",
      "[21]\ttrain-merror:0.43932\ttest-merror:0.70428\n",
      "[22]\ttrain-merror:0.433554\ttest-merror:0.700389\n",
      "[23]\ttrain-merror:0.42923\ttest-merror:0.701686\n",
      "[24]\ttrain-merror:0.424186\ttest-merror:0.693904\n",
      "[25]\ttrain-merror:0.413664\ttest-merror:0.690013\n",
      "[26]\ttrain-merror:0.410493\ttest-merror:0.69131\n",
      "[27]\ttrain-merror:0.407034\ttest-merror:0.687419\n",
      "[28]\ttrain-merror:0.402277\ttest-merror:0.683528\n",
      "[29]\ttrain-merror:0.396944\ttest-merror:0.684825\n",
      "[30]\ttrain-merror:0.394782\ttest-merror:0.690013\n",
      "[31]\ttrain-merror:0.3919\ttest-merror:0.688716\n",
      "[32]\ttrain-merror:0.389017\ttest-merror:0.688716\n",
      "[33]\ttrain-merror:0.387143\ttest-merror:0.687419\n",
      "[34]\ttrain-merror:0.387287\ttest-merror:0.692607\n",
      "predicting, classification accuracy=0.307393\n"
     ]
    }
   ],
   "source": [
    "# backup code\n",
    "import xgboost as xgb\n",
    "# read in data\n",
    "xg_train = xgb.DMatrix(xtrain, label = ytrain)\n",
    "xg_test  = xgb.DMatrix(xtest,  label = ytest)\n",
    "\n",
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 5\n",
    "param['silent'] = 1\n",
    "param['nthread'] = 4\n",
    "param['num_class'] = 8\n",
    "\n",
    "watchlist = [ (xg_train,'train'), (xg_test, 'test') ]\n",
    "num_round = 35\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist );\n",
    "# get prediction\n",
    "pred = bst.predict( xg_test );\n",
    "\n",
    "\n",
    "print ('predicting, classification accuracy=%f' % \\\n",
    "       (sum( int(pred[i]) == ytest[i] for i in range(len(ytest))) / float(len(ytest)) ))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
