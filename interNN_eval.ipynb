{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# batch normalization   https://r2rt.com/implementing-batch-normalization-in-tensorflow.html\n",
    "# batch norm specific for training and testing\n",
    "\n",
    "# https://gist.github.com/tomokishii/0ce3bdac1588b5cca9fa5fbdf6e1c412\n",
    "# weight normalization\n",
    "\n",
    "# optimize the efficiency of codes \n",
    "\n",
    "# start with low regularization, large loss\n",
    "#  more parameters, high learning rate\n",
    "\n",
    "# reguliarization slows down the convergence rate\n",
    "\n",
    "\n",
    "# TO DO:\n",
    "# observe regulairzation and objective losses individually\n",
    "# orthogonal ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTES:\n",
    "\n",
    "# ---Ini\n",
    "\n",
    "# Xavier 1/n_in\n",
    "# He's 2/n_in\n",
    "# Orthogonal\n",
    "\n",
    "# ---Activation\n",
    "\n",
    "\n",
    "# ---Optimizer\n",
    "\n",
    "# Adam\n",
    "# Adadelta\n",
    "\n",
    "# ---Regularization\n",
    "# dropout + max norm\n",
    "# batch normalization\n",
    "# weight normalization\n",
    "\n",
    "# regularization:  closeness of traning and validation performance, training stableness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from six.moves import urllib\n",
    "# from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from interNN import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# continuous and categorical feature split\n",
    "\n",
    "# features on columns [0:split_idx) are continuous\n",
    "# the rest are categorical\n",
    "def conti_cate_split(dta_df, split_idx):\n",
    "    \n",
    "    col_cnt= dta_df.shape[1]\n",
    "    conti_cols= range(split_idx)\n",
    "    dis_cols=range(split_idx, col_cnt)\n",
    "    \n",
    "    return dta_df[conti_cols], dta_df[dis_cols]\n",
    "\n",
    "def conti_normalization_train_dta(dta_df):\n",
    "    \n",
    "    return preprocessing.scale(dta_df)\n",
    "\n",
    "def conti_normalization_test_dta(dta_df, train_df):\n",
    "    \n",
    "    mean_dim = np.mean(train_df, axis=0)\n",
    "    std_dim = np.std(train_df, axis=0)\n",
    "        \n",
    "    df=pd.DataFrame()\n",
    "    cols = train_df.columns\n",
    "    idx=0\n",
    "    \n",
    "    for i in cols:\n",
    "        df[i] = (dta_df[i]- mean_dim[idx])*1.0/std_dim[idx]\n",
    "        idx=idx+1\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6938, 106) (6938, 80) (6938, 8)\n",
      "(771, 106) (771, 80) (771, 8)\n"
     ]
    }
   ],
   "source": [
    "# Demographic data\n",
    "# ------------data prepro-----------------------\n",
    "\n",
    "files_list=[\"../dataset/dataset_demo/xtrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/xtest.csv\",\\\n",
    "            \"../dataset/dataset_demo/ytrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/ytest.csv\"]\n",
    "\n",
    "xtrain_df=pd.read_csv( files_list[0] ,sep=',', header=None)\n",
    "xtest_df=pd.read_csv( files_list[1] ,sep=',', header=None)\n",
    "ytrain_df=pd.read_csv( files_list[2] ,sep=',', header=None)\n",
    "ytest_df=pd.read_csv( files_list[3] ,sep=',', header=None)\n",
    "\n",
    "\n",
    "# get training and testing data prepared\n",
    "\n",
    "cxtrain, dxtrain = conti_cate_split(xtrain_df, 106)\n",
    "cxtest, dxtest = conti_cate_split(xtest_df, 106)\n",
    "\n",
    "cxtest = conti_normalization_test_dta(cxtest, cxtrain)\n",
    "cxtrain = conti_normalization_train_dta(cxtrain)\n",
    "\n",
    "# cxtrain = cxtrain.as_matrix() \n",
    "dxtrain = dxtrain.as_matrix().astype(int)\n",
    "cxtest = cxtest.as_matrix()\n",
    "dxtest = dxtest.as_matrix().astype(int)\n",
    "\n",
    "ytest = np_utils.to_categorical( ytest_df.as_matrix() )\n",
    "ytrain =np_utils.to_categorical( ytrain_df.as_matrix() )\n",
    "\n",
    "\n",
    "print np.shape(cxtrain), np.shape(dxtrain), np.shape(ytrain)\n",
    "print np.shape(cxtest), np.shape(dxtest), np.shape(ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6938, 186) (771, 186) (6938, 1) (771, 1)\n",
      "(6938, 20) (6938, 166)\n",
      "(771, 20) (771, 166)\n"
     ]
    }
   ],
   "source": [
    "# Default data\n",
    "# ------------data prepro-----------------------\n",
    "\n",
    "files_list=[\"../dataset/dataset_demo/default_xtrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/default_xtest.csv\",\\\n",
    "            \"../dataset/dataset_demo/default_ytrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/default_ytest.csv\"]\n",
    "\n",
    "xtrain_df=pd.read_csv( files_list[0] ,sep=',', header=None)\n",
    "xtest_df=pd.read_csv( files_list[1] ,sep=',', header=None)\n",
    "ytrain_df=pd.read_csv( files_list[2] ,sep=',', header=None)\n",
    "ytest_df=pd.read_csv( files_list[3] ,sep=',', header=None)\n",
    "\n",
    "# get training and testing data prepared\n",
    "\n",
    "cxtrain, dxtrain = conti_cate_split(xtrain_df, 20)\n",
    "cxtest, dxtest = conti_cate_split(xtest_df, 20)\n",
    "\n",
    "cxtest = conti_normalization_test_dta(cxtest, cxtrain)\n",
    "cxtrain = conti_normalization_train_dta(cxtrain)\n",
    "\n",
    "# cxtrain = cxtrain.as_matrix() \n",
    "dxtrain = dxtrain.as_matrix().astype(int)\n",
    "cxtest = cxtest.as_matrix()\n",
    "dxtest = dxtest.as_matrix().astype(int)\n",
    "\n",
    "ytest = np_utils.to_categorical( ytest_df.as_matrix() )\n",
    "ytrain =np_utils.to_categorical( ytrain_df.as_matrix() )\n",
    "\n",
    "print np.shape(cxtrain), np.shape(dxtrain), np.shape(ytrain)\n",
    "print np.shape(cxtest), np.shape(dxtest), np.shape(ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wide, embedding, co-occurrence and feature interaction \n",
    "# version 1: interaction as external hiddens\n",
    "\n",
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "\n",
    "#   representation ability\n",
    "para_n_hidden_list = [ 32,32,16 ]\n",
    "#     128,16,8 ]\n",
    "para_lr = 0.045\n",
    "para_n_embedding = 2\n",
    "#   regularization\n",
    "para_batch_size = 32\n",
    "para_keep_prob = 1.0\n",
    "para_l2 = 0.01\n",
    "\n",
    "# fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = [8]*para_n_disc\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 10\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    clf = wide_embed_coocc_NN( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                      para_n_disc_voca, sess, para_n_embedding, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2)\n",
    "    clf.train_ini()\n",
    "    clf.inference_ini()\n",
    "    \n",
    "    para_cur_lr=para_lr\n",
    "    \n",
    "    total_cnt= np.shape(cxtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "    \n",
    "    total_idx=range(total_cnt)\n",
    "    \n",
    "    \n",
    "#   training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc=0.0\n",
    "        \n",
    "#       shuffle traning instances each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "        \n",
    "#       Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                     para_cur_lr )\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "        \n",
    "        tmp_test_acc  = clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "        \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wide, embedding, co-occurrence and feature interaction \n",
    "# version 2: individual hidden layers on embedding and interaction\n",
    "\n",
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "\n",
    "#   representation ability\n",
    "para_n_hidden_list = [ 128,64,32,16 ]\n",
    "#     128,16,8 ]\n",
    "para_lr = 0.005\n",
    "para_n_embedding = 2\n",
    "#   regularization\n",
    "para_batch_size = 32\n",
    "para_keep_prob = 1.0\n",
    "para_l2 = 0.01\n",
    "\n",
    "# fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = [8]*para_n_disc\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 10\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    clf = InterNN_IndiH( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                      para_n_disc_voca, sess, para_n_embedding, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2, para_batch_size)\n",
    "    clf.train_ini()\n",
    "    clf.inference_ini()\n",
    "    \n",
    "    para_cur_lr=para_lr\n",
    "    \n",
    "    total_cnt= np.shape(cxtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "    \n",
    "    total_idx=range(total_cnt)\n",
    "    \n",
    "    \n",
    "#   training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc=0.0\n",
    "        \n",
    "#       shuffle traning instances each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "        \n",
    "#       Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                     para_cur_lr )\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "        \n",
    "        tmp_test_acc  = clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "        \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch  0  :  1358.97594763 testing: [0.1919585, 3.5016201, 1292.3411] training: [0.21706544, 3.3362319, 1292.3411]\n",
      "loss on epoch  5  :  881.529536495 testing: [0.26848248, 1.8457818, 841.95215] training: [0.27716923, 1.8004457, 841.95215]\n",
      "loss on epoch  10  :  582.366710522 testing: [0.27237353, 1.7725637, 556.17255] training: [0.28524071, 1.7447113, 556.17255]\n",
      "loss on epoch  15  :  386.957800971 testing: [0.27107653, 1.7548887, 369.15247] training: [0.30412224, 1.6888459, 369.15247]\n",
      "loss on epoch  20  :  257.798899898 testing: [0.21789883, 1.7894963, 245.42532] training: [0.28062841, 1.716526, 245.42532]\n",
      "loss on epoch  25  :  171.962983308 testing: [0.28664073, 1.6942844, 163.18777] training: [0.31421158, 1.6512724, 163.18777]\n",
      "loss on epoch  30  :  114.826952051 testing: [0.27367055, 1.7005256, 108.43096] training: [0.31349093, 1.6634634, 108.43096]\n",
      "loss on epoch  35  :  76.7746836344 testing: [0.2775616, 1.6957891, 71.969795] training: [0.31550878, 1.6374689, 71.969795]\n",
      "loss on epoch  40  :  51.4546924167 testing: [0.32036316, 1.6633865, 47.719517] training: [0.32314789, 1.6291095, 47.719517]\n",
      "loss on epoch  45  :  34.6477168048 testing: [0.28015563, 1.6772599, 31.618109] training: [0.31695014, 1.6253518, 31.618109]\n",
      "loss on epoch  50  :  23.5045101731 testing: [0.29053178, 1.6741244, 20.957914] training: [0.3253099, 1.6233459, 20.957914]\n",
      "loss on epoch  55  :  16.1468129335 testing: [0.28793773, 1.6674168, 13.914087] training: [0.31651774, 1.6135546, 13.914087]\n",
      "loss on epoch  60  :  11.298211239 testing: [0.27496758, 1.6558578, 9.2752295] training: [0.32559815, 1.6058924, 9.2752295]\n",
      "loss on epoch  65  :  8.10561158922 testing: [0.28274968, 1.6622744, 6.2257199] training: [0.33756125, 1.5955356, 6.2257199]\n",
      "loss on epoch  70  :  5.9956712193 testing: [0.29312581, 1.6735188, 4.2211504] training: [0.32819256, 1.5847284, 4.2211504]\n",
      "loss on epoch  75  :  4.5937670955 testing: [0.30090791, 1.686422, 2.9136178] training: [0.35471317, 1.5339041, 2.9136178]\n",
      "loss on epoch  80  :  3.67553670318 testing: [0.29312581, 1.7228918, 2.0685298] training: [0.36984721, 1.48285, 2.0685298]\n",
      "loss on epoch  85  :  3.04090839845 testing: [0.27626458, 1.77627, 1.521153] training: [0.38916114, 1.4562055, 1.521153]\n",
      "loss on epoch  90  :  2.6369970198 testing: [0.27885863, 1.7866043, 1.1596092] training: [0.43240127, 1.3984789, 1.1596092]\n",
      "loss on epoch  95  :  2.31569142695 testing: [0.28534371, 1.8110874, 0.91200703] training: [0.44782358, 1.3603833, 0.91200703]\n",
      "loss on epoch  100  :  2.13612954705 testing: [0.27367055, 1.8363853, 0.74209684] training: [0.47347939, 1.3201154, 0.74209684]\n",
      "loss on epoch  105  :  1.9792526563 testing: [0.28664073, 1.8954098, 0.62812424] training: [0.47708273, 1.3048874, 0.62812424]\n",
      "loss on epoch  110  :  1.90129739267 testing: [0.28404668, 1.9015998, 0.54477948] training: [0.48443356, 1.2777711, 0.54477948]\n",
      "loss on epoch  115  :  1.79074937326 testing: [0.28664073, 1.9214118, 0.47318909] training: [0.49495533, 1.2638323, 0.47318909]\n",
      "loss on epoch  120  :  1.72756271892 testing: [0.29053178, 1.9280061, 0.42466423] training: [0.50129718, 1.2572137, 0.42466423]\n",
      "loss on epoch  125  :  1.6753470412 testing: [0.26070037, 1.9786859, 0.38628602] training: [0.51412511, 1.2341452, 0.38628602]\n",
      "loss on epoch  130  :  1.63605839676 testing: [0.27885863, 1.9725989, 0.36330569] training: [0.51210725, 1.2274435, 0.36330569]\n",
      "loss on epoch  135  :  1.61199701715 testing: [0.28015563, 1.9801083, 0.3421326] training: [0.50850391, 1.2169116, 0.3421326]\n",
      "loss on epoch  140  :  1.59999675221 testing: [0.26588845, 1.9893003, 0.32071677] training: [0.51945806, 1.2397587, 0.32071677]\n",
      "loss on epoch  145  :  1.55386609943 testing: [0.25551233, 2.0579844, 0.30227086] training: [0.50821561, 1.2432495, 0.30227086]\n",
      "loss on epoch  150  :  1.53471202762 testing: [0.25551233, 2.0032816, 0.29161665] training: [0.51225138, 1.2331886, 0.29161665]\n",
      "loss on epoch  155  :  1.53054167606 testing: [0.28664073, 2.024894, 0.28118321] training: [0.51383686, 1.2063719, 0.28118321]\n",
      "loss on epoch  160  :  1.51997589182 testing: [0.27496758, 2.0441785, 0.27709994] training: [0.52666473, 1.2176754, 0.27709994]\n",
      "loss on epoch  165  :  1.5141688806 testing: [0.26848248, 2.0276663, 0.27407312] training: [0.52652061, 1.1968846, 0.27407312]\n",
      "loss on epoch  170  :  1.49422285733 testing: [0.28404668, 2.0421801, 0.26702714] training: [0.5239262, 1.1993433, 0.26702714]\n",
      "loss on epoch  175  :  1.49403632129 testing: [0.26718548, 2.0696657, 0.27173534] training: [0.52925915, 1.1929297, 0.27173534]\n",
      "loss on epoch  180  :  1.511766809 testing: [0.27367055, 2.0875587, 0.27158546] training: [0.50201786, 1.2142187, 0.27158546]\n",
      "loss on epoch  185  :  1.53806155699 testing: [0.26070037, 2.043761, 0.27038023] training: [0.51671952, 1.2180839, 0.27038023]\n",
      "loss on epoch  190  :  1.48582400216 testing: [0.26459143, 2.0876951, 0.26220492] training: [0.55534738, 1.1602606, 0.26220492]\n",
      "loss on epoch  195  :  1.48254978657 testing: [0.25551233, 2.107204, 0.26554507] training: [0.53689826, 1.1775973, 0.26554507]\n",
      "loss on epoch  200  :  1.49771312873 testing: [0.27496758, 2.0477028, 0.2661936] training: [0.53747475, 1.1822572, 0.2661936]\n",
      "loss on epoch  205  :  1.51444290744 testing: [0.25551233, 2.1656771, 0.27322811] training: [0.50936872, 1.2158041, 0.27322811]\n",
      "loss on epoch  210  :  1.48648478808 testing: [0.26329443, 2.1626205, 0.27117702] training: [0.52825022, 1.2035416, 0.27117702]\n",
      "loss on epoch  215  :  1.50212449939 testing: [0.27496758, 2.0895824, 0.27203718] training: [0.52738541, 1.2020061, 0.27203718]\n",
      "loss on epoch  220  :  1.52027821099 testing: [0.28404668, 1.9958776, 0.25933433] training: [0.5354569, 1.1908466, 0.25933433]\n",
      "loss on epoch  225  :  1.46374849478 testing: [0.27107653, 2.1788363, 0.26484385] training: [0.52248484, 1.1924313, 0.26484385]\n",
      "loss on epoch  230  :  1.47251974212 testing: [0.27885863, 2.1440175, 0.26920447] training: [0.55664456, 1.1647862, 0.26920447]\n",
      "loss on epoch  235  :  1.48824202131 testing: [0.29701686, 2.0760224, 0.2700465] training: [0.54251945, 1.1752274, 0.2700465]\n",
      "loss on epoch  240  :  1.45730245996 testing: [0.25810635, 2.1664288, 0.26826236] training: [0.54107815, 1.1590054, 0.26826236]\n",
      "loss on epoch  245  :  1.44848454882 testing: [0.27367055, 2.1477563, 0.26459011] training: [0.56586915, 1.1300794, 0.26459011]\n",
      "loss on epoch  250  :  1.50965035845 testing: [0.28015563, 2.0622561, 0.27569687] training: [0.55361778, 1.1700041, 0.27569687]\n",
      "loss on epoch  255  :  1.43228342798 testing: [0.26329443, 2.133924, 0.26478216] training: [0.54886132, 1.1508096, 0.26478216]\n",
      "loss on epoch  260  :  1.48729719056 testing: [0.27237353, 2.1426153, 0.27394646] training: [0.52464688, 1.1776762, 0.27394646]\n",
      "loss on epoch  265  :  1.4616978036 testing: [0.28015563, 2.1424813, 0.27380732] training: [0.55347365, 1.13809, 0.27380732]\n",
      "loss on epoch  270  :  1.47559733303 testing: [0.28404668, 2.1680863, 0.27056834] training: [0.53531277, 1.1838094, 0.27056834]\n",
      "loss on epoch  275  :  1.47226390132 testing: [0.27107653, 2.1291456, 0.27419087] training: [0.54583454, 1.1449208, 0.27419087]\n",
      "loss on epoch  280  :  1.48897425775 testing: [0.25291827, 2.1735563, 0.28136238] training: [0.54309601, 1.1544015, 0.28136238]\n",
      "loss on epoch  285  :  1.47599078108 testing: [0.26459143, 2.1694984, 0.28338048] training: [0.54684347, 1.1521975, 0.28338048]\n",
      "loss on epoch  290  :  1.4653577628 testing: [0.2775616, 2.1540833, 0.27292076] training: [0.53257424, 1.1855923, 0.27292076]\n",
      "loss on epoch  295  :  1.47044729303 testing: [0.28923476, 2.1475534, 0.2796866] training: [0.54554629, 1.1606158, 0.2796866]\n",
      "loss on epoch  300  :  1.46488768083 testing: [0.28793773, 2.1657379, 0.28185827] training: [0.53560102, 1.1777629, 0.28185827]\n",
      "loss on epoch  305  :  1.44741200076 testing: [0.27237353, 2.1622684, 0.26996771] training: [0.53761894, 1.1588951, 0.26996771]\n",
      "loss on epoch  310  :  1.4325289947 testing: [0.29053178, 2.1639061, 0.27380097] training: [0.56442779, 1.1301948, 0.27380097]\n",
      "loss on epoch  315  :  1.43473025605 testing: [0.24773023, 2.1975679, 0.27596703] training: [0.5785529, 1.1289233, 0.27596703]\n",
      "loss on epoch  320  :  1.43954343266 testing: [0.26848248, 2.2090926, 0.28192541] training: [0.56183338, 1.1254953, 0.28192541]\n",
      "loss on epoch  325  :  1.44945813108 testing: [0.25551233, 2.2014968, 0.27420801] training: [0.56601328, 1.12524, 0.27420801]\n",
      "loss on epoch  330  :  1.45954811573 testing: [0.27237353, 2.1690507, 0.28125486] training: [0.56788701, 1.123183, 0.28125486]\n",
      "loss on epoch  335  :  1.43881037059 testing: [0.28274968, 2.2246037, 0.27531633] training: [0.52075523, 1.1832567, 0.27531633]\n",
      "loss on epoch  340  :  1.4410353016 testing: [0.28145266, 2.2241096, 0.28173015] training: [0.55188817, 1.1408525, 0.28173015]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 1118, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 300, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 1044, in getinnerframes\n",
      "    framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 1004, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 451, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/usr/lib/python2.7/genericpath.py\", line 18, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "Unfortunately, your original traceback can not be constructed.\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[1;34m(self, code_obj, result)\u001b[0m\n\u001b[0;32m   2900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2902\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2903\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2904\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[0;32m   1828\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1829\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[1;32m-> 1830\u001b[1;33m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[0;32m   1831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1832\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1390\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[1;32m-> 1392\u001b[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1298\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[1;32m-> 1300\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1301\u001b[0m             )\n\u001b[0;32m   1302\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 \u001b[0mstructured_traceback_parts\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m             \u001b[0mstructured_traceback_parts\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstructured_traceback_parts\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "# v3 fusion\n",
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "\n",
    "# learning speed\n",
    "para_lr = 0.001\n",
    "para_batch_size = 256\n",
    "\n",
    "#   representation ability\n",
    "para_n_hidden_list = [ 128, 64 ]\n",
    "#     128,16,8\n",
    "# !!! change\n",
    "para_n_embedding = 4\n",
    "\n",
    "\n",
    "#   regularization\n",
    "para_l2 = 0.1\n",
    "# !!!change\n",
    "para_keep_prob = 1.0\n",
    "para_max_norm = 7\n",
    "\n",
    "\n",
    "#  fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = [8]*para_n_disc\n",
    "para_n_coor = 25\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    clf = InterNN_fuse( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                        para_n_disc_voca, sess, para_n_embedding, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2, para_batch_size, para_max_norm, para_n_coor)\n",
    "    \n",
    "    \n",
    "#   change  \n",
    "    clf.inference_ini()\n",
    "    clf.train_ini()\n",
    "    \n",
    "    \n",
    "    para_cur_lr = para_lr\n",
    "\n",
    "    total_cnt   = np.shape(cxtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "    \n",
    "    total_idx = range(total_cnt)\n",
    "    \n",
    "#   training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc = 0.0\n",
    "        \n",
    "#       shuffle traning instances each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "    \n",
    "#       Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                     para_cur_lr )\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "    \n",
    "        tmp_test_acc  = clf.inference(dxtest,  cxtest,  ytest,  para_keep_prob) \n",
    "        tmp_train_acc = clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, \"testing:\", tmp_test_acc,\\\n",
    "        \"training:\", tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 5 [5]\n",
      "1.0\n",
      "2.08166817117e-17\n",
      "0.813187183618\n",
      "0.194308854259\n"
     ]
    }
   ],
   "source": [
    "def orthogonal(shape):\n",
    "\n",
    "        flat_shape = (shape[0], np.prod(shape[1:]))\n",
    "        \n",
    "        print shape[0],np.prod(shape[1:]), shape[1:]\n",
    "        \n",
    "        a = np.random.normal(0.0, 1.0, flat_shape)\n",
    "        \n",
    "        u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "        q = u if u.shape == flat_shape else v\n",
    "        \n",
    "        return q.reshape(shape)\n",
    "    \n",
    "tmp = orthogonal([3,5])\n",
    "\n",
    "print sum( [tmp[0][i]*tmp[0][i] for i in range(5)] )\n",
    "\n",
    "print sum( [tmp[0][i]*tmp[2][i] for i in range(5)] )\n",
    "\n",
    "print sum( [tmp[i][0]*tmp[i][0] for i in range(3)] )\n",
    "\n",
    "print sum( [tmp[i][0]*tmp[i][2] for i in range(3)] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fusion, orthogonal ini\n",
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "\n",
    "\n",
    "# learning speed\n",
    "para_lr = 0.005\n",
    "para_batch_size = 32\n",
    "\n",
    "#   representation ability\n",
    "para_n_hidden_list = [ 128, 64 ]\n",
    "#  1024, 512, 512\n",
    "#     128,16,8\n",
    "\n",
    "# !!! change\n",
    "para_n_embedding = 3\n",
    "\n",
    "#   regularization\n",
    "para_l2 = 0.01\n",
    "\n",
    "# !!!change\n",
    "para_keep_prob = 0.8\n",
    "para_max_norm = 7\n",
    "\n",
    "\n",
    "#  fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = [8]*para_n_disc\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    clf = InterNN_fuse( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                        para_n_disc_voca, sess, para_n_embedding, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2, para_batch_size, para_max_norm)\n",
    "    \n",
    "    \n",
    "#   change  \n",
    "    clf.inference_ini()\n",
    "    clf.train_ini()\n",
    "    \n",
    "    \n",
    "    para_cur_lr = para_lr\n",
    "    re\n",
    "    total_cnt   = np.shape(cxtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "    \n",
    "    total_idx = range(total_cnt)\n",
    "    \n",
    "#   training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc = 0.0\n",
    "        \n",
    "#       shuffle traning instances each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "    \n",
    "#       Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                     para_cur_lr )\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "    \n",
    "        tmp_test_acc  = clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
