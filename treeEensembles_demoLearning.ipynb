{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# data processing packages\n",
    "import numpy as np   \n",
    "import pandas as pd \n",
    "from pandas import *\n",
    "from numpy import *\n",
    "\n",
    "from scipy import stats # look at scipy\n",
    "from scipy import linalg\n",
    "from scipy import *\n",
    " \n",
    "%matplotlib inline    \n",
    "import matplotlib as mplt\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.style.use('ggplot')\n",
    "import itertools\n",
    "\n",
    "import random\n",
    "\n",
    "# machine leanring packages\n",
    "\n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation, TimeDistributedDense, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "# RMSprop, Adadelta\n",
    "from keras.regularizers import l2, activity_l2\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6938, 186) (771, 186) (6938, 1) (771, 1)\n",
      "(6938, 186) (6938, 1)\n",
      "(771, 186) (771, 1)\n"
     ]
    }
   ],
   "source": [
    "# ------------data prepro-----------------------\n",
    "\n",
    "files_list=[\"../dataset/dataset_demo/xtrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/xtest.csv\",\\\n",
    "            \"../dataset/dataset_demo/ytrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/ytest.csv\"]\n",
    "\n",
    "xtrain_df=pd.read_csv( files_list[0] ,sep=',', header=None)\n",
    "xtest_df=pd.read_csv( files_list[1] ,sep=',', header=None)\n",
    "ytrain_df=pd.read_csv( files_list[2] ,sep=',', header=None)\n",
    "ytest_df=pd.read_csv( files_list[3] ,sep=',', header=None)\n",
    "\n",
    "print xtrain_df.shape, xtest_df.shape, ytrain_df.shape, ytest_df.shape\n",
    "\n",
    "\n",
    "# get training and testing data prepared\n",
    "\n",
    "# cxtest = conti_normalization_test_dta(cxtest, cxtrain)\n",
    "# cxtrain = conti_normalization_train_dta(cxtrain)\n",
    "\n",
    "print xtrain_df.shape, ytrain_df.shape\n",
    "print xtest_df.shape,  ytest_df.shape\n",
    "\n",
    "# cxtrain = cxtrain.as_matrix() \n",
    "xtrain = xtrain_df.as_matrix()\n",
    "xtest  = xtest_df.as_matrix()\n",
    "\n",
    "ytrain = ytrain_df.as_matrix()\n",
    "ytest  = ytest_df.as_matrix()\n",
    "\n",
    "# ytest = np_utils.to_categorical( ytest_df.as_matrix() )\n",
    "# ytrain =np_utils.to_categorical( ytrain_df.as_matrix() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crossv_rf_num_estimatior(maxnum, X,Y):\n",
    "    score_list=[]\n",
    "    tmpscore=0\n",
    "    tmpnum=0\n",
    "    \n",
    "    tmpy = Y.reshape( (len(Y),) )\n",
    "    \n",
    "    for i in range(1,maxnum+1,10):\n",
    "        clf = RandomForestClassifier(n_estimators=i,max_depth=3)\n",
    "        scores = cross_val_score(clf, X, tmpy)\n",
    "        score_list.append(scores)\n",
    "    \n",
    "        if scores.mean()>tmpscore:\n",
    "            tmpscore=scores.mean()\n",
    "            tmpnum=i\n",
    "    return tmpnum, score_list\n",
    "\n",
    "def crossv_gbt_num_estimatior(maxnum,X,Y):\n",
    "    score_list=[]\n",
    "    tmpscore=0\n",
    "    tmpnum=0\n",
    "    \n",
    "    tmpy = Y.reshape( (len(Y),) )\n",
    "    \n",
    "    for i in range(1,maxnum+1,10):\n",
    "        clf = GradientBoostingClassifier(n_estimators=i,learning_rate=0.1)\n",
    "        scores = cross_val_score(clf, X, tmpy)\n",
    "        score_list.append(scores)\n",
    "    \n",
    "        if scores.mean()>tmpscore:\n",
    "            tmpscore=scores.mean()\n",
    "            tmpnum=i\n",
    "    return tmpnum, score_list\n",
    "\n",
    "def rf_num_estimatior(maxnum, X,Y, xtest, ytest):\n",
    "    \n",
    "    tmpy = Y.reshape( (len(Y),) )\n",
    "    score = []\n",
    "    for i in range(10,maxnum+1,10):\n",
    "        clf = RandomForestClassifier(n_estimators=i,max_depth=3)\n",
    "        \n",
    "        clf.fit( X, tmpy )\n",
    "        score.append( clf.score(xtest, ytest))\n",
    "            \n",
    "    return score\n",
    "\n",
    "def gbt_num_estimatior(maxnum,X,Y, xtest, ytest):\n",
    "    \n",
    "    tmpy = Y.reshape( (len(Y),) )\n",
    "    score = []\n",
    "    for i in range(10,maxnum+1,10):\n",
    "        clf = GradientBoostingClassifier(n_estimators=i,learning_rate=0.1)\n",
    "        \n",
    "        clf.fit( X, tmpy )\n",
    "        score.append( clf.score(xtest, ytest))\n",
    "    \n",
    "    return score\n",
    "\n",
    "def xgt_num_estimator( maxdepth, rounds ,X,Y, xtest, ytest):\n",
    "    \n",
    "    score = []\n",
    "    xg_train = xgb.DMatrix(xtrain, label = ytrain)\n",
    "    xg_test  = xgb.DMatrix(xtest,  label = ytest)\n",
    "\n",
    "# setup parameters for xgboost\n",
    "    param = {}\n",
    "# use softmax multi-class classification\n",
    "    param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 0\n",
    "    param['silent'] = 1\n",
    "    param['nthread'] = 8\n",
    "    param['num_class'] = 8\n",
    "    \n",
    "    for i in range(1, maxdepth):\n",
    "        for num_round in range(1, rounds):\n",
    "            \n",
    "            param['max_depth'] = i\n",
    "            bst = xgb.train(param, xg_train, num_round )\n",
    "            \n",
    "            pred = bst.predict( xg_test )\n",
    "            tmp_accur = ( sum( int(pred[i]) == ytest[i] \\\n",
    "                             for i in range(len(ytest))) / float(len(ytest)) )\n",
    "            \n",
    "            score.append( (i,num_round,tmp_accur) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def result_plot( accu_list, feature_name, maxnum, fig_title, filename ):\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    figsize=( 15.4,7)\n",
    "    font_size=15\n",
    "    fig.set_size_inches( figsize )\n",
    "    matplotlib.rcParams.update({'font.size': font_size})\n",
    "\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0.2,0.35])\n",
    "    \n",
    "    x= range(1,maxnum+1,10)\n",
    "    \n",
    "    for i in range(len(accu_list)):\n",
    "        \n",
    "        tmp=[]\n",
    "        for j in range(len(accu_list[i])):\n",
    "            tmp.append( accu_list[i][j].mean() )\n",
    "        \n",
    "        plt.plot( x, tmp ,label=feature_name[i] )\n",
    "\n",
    "    plt.title(fig_title)\n",
    "    plt.ylabel('Accuray on validation datasets')\n",
    "    plt.xlabel('# of tress or iterations')\n",
    "    # plt.legend( loc='upper left',fontsize=12 )\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "#     fig.savefig('./results/'+filename+'.jpg', format='jpg', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest best validation accuracy: 0.306095979248\n"
     ]
    }
   ],
   "source": [
    "rf_accu= rf_num_estimatior(150,xtrain,ytrain,xtest,ytest)\n",
    "print \"Random forest best validation accuracy:\", max(rf_accu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.315175097276\n"
     ]
    }
   ],
   "source": [
    "gbt_accu = gbt_num_estimatior(150, xtrain,ytrain, xtest, ytest)\n",
    "print \"Gradient boosted tree best validation accuracy:\", max(gbt_accu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gradient boosted tree\n",
    "\n",
    "0.315175097276"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-bf01ddf4476b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mxgt_accu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgt_num_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"XGT best validation accuracy:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgt_accu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-5a4e72980c1c>\u001b[0m in \u001b[0;36mxgt_num_estimator\u001b[1;34m(maxdepth, rounds, X, Y, xtest, ytest)\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_depth'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             \u001b[0mbst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxg_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_round\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mxg_test\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/xgboost-0.6-py2.7.egg/xgboost/training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[0;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/xgboost-0.6-py2.7.egg/xgboost/training.pyc\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/xgboost-0.6-py2.7.egg/xgboost/core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m             \u001b[0m_check_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xgt_accu = xgt_num_estimator(20, 100, xtrain,ytrain, xtest, ytest)\n",
    "print \"XGT best validation accuracy:\", max(xgt_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.613722\ttest-merror:0.721141\n",
      "[1]\ttrain-merror:0.579274\ttest-merror:0.709468\n",
      "[2]\ttrain-merror:0.561545\ttest-merror:0.709468\n",
      "[3]\ttrain-merror:0.545835\ttest-merror:0.692607\n",
      "[4]\ttrain-merror:0.536033\ttest-merror:0.687419\n",
      "[5]\ttrain-merror:0.531421\ttest-merror:0.684825\n",
      "[6]\ttrain-merror:0.518737\ttest-merror:0.677043\n",
      "[7]\ttrain-merror:0.515711\ttest-merror:0.67834\n",
      "[8]\ttrain-merror:0.509225\ttest-merror:0.688716\n",
      "[9]\ttrain-merror:0.501009\ttest-merror:0.693904\n",
      "[10]\ttrain-merror:0.496541\ttest-merror:0.684825\n",
      "[11]\ttrain-merror:0.492793\ttest-merror:0.67834\n",
      "[12]\ttrain-merror:0.487172\ttest-merror:0.688716\n",
      "[13]\ttrain-merror:0.479533\ttest-merror:0.688716\n",
      "[14]\ttrain-merror:0.472326\ttest-merror:0.683528\n",
      "[15]\ttrain-merror:0.466129\ttest-merror:0.690013\n",
      "[16]\ttrain-merror:0.460651\ttest-merror:0.692607\n",
      "[17]\ttrain-merror:0.454742\ttest-merror:0.692607\n",
      "[18]\ttrain-merror:0.452292\ttest-merror:0.69131\n",
      "[19]\ttrain-merror:0.445662\ttest-merror:0.696498\n",
      "[20]\ttrain-merror:0.442491\ttest-merror:0.70428\n",
      "[21]\ttrain-merror:0.43932\ttest-merror:0.70428\n",
      "[22]\ttrain-merror:0.433554\ttest-merror:0.700389\n",
      "[23]\ttrain-merror:0.42923\ttest-merror:0.701686\n",
      "[24]\ttrain-merror:0.424186\ttest-merror:0.693904\n",
      "[25]\ttrain-merror:0.413664\ttest-merror:0.690013\n",
      "[26]\ttrain-merror:0.410493\ttest-merror:0.69131\n",
      "[27]\ttrain-merror:0.407034\ttest-merror:0.687419\n",
      "[28]\ttrain-merror:0.402277\ttest-merror:0.683528\n",
      "[29]\ttrain-merror:0.396944\ttest-merror:0.684825\n",
      "[30]\ttrain-merror:0.394782\ttest-merror:0.690013\n",
      "[31]\ttrain-merror:0.3919\ttest-merror:0.688716\n",
      "[32]\ttrain-merror:0.389017\ttest-merror:0.688716\n",
      "[33]\ttrain-merror:0.387143\ttest-merror:0.687419\n",
      "[34]\ttrain-merror:0.387287\ttest-merror:0.692607\n",
      "predicting, classification accuracy=0.307393\n"
     ]
    }
   ],
   "source": [
    "# backup code\n",
    "import xgboost as xgb\n",
    "# read in data\n",
    "xg_train = xgb.DMatrix(xtrain, label = ytrain)\n",
    "xg_test  = xgb.DMatrix(xtest,  label = ytest)\n",
    "\n",
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 5\n",
    "param['silent'] = 1\n",
    "param['nthread'] = 4\n",
    "param['num_class'] = 8\n",
    "\n",
    "watchlist = [ (xg_train,'train'), (xg_test, 'test') ]\n",
    "num_round = 35\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist );\n",
    "# get prediction\n",
    "pred = bst.predict( xg_test );\n",
    "\n",
    "\n",
    "print ('predicting, classification accuracy=%f' % \\\n",
    "       (sum( int(pred[i]) == ytest[i] for i in range(len(ytest))) / float(len(ytest)) ))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
