{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TO DO:\n",
    "\n",
    "# ordinal \n",
    "# categorical+continuous\n",
    "\n",
    "# feature interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from six.moves import urllib\n",
    "# from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6938, 186) (771, 186) (6938, 1) (771, 1)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "files_list=[\"../dataset/dataset_demo/xtrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/xtest.csv\",\\\n",
    "            \"../dataset/dataset_demo/ytrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/ytest.csv\"]\n",
    "\n",
    "xtrain_df=pd.read_csv( files_list[0] ,sep=',', header=None)\n",
    "xtest_df=pd.read_csv( files_list[1] ,sep=',', header=None)\n",
    "ytrain_df=pd.read_csv( files_list[2] ,sep=',', header=None)\n",
    "ytest_df=pd.read_csv( files_list[3] ,sep=',', header=None)\n",
    "\n",
    "print xtrain_df.shape, xtest_df.shape, ytrain_df.shape, ytest_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# continuous and categorical feature split\n",
    "\n",
    "# features on columns [0:split_idx] are continuous\n",
    "# the rest are categorical\n",
    "def conti_cate_split(dta_df, split_idx):\n",
    "    \n",
    "    col_cnt= dta_df.shape[1]\n",
    "    conti_cols= range(split_idx)\n",
    "    dis_cols=range(split_idx, col_cnt)\n",
    "    \n",
    "    return dta_df[conti_cols], dta_df[dis_cols]\n",
    "\n",
    "def conti_normalization_train_dta(dta_df):\n",
    "    \n",
    "    return preprocessing.scale(dta_df)\n",
    "\n",
    "def conti_normalization_test_dta(dta_df, train_df):\n",
    "    \n",
    "    mean_dim = np.mean(train_df, axis=0)\n",
    "    std_dim = np.std(train_df, axis=0)\n",
    "        \n",
    "    df=pd.DataFrame()\n",
    "    cols = train_df.columns\n",
    "    idx=0\n",
    "    \n",
    "    for i in cols:\n",
    "        df[i] = (dta_df[i]- mean_dim[idx])*1.0/std_dim[idx]\n",
    "        idx=idx+1\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6938, 106) (6938, 80)\n",
      "(771, 106) (771, 80)\n"
     ]
    }
   ],
   "source": [
    "# get training and testing data prepared\n",
    "\n",
    "cxtrain, dxtrain = conti_cate_split(xtrain_df, 106)\n",
    "cxtest, dxtest = conti_cate_split(xtest_df, 106)\n",
    "\n",
    "cxtest = conti_normalization_test_dta(cxtest, cxtrain)\n",
    "cxtrain = conti_normalization_train_dta(cxtrain)\n",
    "\n",
    "print cxtrain.shape, dxtrain.shape\n",
    "print cxtest.shape, dxtest.shape\n",
    "\n",
    "# cxtrain = cxtrain.as_matrix() \n",
    "dxtrain = dxtrain.as_matrix().astype(int)\n",
    "cxtest = cxtest.as_matrix()\n",
    "dxtest = dxtest.as_matrix().astype(int)\n",
    "\n",
    "ytest = np_utils.to_categorical( ytest_df.as_matrix() )\n",
    "ytrain =np_utils.to_categorical( ytrain_df.as_matrix() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# demoMLP with embedding\n",
    "\n",
    "class mlp_demo():\n",
    "    \n",
    "    def __init__(self, batch_size, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, n_embedding, session, n_hidden_list, lr, l2):\n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "        self.BATCH_SIZE = batch_size\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.N_EMBED = n_embedding\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list) \n",
    "        self.N_TOTAL = self.N_CONTI+ self.N_DISC*self.N_EMBED\n",
    "        \n",
    "        self.sess=session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "#       embedding categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "        \n",
    "        tmp_embeded = []                \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"disc\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA, self.N_EMBED],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "                              \n",
    "                tmp_embeded.append( tf.nn.embedding_lookup( w, self.dx_trans[i] ) )\n",
    "                \n",
    "        dx_embeded = tf.concat(tmp_embeded, 1)\n",
    "        x_concate =  tf.concat( [self.cx,dx_embeded], 1 )\n",
    "\n",
    "#----unit test---------------------------------------------------------\n",
    "#         print self.N_TOTAL\n",
    "#         self.tmpshape= tf.shape(x_concate)\n",
    "        \n",
    "#         self.tmpshape1= tf.shape(tmp_embeded[0])\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# #     hidden layers\n",
    "        with tf.variable_scope(\"h0\"):\n",
    "            w= tf.Variable(tf.random_normal([self.N_TOTAL, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_TOTAL)))) \n",
    "            \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            \n",
    "            h = tf.nn.relu( tf.add( tf.matmul(x_concate, w),b) )\n",
    "        \n",
    "        \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"layer\"+str(i)):\n",
    "                \n",
    "                w= tf.Variable(tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                \n",
    "                b= tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                \n",
    "                h = tf.nn.relu( tf.add( tf.matmul(h, w),b) )\n",
    "        \n",
    "#   Regularization  \n",
    "#       dropout\n",
    "#         h = tf.nn.dropout(h, self.keep_prob)\n",
    "\n",
    "#       L2\n",
    "        self.regularizer = tf.nn.l2_loss(w)\n",
    "    \n",
    "        \n",
    "#       output layer  \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w= tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1],\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "                \n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            \n",
    "            h = tf.add( tf.matmul(h, w),b)\n",
    "            \n",
    "#           L2  \n",
    "            self.regularizer = self.regularizer + tf.nn.l2_loss(w)\n",
    "            \n",
    "            \n",
    "            self.logit = h\n",
    "    \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        \n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y) \\\n",
    "                                   )\n",
    "#                                   + self.L2*self.regularizer)\n",
    "        \n",
    "        self.optimizer = \\\n",
    "        tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                       feed_dict={self.dx:dx_batch, \\\n",
    "                                  self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                  self.lr:lr,\\\n",
    "                                  self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "    \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        \n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                  self.cx:cx_test, self.y:y_test,\\\n",
    "                                                  self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "        return sess.run( [self.tmpshape], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch  0  :  2.24677761396 [0.23994812] [0.22340733]\n",
      "loss on epoch  1  :  2.21367904433 [0.23994812] [0.22398385]\n",
      "loss on epoch  2  :  2.18426671072 [0.24383917] [0.22484866]\n",
      "loss on epoch  3  :  2.15641204295 [0.2464332] [0.2249928]\n",
      "loss on epoch  4  :  2.13343149644 [0.25162128] [0.22773133]\n",
      "loss on epoch  5  :  2.10959360114 [0.25551233] [0.23119055]\n",
      "loss on epoch  6  :  2.09072771779 [0.25162128] [0.23277602]\n",
      "loss on epoch  7  :  2.06875572381 [0.25940338] [0.23464975]\n",
      "loss on epoch  8  :  2.05185413581 [0.26588845] [0.23767656]\n",
      "loss on epoch  9  :  2.03580170649 [0.26718548] [0.2382531]\n",
      "loss on epoch  10  :  2.01991961841 [0.27885863] [0.24027097]\n",
      "loss on epoch  11  :  2.00784228687 [0.27367055] [0.24099164]\n",
      "loss on epoch  12  :  1.99369510677 [0.27496758] [0.24502738]\n",
      "loss on epoch  13  :  1.98256835452 [0.2775616] [0.24632458]\n",
      "loss on epoch  14  :  1.97036063892 [0.28015563] [0.24675699]\n",
      "loss on epoch  15  :  1.96040311345 [0.27626458] [0.24963966]\n",
      "loss on epoch  16  :  1.95192649409 [0.27885863] [0.25136927]\n",
      "loss on epoch  17  :  1.94270973735 [0.27496758] [0.2528106]\n",
      "loss on epoch  18  :  1.93379043871 [0.27626458] [0.2536754]\n",
      "loss on epoch  19  :  1.92681562018 [0.27496758] [0.25454021]\n",
      "loss on epoch  20  :  1.9179750129 [0.27626458] [0.25569329]\n",
      "loss on epoch  21  :  1.91230210993 [0.27496758] [0.25684637]\n",
      "loss on epoch  22  :  1.904779549 [0.27367055] [0.25756702]\n",
      "loss on epoch  23  :  1.89798520892 [0.27107653] [0.2591525]\n",
      "loss on epoch  24  :  1.89187558051 [0.26848248] [0.2587201]\n",
      "loss on epoch  25  :  1.88707792759 [0.27107653] [0.25987315]\n",
      "loss on epoch  26  :  1.88093649458 [0.2697795] [0.26001731]\n",
      "loss on epoch  27  :  1.87623615618 [0.2697795] [0.26044971]\n",
      "loss on epoch  28  :  1.87062105647 [0.26848248] [0.26131451]\n",
      "loss on epoch  29  :  1.8658794849 [0.26718548] [0.26304412]\n",
      "loss on epoch  30  :  1.86139882715 [0.26588845] [0.26419717]\n",
      "loss on epoch  31  :  1.8562391599 [0.26070037] [0.2647737]\n",
      "loss on epoch  32  :  1.85220504028 [0.26070037] [0.2647737]\n",
      "loss on epoch  33  :  1.84826512469 [0.26070037] [0.2647737]\n",
      "loss on epoch  34  :  1.84327574792 [0.2619974] [0.26578265]\n",
      "loss on epoch  35  :  1.83974642003 [0.2619974] [0.26736811]\n",
      "loss on epoch  36  :  1.83634725323 [0.25940338] [0.26751226]\n",
      "loss on epoch  37  :  1.83212957117 [0.25680932] [0.26707986]\n",
      "loss on epoch  38  :  1.82866803364 [0.2542153] [0.26693571]\n",
      "loss on epoch  39  :  1.82505592152 [0.25551233] [0.26635918]\n",
      "loss on epoch  40  :  1.82205220505 [0.25680932] [0.26679158]\n",
      "loss on epoch  41  :  1.81794807867 [0.25551233] [0.26780051]\n",
      "loss on epoch  42  :  1.81515330076 [0.25940338] [0.26808879]\n",
      "loss on epoch  43  :  1.81247128601 [0.2619974] [0.2706832]\n",
      "loss on epoch  44  :  1.80978472807 [0.26459143] [0.27097145]\n",
      "loss on epoch  45  :  1.80549367048 [0.2619974] [0.27053908]\n",
      "loss on epoch  46  :  1.80310674729 [0.25810635] [0.271548]\n",
      "loss on epoch  47  :  1.80045092547 [0.25551233] [0.27255693]\n",
      "loss on epoch  48  :  1.79848549101 [0.2542153] [0.27226865]\n",
      "loss on epoch  49  :  1.79512891725 [0.2542153] [0.27327761]\n",
      "loss on epoch  50  :  1.79388357533 [0.25551233] [0.27443066]\n",
      "loss on epoch  51  :  1.78935325808 [0.26070037] [0.27471894]\n",
      "loss on epoch  52  :  1.78820284428 [0.25940338] [0.27529547]\n",
      "loss on epoch  53  :  1.78586896481 [0.25810635] [0.27673683]\n",
      "loss on epoch  54  :  1.78167027456 [0.26329443] [0.27659267]\n",
      "loss on epoch  55  :  1.78049725294 [0.26329443] [0.27745748]\n",
      "loss on epoch  56  :  1.77879650284 [0.26329443] [0.2784664]\n",
      "loss on epoch  57  :  1.77640885115 [0.26329443] [0.2784664]\n",
      "loss on epoch  58  :  1.77397118454 [0.26329443] [0.27904296]\n",
      "loss on epoch  59  :  1.77181511234 [0.26459143] [0.27904296]\n",
      "loss on epoch  60  :  1.77053136517 [0.26718548] [0.27861056]\n",
      "loss on epoch  61  :  1.76842506285 [0.26718548] [0.27961949]\n",
      "loss on epoch  62  :  1.76653759347 [0.26848248] [0.28005189]\n",
      "loss on epoch  63  :  1.76417362911 [0.26718548] [0.28034016]\n",
      "loss on epoch  64  :  1.76218940373 [0.26588845] [0.28163737]\n",
      "loss on epoch  65  :  1.76098904345 [0.26588845] [0.28279042]\n",
      "loss on epoch  66  :  1.75826611121 [0.26588845] [0.2822139]\n",
      "loss on epoch  67  :  1.75722702786 [0.26588845] [0.2826463]\n",
      "loss on epoch  68  :  1.75588610879 [0.26588845] [0.28379938]\n",
      "loss on epoch  69  :  1.75326509387 [0.26848248] [0.28379938]\n",
      "loss on epoch  70  :  1.75185095381 [0.26848248] [0.28408763]\n",
      "loss on epoch  71  :  1.74935838691 [0.26718548] [0.28408763]\n",
      "loss on epoch  72  :  1.74915064485 [0.2697795] [0.28466415]\n",
      "loss on epoch  73  :  1.74616176552 [0.2697795] [0.28552896]\n",
      "loss on epoch  74  :  1.74493840006 [0.2697795] [0.28596136]\n",
      "loss on epoch  75  :  1.74351048911 [0.2697795] [0.28624964]\n",
      "loss on epoch  76  :  1.74090056728 [0.26848248] [0.28740272]\n",
      "loss on epoch  77  :  1.73931331325 [0.26718548] [0.28783512]\n",
      "loss on epoch  78  :  1.73906768914 [0.2697795] [0.28797925]\n",
      "loss on epoch  79  :  1.7374349678 [0.2697795] [0.28826752]\n",
      "loss on epoch  80  :  1.73629109065 [0.2697795] [0.28754684]\n",
      "loss on epoch  81  :  1.73388246033 [0.2697795] [0.28869992]\n",
      "loss on epoch  82  :  1.73349619133 [0.2697795] [0.28826752]\n",
      "loss on epoch  83  :  1.73117104725 [0.2697795] [0.28913233]\n",
      "loss on epoch  84  :  1.72991042667 [0.27367055] [0.28942057]\n",
      "loss on epoch  85  :  1.72851267788 [0.27496758] [0.29014125]\n",
      "loss on epoch  86  :  1.72676563042 [0.27367055] [0.29057366]\n",
      "loss on epoch  87  :  1.72607477064 [0.27496758] [0.29071778]\n",
      "loss on epoch  88  :  1.72453218478 [0.27496758] [0.29057366]\n",
      "loss on epoch  89  :  1.72355694462 [0.27626458] [0.29172671]\n",
      "loss on epoch  90  :  1.72240254835 [0.27626458] [0.29172671]\n",
      "loss on epoch  91  :  1.72163606352 [0.2775616] [0.29187086]\n",
      "loss on epoch  92  :  1.72035262099 [0.27626458] [0.29201499]\n",
      "loss on epoch  93  :  1.71805611805 [0.27626458] [0.29230326]\n",
      "loss on epoch  94  :  1.71706646239 [0.27626458] [0.29345632]\n",
      "loss on epoch  95  :  1.71593132284 [0.27626458] [0.29388872]\n",
      "loss on epoch  96  :  1.71388163831 [0.27626458] [0.29432112]\n",
      "loss on epoch  97  :  1.7139884388 [0.27626458] [0.29475352]\n",
      "loss on epoch  98  :  1.71232897043 [0.27367055] [0.29533008]\n",
      "loss on epoch  99  :  1.71122951199 [0.27626458] [0.2954742]\n",
      "loss on epoch  100  :  1.70923778084 [0.2775616] [0.29605073]\n",
      "loss on epoch  101  :  1.70887911982 [0.2775616] [0.29619488]\n",
      "loss on epoch  102  :  1.70740246773 [0.27885863] [0.29662728]\n",
      "loss on epoch  103  :  1.70703055682 [0.27885863] [0.29720381]\n",
      "loss on epoch  104  :  1.70541297286 [0.27885863] [0.29850101]\n",
      "loss on epoch  105  :  1.70397143452 [0.28015563] [0.29922166]\n",
      "loss on epoch  106  :  1.70286445706 [0.28015563] [0.29979822]\n",
      "loss on epoch  107  :  1.70127298655 [0.28145266] [0.30008647]\n",
      "loss on epoch  108  :  1.69968599964 [0.28274968] [0.30023062]\n",
      "loss on epoch  109  :  1.69904092285 [0.28404668] [0.30080715]\n",
      "loss on epoch  110  :  1.69793230516 [0.28534371] [0.30138367]\n",
      "loss on epoch  111  :  1.69732098668 [0.28534371] [0.30196023]\n",
      "loss on epoch  112  :  1.69586436395 [0.28534371] [0.30253676]\n",
      "loss on epoch  113  :  1.69538138089 [0.28534371] [0.30340156]\n",
      "loss on epoch  114  :  1.69454824483 [0.28404668] [0.30296916]\n",
      "loss on epoch  115  :  1.69266943137 [0.28274968] [0.30325744]\n",
      "loss on epoch  116  :  1.69176880739 [0.28274968] [0.30383396]\n",
      "loss on epoch  117  :  1.69156817374 [0.28404668] [0.30397809]\n",
      "loss on epoch  118  :  1.69018824674 [0.28274968] [0.30484289]\n",
      "loss on epoch  119  :  1.68875157612 [0.28404668] [0.30513117]\n",
      "loss on epoch  120  :  1.6886849889 [0.28404668] [0.30570769]\n",
      "loss on epoch  121  :  1.68614518422 [0.28534371] [0.30628422]\n",
      "loss on epoch  122  :  1.68556002776 [0.28534371] [0.30570769]\n",
      "loss on epoch  123  :  1.68548282208 [0.28534371] [0.30585182]\n",
      "loss on epoch  124  :  1.6846566708 [0.28534371] [0.3065725]\n",
      "loss on epoch  125  :  1.68259819349 [0.28534371] [0.3070049]\n",
      "loss on epoch  126  :  1.68148030837 [0.28404668] [0.3074373]\n",
      "loss on epoch  127  :  1.6801008538 [0.28404668] [0.30772558]\n",
      "loss on epoch  128  :  1.68082086024 [0.28534371] [0.30801383]\n",
      "loss on epoch  129  :  1.67909457065 [0.28664073] [0.30859038]\n",
      "loss on epoch  130  :  1.67774566015 [0.28664073] [0.30887863]\n",
      "loss on epoch  131  :  1.67672831262 [0.28793773] [0.30959931]\n",
      "loss on epoch  132  :  1.67608955392 [0.28793773] [0.31046411]\n",
      "loss on epoch  133  :  1.67579628362 [0.28793773] [0.31132892]\n",
      "loss on epoch  134  :  1.67528799949 [0.28793773] [0.3116172]\n",
      "loss on epoch  135  :  1.67372104415 [0.28923476] [0.31233785]\n",
      "loss on epoch  136  :  1.67327671139 [0.28923476] [0.3116172]\n",
      "loss on epoch  137  :  1.67152620245 [0.29312581] [0.31248197]\n",
      "loss on epoch  138  :  1.67094062876 [0.29312581] [0.31262612]\n",
      "loss on epoch  139  :  1.6713866459 [0.29312581] [0.31363505]\n",
      "loss on epoch  140  :  1.66980705879 [0.29312581] [0.31421158]\n",
      "loss on epoch  141  :  1.66885236678 [0.29442284] [0.31435573]\n",
      "loss on epoch  142  :  1.66726079031 [0.29442284] [0.31435573]\n",
      "loss on epoch  143  :  1.66641765391 [0.29312581] [0.31493226]\n",
      "loss on epoch  144  :  1.66566850742 [0.29182878] [0.31464398]\n",
      "loss on epoch  145  :  1.66524176465 [0.29182878] [0.31536466]\n",
      "loss on epoch  146  :  1.66409896259 [0.29182878] [0.31594118]\n",
      "loss on epoch  147  :  1.66333548228 [0.29182878] [0.31608534]\n",
      "loss on epoch  148  :  1.66236158654 [0.29182878] [0.31637359]\n",
      "loss on epoch  149  :  1.6611965409 [0.29182878] [0.31695014]\n",
      "loss on epoch  150  :  1.6614547703 [0.29182878] [0.31738254]\n",
      "loss on epoch  151  :  1.65963500738 [0.29182878] [0.31781495]\n",
      "loss on epoch  152  :  1.65946279411 [0.29182878] [0.31810319]\n",
      "loss on epoch  153  :  1.65847549174 [0.29312581] [0.31810319]\n",
      "loss on epoch  154  :  1.65814139225 [0.29312581] [0.31795907]\n",
      "loss on epoch  155  :  1.65626824785 [0.29182878] [0.31839147]\n",
      "loss on epoch  156  :  1.65687034969 [0.29182878] [0.3185356]\n",
      "loss on epoch  157  :  1.65498780983 [0.29182878] [0.3185356]\n",
      "loss on epoch  158  :  1.65498089349 [0.29182878] [0.31839147]\n",
      "loss on epoch  159  :  1.65345665481 [0.29182878] [0.31867972]\n",
      "loss on epoch  160  :  1.65357869643 [0.29312581] [0.318968]\n",
      "loss on epoch  161  :  1.65220238765 [0.29312581] [0.31882387]\n",
      "loss on epoch  162  :  1.65191326097 [0.29442284] [0.31954452]\n",
      "loss on epoch  163  :  1.65021123489 [0.29701686] [0.32012108]\n",
      "loss on epoch  164  :  1.65037104819 [0.29701686] [0.32040933]\n",
      "loss on epoch  165  :  1.64934675782 [0.29701686] [0.32127413]\n",
      "loss on epoch  166  :  1.64828757666 [0.29701686] [0.32185069]\n",
      "loss on epoch  167  :  1.64818272546 [0.29701686] [0.32213894]\n",
      "loss on epoch  168  :  1.64698658608 [0.29701686] [0.32213894]\n",
      "loss on epoch  169  :  1.6464414729 [0.29701686] [0.32271549]\n",
      "loss on epoch  170  :  1.64575216726 [0.29701686] [0.32285962]\n",
      "loss on epoch  171  :  1.64443427766 [0.29701686] [0.32314789]\n",
      "loss on epoch  172  :  1.64360200697 [0.29701686] [0.32343614]\n",
      "loss on epoch  173  :  1.64338429327 [0.29571983] [0.32343614]\n",
      "loss on epoch  174  :  1.64195145943 [0.29571983] [0.32358029]\n",
      "loss on epoch  175  :  1.64197801219 [0.29571983] [0.32372442]\n",
      "loss on epoch  176  :  1.64147181423 [0.29571983] [0.3244451]\n",
      "loss on epoch  177  :  1.63997624318 [0.29571983] [0.32430094]\n",
      "loss on epoch  178  :  1.63955082938 [0.29701686] [0.32458922]\n",
      "loss on epoch  179  :  1.63868661059 [0.29701686] [0.3248775]\n",
      "loss on epoch  180  :  1.63845819014 [0.29701686] [0.32458922]\n",
      "loss on epoch  181  :  1.637655788 [0.29701686] [0.32516575]\n",
      "loss on epoch  182  :  1.63677531039 [0.29701686] [0.32559815]\n",
      "loss on epoch  183  :  1.63544342915 [0.29961088] [0.32545403]\n",
      "loss on epoch  184  :  1.63592534816 [0.29961088] [0.32559815]\n",
      "loss on epoch  185  :  1.63359191241 [0.30090791] [0.32603055]\n",
      "loss on epoch  186  :  1.63396993831 [0.29961088] [0.32631883]\n",
      "loss on epoch  187  :  1.63385762992 [0.29961088] [0.32660708]\n",
      "loss on epoch  188  :  1.63290699544 [0.29831389] [0.32703948]\n",
      "loss on epoch  189  :  1.63165392258 [0.29701686] [0.32732776]\n",
      "loss on epoch  190  :  1.63109722623 [0.29701686] [0.32819256]\n",
      "loss on epoch  191  :  1.63103281789 [0.29701686] [0.32819256]\n",
      "loss on epoch  192  :  1.63088338022 [0.29701686] [0.32876909]\n",
      "loss on epoch  193  :  1.62936556339 [0.29831389] [0.32934564]\n",
      "loss on epoch  194  :  1.62838119268 [0.29831389] [0.33006629]\n",
      "loss on epoch  195  :  1.62819022161 [0.29831389] [0.33006629]\n",
      "loss on epoch  196  :  1.62754946947 [0.29701686] [0.33078697]\n",
      "loss on epoch  197  :  1.62619755886 [0.29831389] [0.33035457]\n",
      "loss on epoch  198  :  1.62572847693 [0.29831389] [0.33078697]\n",
      "loss on epoch  199  :  1.62455812207 [0.29831389] [0.33165178]\n",
      "loss on epoch  200  :  1.62490242057 [0.29831389] [0.33194005]\n",
      "loss on epoch  201  :  1.62241310323 [0.29831389] [0.33165178]\n",
      "loss on epoch  202  :  1.62383212867 [0.29831389] [0.3322283]\n",
      "loss on epoch  203  :  1.62274087138 [0.29961088] [0.33251658]\n",
      "loss on epoch  204  :  1.6219438182 [0.29961088] [0.3326607]\n",
      "loss on epoch  205  :  1.62125065592 [0.29961088] [0.33294898]\n",
      "loss on epoch  206  :  1.6211367404 [0.29961088] [0.33309311]\n",
      "loss on epoch  207  :  1.6208529671 [0.29961088] [0.33294898]\n",
      "loss on epoch  208  :  1.6191369847 [0.29961088] [0.33309311]\n",
      "loss on epoch  209  :  1.61851443406 [0.30090791] [0.33323723]\n",
      "loss on epoch  210  :  1.6186031346 [0.30090791] [0.33366963]\n",
      "loss on epoch  211  :  1.6176189714 [0.30090791] [0.33352551]\n",
      "loss on epoch  212  :  1.61651934959 [0.30090791] [0.33381379]\n",
      "loss on epoch  213  :  1.61647874558 [0.29831389] [0.33381379]\n",
      "loss on epoch  214  :  1.61594000128 [0.29961088] [0.33395791]\n",
      "loss on epoch  215  :  1.61507791502 [0.29831389] [0.33410203]\n",
      "loss on epoch  216  :  1.61486239345 [0.29831389] [0.33496684]\n",
      "loss on epoch  217  :  1.61335104925 [0.29961088] [0.33539924]\n",
      "loss on epoch  218  :  1.61305086039 [0.29961088] [0.33539924]\n",
      "loss on epoch  219  :  1.61153453809 [0.29961088] [0.33568752]\n",
      "loss on epoch  220  :  1.61210795906 [0.29831389] [0.33583164]\n",
      "loss on epoch  221  :  1.61151972303 [0.29831389] [0.33611992]\n",
      "loss on epoch  222  :  1.61085816887 [0.29701686] [0.33698472]\n",
      "loss on epoch  223  :  1.60982896443 [0.29701686] [0.337273]\n",
      "loss on epoch  224  :  1.60982156683 [0.29571983] [0.33712885]\n",
      "loss on epoch  225  :  1.60884545468 [0.29701686] [0.33784953]\n",
      "loss on epoch  226  :  1.60783340975 [0.29701686] [0.33857021]\n",
      "loss on epoch  227  :  1.60814812228 [0.29442284] [0.33885846]\n",
      "loss on epoch  228  :  1.60678201031 [0.29701686] [0.33929086]\n",
      "loss on epoch  229  :  1.6062734392 [0.29442284] [0.34001154]\n",
      "loss on epoch  230  :  1.60542902019 [0.29312581] [0.33986738]\n",
      "loss on epoch  231  :  1.6056118166 [0.29312581] [0.33957914]\n",
      "loss on epoch  232  :  1.60489498907 [0.29312581] [0.33957914]\n",
      "loss on epoch  233  :  1.60458981329 [0.29312581] [0.34044394]\n",
      "loss on epoch  234  :  1.6037952922 [0.29053178] [0.34087634]\n",
      "loss on epoch  235  :  1.60294440278 [0.29182878] [0.34087634]\n",
      "loss on epoch  236  :  1.60299259203 [0.29442284] [0.34116459]\n",
      "loss on epoch  237  :  1.60173483027 [0.29312581] [0.34116459]\n",
      "loss on epoch  238  :  1.60096168739 [0.29053178] [0.34159699]\n",
      "loss on epoch  239  :  1.60094774211 [0.29182878] [0.34145287]\n",
      "loss on epoch  240  :  1.60045700603 [0.29182878] [0.34174114]\n",
      "loss on epoch  241  :  1.59918382212 [0.29312581] [0.34202939]\n",
      "loss on epoch  242  :  1.59920687146 [0.29182878] [0.34217355]\n",
      "loss on epoch  243  :  1.59840087096 [0.29182878] [0.34246179]\n",
      "loss on epoch  244  :  1.59795804377 [0.29053178] [0.34260595]\n",
      "loss on epoch  245  :  1.59739924802 [0.29053178] [0.34275007]\n",
      "loss on epoch  246  :  1.59714761708 [0.29182878] [0.34347075]\n",
      "loss on epoch  247  :  1.59632873094 [0.29053178] [0.343759]\n",
      "loss on epoch  248  :  1.59614418392 [0.29182878] [0.34433556]\n",
      "loss on epoch  249  :  1.59549749118 [0.29053178] [0.34447968]\n",
      "loss on epoch  250  :  1.59340771039 [0.29053178] [0.34520036]\n",
      "loss on epoch  251  :  1.59392226184 [0.29053178] [0.34520036]\n",
      "loss on epoch  252  :  1.59398348905 [0.29053178] [0.34664169]\n",
      "loss on epoch  253  :  1.5935255554 [0.29182878] [0.34649754]\n",
      "loss on epoch  254  :  1.59185224109 [0.29053178] [0.34664169]\n",
      "loss on epoch  255  :  1.59210270864 [0.29053178] [0.34736234]\n",
      "loss on epoch  256  :  1.59017012737 [0.28923476] [0.34736234]\n",
      "loss on epoch  257  :  1.59031639055 [0.28923476] [0.34808302]\n",
      "loss on epoch  258  :  1.59024648534 [0.28793773] [0.34808302]\n",
      "loss on epoch  259  :  1.58896367417 [0.28923476] [0.34851542]\n",
      "loss on epoch  260  :  1.58902832535 [0.29053178] [0.34851542]\n",
      "loss on epoch  261  :  1.58810526574 [0.29053178] [0.34865955]\n",
      "loss on epoch  262  :  1.58812719142 [0.29053178] [0.34909195]\n",
      "loss on epoch  263  :  1.58782760081 [0.29053178] [0.3492361]\n",
      "loss on epoch  264  :  1.58691438922 [0.29053178] [0.34981263]\n",
      "loss on epoch  265  :  1.5859299567 [0.28923476] [0.35038915]\n",
      "loss on epoch  266  :  1.5859279169 [0.29053178] [0.35082155]\n",
      "loss on epoch  267  :  1.58544056283 [0.29053178] [0.35168636]\n",
      "loss on epoch  268  :  1.58478446139 [0.29053178] [0.35211876]\n",
      "loss on epoch  269  :  1.58422222182 [0.29053178] [0.35168636]\n",
      "loss on epoch  270  :  1.58427201377 [0.29053178] [0.35240704]\n",
      "loss on epoch  271  :  1.58309637396 [0.29053178] [0.35240704]\n",
      "loss on epoch  272  :  1.58229245742 [0.29053178] [0.35255116]\n",
      "loss on epoch  273  :  1.58285880972 [0.29053178] [0.35269532]\n",
      "loss on epoch  274  :  1.58179307205 [0.28923476] [0.35240704]\n",
      "loss on epoch  275  :  1.58082001739 [0.29053178] [0.35240704]\n",
      "loss on epoch  276  :  1.58094363742 [0.29053178] [0.35240704]\n",
      "loss on epoch  277  :  1.58016088274 [0.29182878] [0.35327184]\n",
      "loss on epoch  278  :  1.5797201086 [0.29312581] [0.35413665]\n",
      "loss on epoch  279  :  1.57795304722 [0.29312581] [0.35428077]\n",
      "loss on epoch  280  :  1.57865288743 [0.29182878] [0.35428077]\n",
      "loss on epoch  281  :  1.57674715033 [0.29312581] [0.35471317]\n",
      "loss on epoch  282  :  1.57692717844 [0.29312581] [0.35471317]\n",
      "loss on epoch  283  :  1.57690055503 [0.29312581] [0.3548573]\n",
      "loss on epoch  284  :  1.57582928075 [0.29312581] [0.35543385]\n",
      "loss on epoch  285  :  1.57621335763 [0.29312581] [0.35543385]\n",
      "loss on epoch  286  :  1.57519520654 [0.29312581] [0.3557221]\n",
      "loss on epoch  287  :  1.57398448609 [0.29312581] [0.3557221]\n",
      "loss on epoch  288  :  1.57316487365 [0.29312581] [0.3552897]\n",
      "loss on epoch  289  :  1.57365631836 [0.29312581] [0.3561545]\n",
      "loss on epoch  290  :  1.57272537329 [0.29312581] [0.3561545]\n",
      "loss on epoch  291  :  1.57277108563 [0.29312581] [0.3557221]\n",
      "loss on epoch  292  :  1.57262710509 [0.29182878] [0.35644278]\n",
      "loss on epoch  293  :  1.57203182909 [0.29312581] [0.35673106]\n",
      "loss on epoch  294  :  1.57067027798 [0.29312581] [0.35701931]\n",
      "loss on epoch  295  :  1.57039634387 [0.29312581] [0.35802826]\n",
      "loss on epoch  296  :  1.56968821861 [0.29312581] [0.35846066]\n",
      "loss on epoch  297  :  1.56968118085 [0.29442284] [0.35889307]\n",
      "loss on epoch  298  :  1.56890934485 [0.29442284] [0.35846066]\n",
      "loss on epoch  299  :  1.56881869281 [0.29442284] [0.35889307]\n",
      "loss on epoch  300  :  1.56891814426 [0.29312581] [0.35918131]\n",
      "loss on epoch  301  :  1.56772234263 [0.29442284] [0.35946959]\n",
      "loss on epoch  302  :  1.56746155465 [0.29442284] [0.35975784]\n",
      "loss on epoch  303  :  1.56698511927 [0.29442284] [0.36019024]\n",
      "loss on epoch  304  :  1.56658881903 [0.29442284] [0.36047852]\n",
      "loss on epoch  305  :  1.56607062287 [0.29571983] [0.3616316]\n",
      "loss on epoch  306  :  1.56510806304 [0.29571983] [0.362064]\n",
      "loss on epoch  307  :  1.56439561976 [0.29571983] [0.36220813]\n",
      "loss on epoch  308  :  1.5640073242 [0.29571983] [0.36264053]\n",
      "loss on epoch  309  :  1.56381100195 [0.29571983] [0.36249641]\n",
      "loss on epoch  310  :  1.56238835167 [0.29571983] [0.36278465]\n",
      "loss on epoch  311  :  1.56223139498 [0.29442284] [0.36321706]\n",
      "loss on epoch  312  :  1.56179292997 [0.29442284] [0.36364946]\n",
      "loss on epoch  313  :  1.56193601423 [0.29312581] [0.36393774]\n",
      "loss on epoch  314  :  1.56108287529 [0.29312581] [0.36364946]\n",
      "loss on epoch  315  :  1.56050961106 [0.29442284] [0.36408186]\n",
      "loss on epoch  316  :  1.55999791181 [0.29442284] [0.36437014]\n",
      "loss on epoch  317  :  1.55928291656 [0.29442284] [0.36422601]\n",
      "loss on epoch  318  :  1.55830025231 [0.29442284] [0.36465842]\n",
      "loss on epoch  319  :  1.55874429146 [0.29442284] [0.36451426]\n",
      "loss on epoch  320  :  1.55825491526 [0.29442284] [0.36552322]\n",
      "loss on epoch  321  :  1.55768358266 [0.29442284] [0.36523494]\n",
      "loss on epoch  322  :  1.55748773504 [0.29442284] [0.36552322]\n",
      "loss on epoch  323  :  1.55723968038 [0.29442284] [0.36624387]\n",
      "loss on epoch  324  :  1.55583118951 [0.29442284] [0.36653215]\n",
      "loss on epoch  325  :  1.55566612217 [0.29312581] [0.36653215]\n",
      "loss on epoch  326  :  1.55558922997 [0.29312581] [0.36696455]\n",
      "loss on epoch  327  :  1.55481132975 [0.29182878] [0.3676852]\n",
      "loss on epoch  328  :  1.5550032545 [0.29182878] [0.3681176]\n",
      "loss on epoch  329  :  1.55420106429 [0.29182878] [0.36797348]\n",
      "loss on epoch  330  :  1.55270019505 [0.29312581] [0.36782935]\n",
      "loss on epoch  331  :  1.55203608893 [0.29442284] [0.36840588]\n",
      "loss on epoch  332  :  1.55221880586 [0.29442284] [0.36840588]\n",
      "loss on epoch  333  :  1.55177403379 [0.29442284] [0.36869416]\n",
      "loss on epoch  334  :  1.55192544504 [0.29442284] [0.36927068]\n",
      "loss on epoch  335  :  1.55076078795 [0.29442284] [0.36984721]\n",
      "loss on epoch  336  :  1.55044410185 [0.29571983] [0.37042376]\n",
      "loss on epoch  337  :  1.55072989508 [0.29442284] [0.37085617]\n",
      "loss on epoch  338  :  1.5496544838 [0.29571983] [0.37128857]\n",
      "loss on epoch  339  :  1.54969686932 [0.29701686] [0.37186509]\n",
      "loss on epoch  340  :  1.5491190928 [0.29701686] [0.37215337]\n",
      "loss on epoch  341  :  1.54895950688 [0.29701686] [0.37258577]\n",
      "loss on epoch  342  :  1.54785650306 [0.30090791] [0.3731623]\n",
      "loss on epoch  343  :  1.54695721467 [0.29831389] [0.3731623]\n",
      "loss on epoch  344  :  1.54689177098 [0.29831389] [0.3731623]\n",
      "loss on epoch  345  :  1.54643728336 [0.29961088] [0.37330642]\n",
      "loss on epoch  346  :  1.5454476564 [0.29961088] [0.37373883]\n",
      "loss on epoch  347  :  1.54521634181 [0.29961088] [0.37431535]\n",
      "loss on epoch  348  :  1.5450263487 [0.29961088] [0.37445951]\n",
      "loss on epoch  349  :  1.54375784265 [0.29961088] [0.37489191]\n",
      "loss on epoch  350  :  1.54319658765 [0.29831389] [0.37489191]\n",
      "loss on epoch  351  :  1.54288693048 [0.29961088] [0.37474775]\n",
      "loss on epoch  352  :  1.54375491319 [0.29831389] [0.37532431]\n",
      "loss on epoch  353  :  1.541084071 [0.29701686] [0.37575671]\n",
      "loss on epoch  354  :  1.54125170796 [0.29701686] [0.37546843]\n",
      "loss on epoch  355  :  1.54153931141 [0.29961088] [0.37575671]\n",
      "loss on epoch  356  :  1.5407498523 [0.29961088] [0.37590083]\n",
      "loss on epoch  357  :  1.54090600985 [0.29831389] [0.37590083]\n",
      "loss on epoch  358  :  1.54022538883 [0.29831389] [0.37618911]\n",
      "loss on epoch  359  :  1.54001661583 [0.29701686] [0.37561256]\n",
      "loss on epoch  360  :  1.53870391183 [0.29831389] [0.37662151]\n",
      "loss on epoch  361  :  1.53874105657 [0.29701686] [0.37647736]\n",
      "loss on epoch  362  :  1.53826421941 [0.29701686] [0.37662151]\n",
      "loss on epoch  363  :  1.53781176276 [0.29701686] [0.37690976]\n",
      "loss on epoch  364  :  1.53722389539 [0.29701686] [0.37676564]\n",
      "loss on epoch  365  :  1.53714410464 [0.29701686] [0.37647736]\n",
      "loss on epoch  366  :  1.53551349154 [0.29701686] [0.37791872]\n",
      "loss on epoch  367  :  1.53454987208 [0.29831389] [0.37835112]\n",
      "loss on epoch  368  :  1.53531578514 [0.29701686] [0.37878352]\n",
      "loss on epoch  369  :  1.53497017754 [0.29701686] [0.37892765]\n",
      "loss on epoch  370  :  1.53376913291 [0.29701686] [0.37921593]\n",
      "loss on epoch  371  :  1.53453667517 [0.29571983] [0.37921593]\n",
      "loss on epoch  372  :  1.5328243419 [0.29571983] [0.37964833]\n",
      "loss on epoch  373  :  1.53335817434 [0.29571983] [0.37950417]\n",
      "loss on epoch  374  :  1.53359036313 [0.29571983] [0.38022485]\n",
      "loss on epoch  375  :  1.53240537643 [0.29571983] [0.38008073]\n",
      "loss on epoch  376  :  1.53143589585 [0.29571983] [0.3805131]\n",
      "loss on epoch  377  :  1.53145159615 [0.29701686] [0.38036898]\n",
      "loss on epoch  378  :  1.53107842693 [0.29701686] [0.38108966]\n",
      "loss on epoch  379  :  1.53068627914 [0.29701686] [0.38108966]\n",
      "loss on epoch  380  :  1.53050900168 [0.29571983] [0.38181031]\n",
      "loss on epoch  381  :  1.52997465928 [0.29571983] [0.38224271]\n",
      "loss on epoch  382  :  1.52838459942 [0.29571983] [0.38267511]\n",
      "loss on epoch  383  :  1.52894954107 [0.29571983] [0.38296339]\n",
      "loss on epoch  384  :  1.52867207704 [0.29571983] [0.38325167]\n",
      "loss on epoch  385  :  1.52836845981 [0.29571983] [0.38310751]\n",
      "loss on epoch  386  :  1.52700443842 [0.29571983] [0.38426059]\n",
      "loss on epoch  387  :  1.52718957707 [0.29571983] [0.38426059]\n",
      "loss on epoch  388  :  1.52646661467 [0.29571983] [0.38454887]\n",
      "loss on epoch  389  :  1.52644291189 [0.29442284] [0.38483712]\n",
      "loss on epoch  390  :  1.5249187284 [0.29571983] [0.38483712]\n",
      "loss on epoch  391  :  1.52500331402 [0.29571983] [0.3851254]\n",
      "loss on epoch  392  :  1.52351109849 [0.29571983] [0.38526952]\n",
      "loss on epoch  393  :  1.52481030756 [0.29442284] [0.3855578]\n",
      "loss on epoch  394  :  1.52368662092 [0.29442284] [0.3859902]\n",
      "loss on epoch  395  :  1.52180541665 [0.29831389] [0.38656673]\n",
      "loss on epoch  396  :  1.52369414215 [0.29571983] [0.38613433]\n",
      "loss on epoch  397  :  1.52269369585 [0.29571983] [0.38584608]\n",
      "loss on epoch  398  :  1.52190430518 [0.29961088] [0.38671088]\n",
      "loss on epoch  399  :  1.52194876362 [0.29831389] [0.38699913]\n",
      "loss on epoch  400  :  1.5212539898 [0.29571983] [0.38728741]\n",
      "loss on epoch  401  :  1.52092206478 [0.29701686] [0.38728741]\n",
      "loss on epoch  402  :  1.52016100398 [0.29701686] [0.38786393]\n",
      "loss on epoch  403  :  1.52045105343 [0.29701686] [0.38800806]\n",
      "loss on epoch  404  :  1.51973575795 [0.29831389] [0.38858461]\n",
      "loss on epoch  405  :  1.51908392156 [0.29701686] [0.38872874]\n",
      "loss on epoch  406  :  1.51889734577 [0.29961088] [0.38844046]\n",
      "loss on epoch  407  :  1.51815952195 [0.29961088] [0.38887286]\n",
      "loss on epoch  408  :  1.51768401155 [0.29961088] [0.38916114]\n",
      "loss on epoch  409  :  1.51738987587 [0.29961088] [0.38988182]\n",
      "loss on epoch  410  :  1.51775712216 [0.29961088] [0.39002594]\n",
      "loss on epoch  411  :  1.51626410087 [0.30220494] [0.39031422]\n",
      "loss on epoch  412  :  1.51577691016 [0.29961088] [0.39002594]\n",
      "loss on epoch  413  :  1.51540520015 [0.30220494] [0.39002594]\n",
      "loss on epoch  414  :  1.51491748404 [0.30220494] [0.39031422]\n",
      "loss on epoch  415  :  1.51530283469 [0.30220494] [0.39045835]\n",
      "loss on epoch  416  :  1.51436346328 [0.30220494] [0.39103487]\n",
      "loss on epoch  417  :  1.51412168697 [0.30220494] [0.39132315]\n",
      "loss on epoch  418  :  1.5131509878 [0.30220494] [0.39146727]\n",
      "loss on epoch  419  :  1.51410573721 [0.30090791] [0.39189968]\n",
      "loss on epoch  420  :  1.51149458135 [0.30090791] [0.39189968]\n",
      "loss on epoch  421  :  1.51297786721 [0.29961088] [0.39204383]\n",
      "loss on epoch  422  :  1.51183008485 [0.29961088] [0.39233208]\n",
      "loss on epoch  423  :  1.51102940462 [0.29831389] [0.39233208]\n",
      "loss on epoch  424  :  1.51085515817 [0.29831389] [0.39262035]\n",
      "loss on epoch  425  :  1.51111165241 [0.29831389] [0.39247623]\n",
      "loss on epoch  426  :  1.51047801751 [0.29831389] [0.39290863]\n",
      "loss on epoch  427  :  1.5097983612 [0.29831389] [0.39348516]\n",
      "loss on epoch  428  :  1.50961966206 [0.29831389] [0.39319688]\n",
      "loss on epoch  429  :  1.50892756824 [0.29831389] [0.39290863]\n",
      "loss on epoch  430  :  1.50820990183 [0.29831389] [0.39348516]\n",
      "loss on epoch  431  :  1.50867048237 [0.29701686] [0.39319688]\n",
      "loss on epoch  432  :  1.50717601732 [0.29701686] [0.39348516]\n",
      "loss on epoch  433  :  1.50760819735 [0.29831389] [0.39348516]\n",
      "loss on epoch  434  :  1.50678776812 [0.29701686] [0.39391756]\n",
      "loss on epoch  435  :  1.5066146983 [0.29831389] [0.39420581]\n",
      "loss on epoch  436  :  1.50600479267 [0.29701686] [0.39463821]\n",
      "loss on epoch  437  :  1.50607792978 [0.29831389] [0.39406168]\n",
      "loss on epoch  438  :  1.50536457918 [0.29701686] [0.39478236]\n",
      "loss on epoch  439  :  1.5050940911 [0.29831389] [0.39463821]\n",
      "loss on epoch  440  :  1.50483140018 [0.29831389] [0.39449409]\n",
      "loss on epoch  441  :  1.50460133067 [0.29831389] [0.39507061]\n",
      "loss on epoch  442  :  1.50249472592 [0.29831389] [0.39550301]\n",
      "loss on epoch  443  :  1.50258431611 [0.29831389] [0.39579129]\n",
      "loss on epoch  444  :  1.50263162233 [0.29831389] [0.39521477]\n",
      "loss on epoch  445  :  1.50122880936 [0.29831389] [0.39535889]\n",
      "loss on epoch  446  :  1.50165340194 [0.29701686] [0.39521477]\n",
      "loss on epoch  447  :  1.50245543983 [0.29701686] [0.39564717]\n",
      "loss on epoch  448  :  1.50107693672 [0.29831389] [0.39607957]\n",
      "loss on epoch  449  :  1.50051118489 [0.29961088] [0.39607957]\n",
      "loss on epoch  450  :  1.49986687634 [0.29701686] [0.39651197]\n",
      "loss on epoch  451  :  1.4999719814 [0.29701686] [0.3970885]\n",
      "loss on epoch  452  :  1.49866338792 [0.29701686] [0.39737678]\n",
      "loss on epoch  453  :  1.49935275095 [0.29831389] [0.39766502]\n",
      "loss on epoch  454  :  1.49914392277 [0.29701686] [0.39766502]\n",
      "loss on epoch  455  :  1.49785868106 [0.29831389] [0.3979533]\n",
      "loss on epoch  456  :  1.49798751098 [0.29831389] [0.3979533]\n",
      "loss on epoch  457  :  1.49795931357 [0.29831389] [0.3983857]\n",
      "loss on epoch  458  :  1.49656929793 [0.29961088] [0.39896223]\n",
      "loss on epoch  459  :  1.49648748945 [0.29961088] [0.39896223]\n",
      "loss on epoch  460  :  1.49601405638 [0.29961088] [0.39910638]\n",
      "loss on epoch  461  :  1.49610789396 [0.29961088] [0.39953879]\n",
      "loss on epoch  462  :  1.49555834797 [0.29961088] [0.39953879]\n",
      "loss on epoch  463  :  1.4943095269 [0.29961088] [0.39953879]\n",
      "loss on epoch  464  :  1.49385539249 [0.29961088] [0.40011531]\n",
      "loss on epoch  465  :  1.49385777226 [0.29961088] [0.39997119]\n",
      "loss on epoch  466  :  1.49344126163 [0.29961088] [0.40069184]\n",
      "loss on epoch  467  :  1.49338656664 [0.29961088] [0.40112424]\n",
      "loss on epoch  468  :  1.49271950898 [0.29961088] [0.40198904]\n",
      "loss on epoch  469  :  1.49267762237 [0.30090791] [0.40256557]\n",
      "loss on epoch  470  :  1.49247542796 [0.30090791] [0.40299797]\n",
      "loss on epoch  471  :  1.49120945401 [0.30090791] [0.40285385]\n",
      "loss on epoch  472  :  1.49105508018 [0.30090791] [0.40299797]\n",
      "loss on epoch  473  :  1.49084388106 [0.30220494] [0.40299797]\n",
      "loss on epoch  474  :  1.49152772956 [0.30220494] [0.40357453]\n",
      "loss on epoch  475  :  1.491060619 [0.30090791] [0.40314212]\n",
      "loss on epoch  476  :  1.4894179172 [0.30090791] [0.40328625]\n",
      "loss on epoch  477  :  1.48957617857 [0.30090791] [0.40343037]\n",
      "loss on epoch  478  :  1.4892763363 [0.30220494] [0.40299797]\n",
      "loss on epoch  479  :  1.48902332562 [0.30090791] [0.40343037]\n",
      "loss on epoch  480  :  1.4884964912 [0.30350193] [0.40328625]\n",
      "loss on epoch  481  :  1.48735129833 [0.30350193] [0.40371865]\n",
      "loss on epoch  482  :  1.48730965014 [0.30220494] [0.40400693]\n",
      "loss on epoch  483  :  1.48674611471 [0.30220494] [0.40429518]\n",
      "loss on epoch  484  :  1.4866145209 [0.30220494] [0.40458345]\n",
      "loss on epoch  485  :  1.48576077267 [0.30350193] [0.40443933]\n",
      "loss on epoch  486  :  1.48514200581 [0.30090791] [0.40487173]\n",
      "loss on epoch  487  :  1.48544690344 [0.30220494] [0.40515998]\n",
      "loss on epoch  488  :  1.48519441154 [0.30220494] [0.40573654]\n",
      "loss on epoch  489  :  1.48463872406 [0.30220494] [0.40573654]\n",
      "loss on epoch  490  :  1.4843160201 [0.30090791] [0.40616894]\n",
      "loss on epoch  491  :  1.48498996982 [0.30090791] [0.40616894]\n",
      "loss on epoch  492  :  1.48298860921 [0.30350193] [0.40631306]\n",
      "loss on epoch  493  :  1.48322760838 [0.30479896] [0.40616894]\n",
      "loss on epoch  494  :  1.48268895017 [0.30350193] [0.40645719]\n",
      "loss on epoch  495  :  1.48306445281 [0.30350193] [0.40703374]\n",
      "loss on epoch  496  :  1.48181039757 [0.30350193] [0.40746614]\n",
      "loss on epoch  497  :  1.48158174091 [0.30350193] [0.40688959]\n",
      "loss on epoch  498  :  1.48073724023 [0.30220494] [0.40703374]\n",
      "loss on epoch  499  :  1.48047624473 [0.30350193] [0.40746614]\n",
      "loss on epoch  500  :  1.48075893632 [0.30350193] [0.40804267]\n",
      "loss on epoch  501  :  1.4786136614 [0.30350193] [0.40818679]\n",
      "loss on epoch  502  :  1.478976806 [0.30350193] [0.4086192]\n",
      "loss on epoch  503  :  1.47916612802 [0.30350193] [0.4090516]\n",
      "loss on epoch  504  :  1.47856002163 [0.30350193] [0.40919572]\n",
      "loss on epoch  505  :  1.4792936665 [0.30350193] [0.40933987]\n",
      "loss on epoch  506  :  1.47652054054 [0.30350193] [0.40933987]\n",
      "loss on epoch  507  :  1.47759374204 [0.30350193] [0.41006052]\n",
      "loss on epoch  508  :  1.47734618408 [0.30350193] [0.40962812]\n",
      "loss on epoch  509  :  1.47665993814 [0.30220494] [0.41020468]\n",
      "loss on epoch  510  :  1.47715320411 [0.30220494] [0.41020468]\n",
      "loss on epoch  511  :  1.47590920219 [0.30220494] [0.41063708]\n",
      "loss on epoch  512  :  1.47490287269 [0.30220494] [0.41092533]\n",
      "loss on epoch  513  :  1.47435109041 [0.30220494] [0.41063708]\n",
      "loss on epoch  514  :  1.47518721554 [0.30220494] [0.41150188]\n",
      "loss on epoch  515  :  1.47450895442 [0.30220494] [0.41135773]\n",
      "loss on epoch  516  :  1.47414802622 [0.30220494] [0.41135773]\n",
      "loss on epoch  517  :  1.4745536292 [0.30350193] [0.41164601]\n",
      "loss on epoch  518  :  1.47352356822 [0.30350193] [0.41207841]\n",
      "loss on epoch  519  :  1.47311845753 [0.30350193] [0.41251081]\n",
      "loss on epoch  520  :  1.47237281667 [0.30220494] [0.41308734]\n",
      "loss on epoch  521  :  1.47250149647 [0.30220494] [0.41323149]\n",
      "loss on epoch  522  :  1.47075670737 [0.30220494] [0.41366389]\n",
      "loss on epoch  523  :  1.47132346144 [0.30350193] [0.41380802]\n",
      "loss on epoch  524  :  1.47165044811 [0.30479896] [0.41395214]\n",
      "loss on epoch  525  :  1.47122038515 [0.30350193] [0.4140963]\n",
      "loss on epoch  526  :  1.47020349238 [0.30350193] [0.41438454]\n",
      "loss on epoch  527  :  1.46984488214 [0.30609599] [0.41424042]\n",
      "loss on epoch  528  :  1.46898733024 [0.30609599] [0.41467282]\n",
      "loss on epoch  529  :  1.46957928163 [0.30350193] [0.41510522]\n",
      "loss on epoch  530  :  1.46883345092 [0.30609599] [0.41481695]\n",
      "loss on epoch  531  :  1.46894560037 [0.30609599] [0.41582587]\n",
      "loss on epoch  532  :  1.46818341811 [0.30479896] [0.41625828]\n",
      "loss on epoch  533  :  1.46798757491 [0.30609599] [0.41611415]\n",
      "loss on epoch  534  :  1.46776390076 [0.30479896] [0.41683483]\n",
      "loss on epoch  535  :  1.46760950707 [0.30609599] [0.41669068]\n",
      "loss on epoch  536  :  1.46645000687 [0.30609599] [0.41683483]\n",
      "loss on epoch  537  :  1.46584363558 [0.30479896] [0.41712308]\n",
      "loss on epoch  538  :  1.46637991623 [0.30869001] [0.41726723]\n",
      "loss on epoch  539  :  1.46558943501 [0.30609599] [0.41755548]\n",
      "loss on epoch  540  :  1.46514512654 [0.30609599] [0.41784376]\n",
      "loss on epoch  541  :  1.4652220077 [0.30479896] [0.41827616]\n",
      "loss on epoch  542  :  1.46427229819 [0.30350193] [0.41827616]\n",
      "loss on epoch  543  :  1.46360237731 [0.30479896] [0.41856444]\n",
      "loss on epoch  544  :  1.46428121902 [0.30350193] [0.41885269]\n",
      "loss on epoch  545  :  1.46397783138 [0.30479896] [0.41957337]\n",
      "loss on epoch  546  :  1.46315103769 [0.30479896] [0.41914096]\n",
      "loss on epoch  547  :  1.46279725543 [0.30479896] [0.41899684]\n",
      "loss on epoch  548  :  1.46204381077 [0.30609599] [0.41942924]\n",
      "loss on epoch  549  :  1.46157791879 [0.30479896] [0.42000577]\n",
      "loss on epoch  550  :  1.46114827085 [0.30609599] [0.41971749]\n",
      "loss on epoch  551  :  1.46043635739 [0.30479896] [0.42029405]\n",
      "loss on epoch  552  :  1.45916551572 [0.30479896] [0.42058229]\n",
      "loss on epoch  553  :  1.45933708659 [0.30479896] [0.42043817]\n",
      "loss on epoch  554  :  1.460379797 [0.30479896] [0.42115885]\n",
      "loss on epoch  555  :  1.45999850167 [0.30479896] [0.42087057]\n",
      "loss on epoch  556  :  1.45879307941 [0.30479896] [0.42159122]\n",
      "loss on epoch  557  :  1.4583797212 [0.30609599] [0.42216778]\n",
      "loss on epoch  558  :  1.4584214886 [0.30609599] [0.42245603]\n",
      "loss on epoch  559  :  1.45707108136 [0.30479896] [0.4227443]\n",
      "loss on epoch  560  :  1.4577746877 [0.30609599] [0.42288843]\n",
      "loss on epoch  561  :  1.45711544929 [0.30739298] [0.42360911]\n",
      "loss on epoch  562  :  1.45682459628 [0.30479896] [0.42360911]\n",
      "loss on epoch  563  :  1.45659100568 [0.30609599] [0.42360911]\n",
      "loss on epoch  564  :  1.456700729 [0.30479896] [0.42389739]\n",
      "loss on epoch  565  :  1.45643886151 [0.30479896] [0.42404151]\n",
      "loss on epoch  566  :  1.45582547011 [0.30609599] [0.42375323]\n",
      "loss on epoch  567  :  1.45521108751 [0.30479896] [0.42461804]\n",
      "loss on epoch  568  :  1.45571364518 [0.30479896] [0.42447391]\n",
      "loss on epoch  569  :  1.45504347704 [0.30350193] [0.42490631]\n",
      "loss on epoch  570  :  1.45366446177 [0.30479896] [0.42548284]\n",
      "loss on epoch  571  :  1.45333419243 [0.30350193] [0.42519459]\n",
      "loss on epoch  572  :  1.45354746448 [0.30350193] [0.42591524]\n",
      "loss on epoch  573  :  1.45267350365 [0.30350193] [0.42591524]\n",
      "loss on epoch  574  :  1.45260820565 [0.30479896] [0.42577112]\n",
      "loss on epoch  575  :  1.45273784796 [0.30350193] [0.42620352]\n",
      "loss on epoch  576  :  1.45230904332 [0.30350193] [0.42678005]\n",
      "loss on epoch  577  :  1.45145901706 [0.30220494] [0.42663592]\n",
      "loss on epoch  578  :  1.45170807618 [0.30220494] [0.4273566]\n",
      "loss on epoch  579  :  1.45074486291 [0.30220494] [0.42750072]\n",
      "loss on epoch  580  :  1.44983686341 [0.30220494] [0.42850965]\n",
      "loss on epoch  581  :  1.45057106239 [0.30220494] [0.42894205]\n",
      "loss on epoch  582  :  1.4502205076 [0.30220494] [0.42879793]\n",
      "loss on epoch  583  :  1.44923660049 [0.30220494] [0.42923033]\n",
      "loss on epoch  584  :  1.44951579306 [0.30220494] [0.42995098]\n",
      "loss on epoch  585  :  1.44825431373 [0.30220494] [0.42937446]\n",
      "loss on epoch  586  :  1.44875314942 [0.30220494] [0.42966273]\n",
      "loss on epoch  587  :  1.44888134797 [0.30220494] [0.43110406]\n",
      "loss on epoch  588  :  1.44793606025 [0.30220494] [0.43067166]\n",
      "loss on epoch  589  :  1.44750756688 [0.30220494] [0.43139234]\n",
      "loss on epoch  590  :  1.44692940182 [0.30220494] [0.43196887]\n",
      "loss on epoch  591  :  1.44675930341 [0.30220494] [0.43139234]\n",
      "loss on epoch  592  :  1.44623762369 [0.30220494] [0.43196887]\n",
      "loss on epoch  593  :  1.44563113539 [0.30220494] [0.43153647]\n",
      "loss on epoch  594  :  1.44584295926 [0.30220494] [0.43268955]\n",
      "loss on epoch  595  :  1.44604023077 [0.30220494] [0.43312195]\n",
      "loss on epoch  596  :  1.44482555213 [0.30350193] [0.4334102]\n",
      "loss on epoch  597  :  1.44475165782 [0.30350193] [0.43355435]\n",
      "loss on epoch  598  :  1.44298649055 [0.30479896] [0.43312195]\n",
      "loss on epoch  599  :  1.44476014155 [0.30479896] [0.43369848]\n",
      "loss on epoch  600  :  1.44377723447 [0.30479896] [0.43326607]\n",
      "loss on epoch  601  :  1.44334902145 [0.30479896] [0.43355435]\n",
      "loss on epoch  602  :  1.44274909629 [0.30479896] [0.434275]\n",
      "loss on epoch  603  :  1.44182431256 [0.30479896] [0.43326607]\n",
      "loss on epoch  604  :  1.4416498012 [0.30350193] [0.434275]\n",
      "loss on epoch  605  :  1.44205195374 [0.30350193] [0.43413088]\n",
      "loss on epoch  606  :  1.44122628812 [0.30350193] [0.43456328]\n",
      "loss on epoch  607  :  1.440985322 [0.30479896] [0.43499568]\n",
      "loss on epoch  608  :  1.44125691608 [0.30350193] [0.43542808]\n",
      "loss on epoch  609  :  1.43991333025 [0.30350193] [0.43542808]\n",
      "loss on epoch  610  :  1.44055300951 [0.30350193] [0.43542808]\n",
      "loss on epoch  611  :  1.44013393808 [0.30350193] [0.43557221]\n",
      "loss on epoch  612  :  1.4401314965 [0.30350193] [0.43629289]\n",
      "loss on epoch  613  :  1.43898557513 [0.30350193] [0.43701354]\n",
      "loss on epoch  614  :  1.43892538768 [0.30350193] [0.43773422]\n",
      "loss on epoch  615  :  1.4379928112 [0.30350193] [0.43759009]\n",
      "loss on epoch  616  :  1.43788746993 [0.30350193] [0.43802249]\n",
      "loss on epoch  617  :  1.43724475084 [0.30350193] [0.43859902]\n",
      "loss on epoch  618  :  1.43637310796 [0.30350193] [0.4384549]\n",
      "loss on epoch  619  :  1.4360459182 [0.30350193] [0.43859902]\n",
      "loss on epoch  620  :  1.43642658437 [0.30350193] [0.43816662]\n",
      "loss on epoch  621  :  1.43597278772 [0.30350193] [0.43874314]\n",
      "loss on epoch  622  :  1.43551982332 [0.30220494] [0.43903142]\n",
      "loss on epoch  623  :  1.43598457177 [0.30220494] [0.43903142]\n",
      "loss on epoch  624  :  1.43575153307 [0.30220494] [0.43917555]\n",
      "loss on epoch  625  :  1.43438764413 [0.30090791] [0.4393197]\n",
      "loss on epoch  626  :  1.43416091689 [0.30350193] [0.4393197]\n",
      "loss on epoch  627  :  1.43402261646 [0.30350193] [0.43946382]\n",
      "loss on epoch  628  :  1.43301471737 [0.30220494] [0.43946382]\n",
      "loss on epoch  629  :  1.43319657997 [0.30220494] [0.4397521]\n",
      "loss on epoch  630  :  1.43351550897 [0.30090791] [0.43946382]\n",
      "loss on epoch  631  :  1.43257804491 [0.30090791] [0.44004035]\n",
      "loss on epoch  632  :  1.43189992949 [0.30090791] [0.44076103]\n",
      "loss on epoch  633  :  1.43185625694 [0.30090791] [0.44090515]\n",
      "loss on epoch  634  :  1.4317213915 [0.30220494] [0.44104931]\n",
      "loss on epoch  635  :  1.43129462887 [0.30220494] [0.44090515]\n",
      "loss on epoch  636  :  1.430668577 [0.30090791] [0.44104931]\n",
      "loss on epoch  637  :  1.43059311973 [0.30090791] [0.44090515]\n",
      "loss on epoch  638  :  1.42963649388 [0.30220494] [0.44119343]\n",
      "loss on epoch  639  :  1.42942785996 [0.30220494] [0.44133756]\n",
      "loss on epoch  640  :  1.43017754511 [0.30090791] [0.44119343]\n",
      "loss on epoch  641  :  1.42969537223 [0.30090791] [0.44191408]\n",
      "loss on epoch  642  :  1.42820355627 [0.30090791] [0.44191408]\n",
      "loss on epoch  643  :  1.42838075647 [0.30090791] [0.44162583]\n",
      "loss on epoch  644  :  1.42779583843 [0.30090791] [0.44220236]\n",
      "loss on epoch  645  :  1.42786945458 [0.30090791] [0.44220236]\n",
      "loss on epoch  646  :  1.42740938619 [0.30090791] [0.44263476]\n",
      "loss on epoch  647  :  1.42757245567 [0.30090791] [0.44321129]\n",
      "loss on epoch  648  :  1.42645971422 [0.30090791] [0.44335544]\n",
      "loss on epoch  649  :  1.42626198795 [0.30220494] [0.44292304]\n",
      "loss on epoch  650  :  1.42683798075 [0.30090791] [0.44349957]\n",
      "loss on epoch  651  :  1.42614369922 [0.29961088] [0.44407609]\n",
      "loss on epoch  652  :  1.42532556808 [0.29961088] [0.44450849]\n",
      "loss on epoch  653  :  1.42384906168 [0.30090791] [0.44378784]\n",
      "loss on epoch  654  :  1.42498228506 [0.29961088] [0.44450849]\n",
      "loss on epoch  655  :  1.42458405539 [0.30090791] [0.44450849]\n",
      "loss on epoch  656  :  1.42389374088 [0.29961088] [0.44508505]\n",
      "loss on epoch  657  :  1.42405594058 [0.30220494] [0.44522917]\n",
      "loss on epoch  658  :  1.42368621517 [0.30090791] [0.44551745]\n",
      "loss on epoch  659  :  1.42358479456 [0.30220494] [0.44566157]\n",
      "loss on epoch  660  :  1.42231876541 [0.30350193] [0.44566157]\n",
      "loss on epoch  661  :  1.42227991422 [0.30220494] [0.44609398]\n",
      "loss on epoch  662  :  1.42211901038 [0.30350193] [0.44652638]\n",
      "loss on epoch  663  :  1.42184129909 [0.30220494] [0.44695878]\n",
      "loss on epoch  664  :  1.4210631141 [0.30350193] [0.4471029]\n",
      "loss on epoch  665  :  1.42021113634 [0.30350193] [0.4471029]\n",
      "loss on epoch  666  :  1.42107233295 [0.30350193] [0.44681466]\n",
      "loss on epoch  667  :  1.42080000816 [0.29961088] [0.44753531]\n",
      "loss on epoch  668  :  1.41971797634 [0.29961088] [0.44753531]\n",
      "loss on epoch  669  :  1.41869905701 [0.30220494] [0.4471029]\n",
      "loss on epoch  670  :  1.4195256763 [0.30090791] [0.44767946]\n",
      "loss on epoch  671  :  1.41908604348 [0.29961088] [0.44782358]\n",
      "loss on epoch  672  :  1.41823363525 [0.30479896] [0.44811186]\n",
      "loss on epoch  673  :  1.41854550441 [0.30090791] [0.44825599]\n",
      "loss on epoch  674  :  1.41816931521 [0.29961088] [0.44753531]\n",
      "loss on epoch  675  :  1.41803317158 [0.30090791] [0.44811186]\n",
      "loss on epoch  676  :  1.41670097245 [0.29961088] [0.44782358]\n",
      "loss on epoch  677  :  1.41640356293 [0.30220494] [0.44840011]\n",
      "loss on epoch  678  :  1.41641671349 [0.30350193] [0.44883251]\n",
      "loss on epoch  679  :  1.41627845499 [0.30220494] [0.44883251]\n",
      "loss on epoch  680  :  1.41575837135 [0.30220494] [0.44868839]\n",
      "loss on epoch  681  :  1.41514773501 [0.29831389] [0.44811186]\n",
      "loss on epoch  682  :  1.41449930712 [0.30220494] [0.44926491]\n",
      "loss on epoch  683  :  1.4148518112 [0.30350193] [0.44926491]\n",
      "loss on epoch  684  :  1.41579145635 [0.30350193] [0.44940904]\n",
      "loss on epoch  685  :  1.41450576871 [0.30479896] [0.44969732]\n",
      "loss on epoch  686  :  1.41418216405 [0.30609599] [0.44969732]\n",
      "loss on epoch  687  :  1.4144161763 [0.30479896] [0.44984144]\n",
      "loss on epoch  688  :  1.41251260484 [0.30479896] [0.45012972]\n",
      "loss on epoch  689  :  1.41327795055 [0.30220494] [0.44955319]\n",
      "loss on epoch  690  :  1.41312214401 [0.30350193] [0.44912079]\n",
      "loss on epoch  691  :  1.41222936798 [0.30609599] [0.45012972]\n",
      "loss on epoch  692  :  1.41230633303 [0.30609599] [0.44984144]\n",
      "loss on epoch  693  :  1.41160785048 [0.30479896] [0.45056212]\n",
      "loss on epoch  694  :  1.41219838019 [0.30609599] [0.45056212]\n",
      "loss on epoch  695  :  1.4106436151 [0.30479896] [0.45099452]\n",
      "loss on epoch  696  :  1.4103857632 [0.30609599] [0.45142692]\n",
      "loss on epoch  697  :  1.4107040498 [0.30609599] [0.45157105]\n",
      "loss on epoch  698  :  1.4091791047 [0.30609599] [0.45142692]\n",
      "loss on epoch  699  :  1.41053795594 [0.30609599] [0.45157105]\n",
      "loss on epoch  700  :  1.40948261817 [0.30609599] [0.4517152]\n",
      "loss on epoch  701  :  1.40876632929 [0.30609599] [0.45185933]\n",
      "loss on epoch  702  :  1.40812891501 [0.30609599] [0.45157105]\n",
      "loss on epoch  703  :  1.40873083583 [0.30609599] [0.45229173]\n",
      "loss on epoch  704  :  1.40834992241 [0.30609599] [0.45229173]\n",
      "loss on epoch  705  :  1.40717472191 [0.30609599] [0.45243585]\n",
      "loss on epoch  706  :  1.40685463835 [0.30609599] [0.45258]\n",
      "loss on epoch  707  :  1.40738527422 [0.30609599] [0.45301241]\n",
      "loss on epoch  708  :  1.40752496984 [0.30609599] [0.45258]\n",
      "loss on epoch  709  :  1.40612505321 [0.30609599] [0.45272413]\n",
      "loss on epoch  710  :  1.40612715703 [0.30609599] [0.45315653]\n",
      "loss on epoch  711  :  1.40469662128 [0.30609599] [0.45301241]\n",
      "loss on epoch  712  :  1.40576813839 [0.30479896] [0.45330065]\n",
      "loss on epoch  713  :  1.4050346348 [0.30609599] [0.45315653]\n",
      "loss on epoch  714  :  1.40528338265 [0.30609599] [0.45315653]\n",
      "loss on epoch  715  :  1.40446536188 [0.30609599] [0.45358893]\n",
      "loss on epoch  716  :  1.40388944855 [0.30479896] [0.45358893]\n",
      "loss on epoch  717  :  1.40332324196 [0.30479896] [0.45330065]\n",
      "loss on epoch  718  :  1.40366323127 [0.30609599] [0.45330065]\n",
      "loss on epoch  719  :  1.40283049257 [0.30609599] [0.45373306]\n",
      "loss on epoch  720  :  1.40279133894 [0.30479896] [0.45387721]\n",
      "loss on epoch  721  :  1.40298692385 [0.30609599] [0.45344481]\n",
      "loss on epoch  722  :  1.40195982103 [0.30739298] [0.45459786]\n",
      "loss on epoch  723  :  1.40173833017 [0.30739298] [0.45430961]\n",
      "loss on epoch  724  :  1.40132168487 [0.30739298] [0.45416546]\n",
      "loss on epoch  725  :  1.40128081375 [0.30739298] [0.45445374]\n",
      "loss on epoch  726  :  1.40108099911 [0.30739298] [0.45503026]\n",
      "loss on epoch  727  :  1.40032056084 [0.30739298] [0.45488614]\n",
      "loss on epoch  728  :  1.39977610994 [0.30998704] [0.45546266]\n",
      "loss on epoch  729  :  1.39875929665 [0.30739298] [0.45503026]\n",
      "loss on epoch  730  :  1.39909772078 [0.30869001] [0.45560679]\n",
      "loss on epoch  731  :  1.39923743186 [0.30998704] [0.45517442]\n",
      "loss on epoch  732  :  1.39830057047 [0.30998704] [0.45560679]\n",
      "loss on epoch  733  :  1.39861603136 [0.30869001] [0.45575094]\n",
      "loss on epoch  734  :  1.39815079283 [0.31128404] [0.45589507]\n",
      "loss on epoch  735  :  1.39755853238 [0.30998704] [0.45589507]\n",
      "loss on epoch  736  :  1.39748330911 [0.30869001] [0.45603919]\n",
      "loss on epoch  737  :  1.39671997229 [0.30998704] [0.45661575]\n",
      "loss on epoch  738  :  1.39647234811 [0.30998704] [0.45603919]\n",
      "loss on epoch  739  :  1.39622517427 [0.30998704] [0.45647159]\n",
      "loss on epoch  740  :  1.39575358894 [0.31128404] [0.4573364]\n",
      "loss on epoch  741  :  1.39577860082 [0.31128404] [0.45690399]\n",
      "loss on epoch  742  :  1.39493408468 [0.30998704] [0.45675987]\n",
      "loss on epoch  743  :  1.394910936 [0.31128404] [0.4573364]\n",
      "loss on epoch  744  :  1.39476417612 [0.31128404] [0.45704815]\n",
      "loss on epoch  745  :  1.39493334514 [0.31258106] [0.45719227]\n",
      "loss on epoch  746  :  1.39331133057 [0.31258106] [0.4573364]\n",
      "loss on epoch  747  :  1.39391605942 [0.31128404] [0.45791295]\n",
      "loss on epoch  748  :  1.39299418087 [0.31128404] [0.45834535]\n",
      "loss on epoch  749  :  1.39305303273 [0.31128404] [0.4586336]\n",
      "loss on epoch  750  :  1.39234467127 [0.31258106] [0.4582012]\n",
      "loss on epoch  751  :  1.39198251345 [0.31517509] [0.45805708]\n",
      "loss on epoch  752  :  1.39207579251 [0.31128404] [0.45805708]\n",
      "loss on epoch  753  :  1.39225363731 [0.31387809] [0.4582012]\n",
      "loss on epoch  754  :  1.3906995345 [0.31258106] [0.459066]\n",
      "loss on epoch  755  :  1.39071363211 [0.31387809] [0.45935428]\n",
      "loss on epoch  756  :  1.39103663409 [0.31387809] [0.45921016]\n",
      "loss on epoch  757  :  1.39094761566 [0.31258106] [0.45978668]\n",
      "loss on epoch  758  :  1.39054744553 [0.31387809] [0.45993081]\n",
      "loss on epoch  759  :  1.3899747641 [0.31258106] [0.46007496]\n",
      "loss on epoch  760  :  1.39006541835 [0.31387809] [0.46036321]\n",
      "loss on epoch  761  :  1.38892223438 [0.31128404] [0.46065149]\n",
      "loss on epoch  762  :  1.3886043915 [0.31128404] [0.46050736]\n",
      "loss on epoch  763  :  1.38725083183 [0.31128404] [0.46036321]\n",
      "loss on epoch  764  :  1.38836953596 [0.31258106] [0.46050736]\n",
      "loss on epoch  765  :  1.38758115636 [0.31517509] [0.46108389]\n",
      "loss on epoch  766  :  1.38776228384 [0.31387809] [0.46122801]\n",
      "loss on epoch  767  :  1.38752176806 [0.31647211] [0.46151629]\n",
      "loss on epoch  768  :  1.38713497806 [0.31517509] [0.46180457]\n",
      "loss on epoch  769  :  1.38711833071 [0.31647211] [0.4628135]\n",
      "loss on epoch  770  :  1.3862177266 [0.31517509] [0.46266934]\n",
      "loss on epoch  771  :  1.3860020792 [0.31387809] [0.46252522]\n",
      "loss on epoch  772  :  1.38431455692 [0.31517509] [0.46310174]\n",
      "loss on epoch  773  :  1.38486174301 [0.31517509] [0.46353415]\n",
      "loss on epoch  774  :  1.38456643732 [0.31517509] [0.46339002]\n",
      "loss on epoch  775  :  1.38524828575 [0.31517509] [0.4636783]\n",
      "loss on epoch  776  :  1.38436015226 [0.31517509] [0.46339002]\n",
      "loss on epoch  777  :  1.38461657144 [0.31387809] [0.46382242]\n",
      "loss on epoch  778  :  1.38332808901 [0.31387809] [0.46353415]\n",
      "loss on epoch  779  :  1.38360434108 [0.31387809] [0.4636783]\n",
      "loss on epoch  780  :  1.38299288352 [0.31387809] [0.46382242]\n",
      "loss on epoch  781  :  1.38326912456 [0.31387809] [0.46425483]\n",
      "loss on epoch  782  :  1.38264188943 [0.31387809] [0.4641107]\n",
      "loss on epoch  783  :  1.3820101972 [0.31387809] [0.46425483]\n",
      "loss on epoch  784  :  1.38214193671 [0.31517509] [0.4645431]\n",
      "loss on epoch  785  :  1.38063854641 [0.31387809] [0.46483135]\n",
      "loss on epoch  786  :  1.38130243619 [0.31387809] [0.46555203]\n",
      "loss on epoch  787  :  1.38030373609 [0.31387809] [0.46526375]\n",
      "loss on epoch  788  :  1.38055922808 [0.31387809] [0.46497551]\n",
      "loss on epoch  789  :  1.3798114238 [0.31387809] [0.4645431]\n",
      "loss on epoch  790  :  1.37940429758 [0.31387809] [0.4645431]\n",
      "loss on epoch  791  :  1.38033360684 [0.31387809] [0.46526375]\n",
      "loss on epoch  792  :  1.37954260023 [0.31517509] [0.46555203]\n",
      "loss on epoch  793  :  1.3785464587 [0.31387809] [0.46526375]\n",
      "loss on epoch  794  :  1.3788255122 [0.31387809] [0.46584031]\n",
      "loss on epoch  795  :  1.37731050341 [0.31387809] [0.46612856]\n",
      "loss on epoch  796  :  1.37603349377 [0.31258106] [0.46670511]\n",
      "loss on epoch  797  :  1.37711152103 [0.31258106] [0.46641684]\n",
      "loss on epoch  798  :  1.37718283247 [0.31258106] [0.46684924]\n",
      "loss on epoch  799  :  1.37707690839 [0.31258106] [0.46670511]\n",
      "loss on epoch  800  :  1.37616578076 [0.31258106] [0.46728164]\n",
      "loss on epoch  801  :  1.37473938862 [0.31258106] [0.46684924]\n",
      "loss on epoch  802  :  1.37399646529 [0.31258106] [0.46742576]\n",
      "loss on epoch  803  :  1.37612980604 [0.31258106] [0.46756992]\n",
      "loss on epoch  804  :  1.37616578959 [0.30998704] [0.46771404]\n",
      "loss on epoch  805  :  1.37516699455 [0.31258106] [0.46829057]\n",
      "loss on epoch  806  :  1.37422483718 [0.31128404] [0.46843472]\n",
      "loss on epoch  807  :  1.37421646162 [0.31128404] [0.46829057]\n",
      "loss on epoch  808  :  1.3732915322 [0.30998704] [0.46901125]\n",
      "loss on epoch  809  :  1.37357523044 [0.31258106] [0.4692995]\n",
      "loss on epoch  810  :  1.37317557909 [0.31128404] [0.46915537]\n",
      "loss on epoch  811  :  1.37267811652 [0.31128404] [0.4697319]\n",
      "loss on epoch  812  :  1.37233501894 [0.31128404] [0.4701643]\n",
      "loss on epoch  813  :  1.37327496652 [0.31128404] [0.47045258]\n",
      "loss on epoch  814  :  1.37254868613 [0.31128404] [0.4705967]\n",
      "loss on epoch  815  :  1.37122619594 [0.31128404] [0.47045258]\n",
      "loss on epoch  816  :  1.37091181676 [0.31128404] [0.4705967]\n",
      "loss on epoch  817  :  1.37078805985 [0.31128404] [0.4710291]\n",
      "loss on epoch  818  :  1.37022495711 [0.31258106] [0.4710291]\n",
      "loss on epoch  819  :  1.37128524648 [0.31387809] [0.47160566]\n",
      "loss on epoch  820  :  1.37041760374 [0.31128404] [0.47160566]\n",
      "loss on epoch  821  :  1.36999793185 [0.31258106] [0.47174978]\n",
      "loss on epoch  822  :  1.36937442753 [0.31258106] [0.47232631]\n",
      "loss on epoch  823  :  1.36995937427 [0.31258106] [0.47232631]\n",
      "loss on epoch  824  :  1.36922291473 [0.31258106] [0.47275871]\n",
      "loss on epoch  825  :  1.36881666272 [0.31128404] [0.47247046]\n",
      "loss on epoch  826  :  1.36812784054 [0.31387809] [0.47247046]\n",
      "loss on epoch  827  :  1.36811329259 [0.31258106] [0.47290286]\n",
      "loss on epoch  828  :  1.36709379046 [0.31387809] [0.47304699]\n",
      "loss on epoch  829  :  1.36811728831 [0.31258106] [0.47319111]\n",
      "loss on epoch  830  :  1.36648863554 [0.31517509] [0.47319111]\n",
      "loss on epoch  831  :  1.36652455065 [0.31517509] [0.47319111]\n",
      "loss on epoch  832  :  1.3663928884 [0.31517509] [0.47290286]\n",
      "loss on epoch  833  :  1.36576954965 [0.31647211] [0.47391179]\n",
      "loss on epoch  834  :  1.3657362726 [0.31517509] [0.47405592]\n",
      "loss on epoch  835  :  1.36511879718 [0.31517509] [0.4747766]\n",
      "loss on epoch  836  :  1.36474434755 [0.31776914] [0.47362351]\n",
      "loss on epoch  837  :  1.36508492187 [0.31517509] [0.47463247]\n",
      "loss on epoch  838  :  1.36517167312 [0.31647211] [0.47492072]\n",
      "loss on epoch  839  :  1.36438886766 [0.31647211] [0.47535312]\n",
      "loss on epoch  840  :  1.36383675425 [0.31776914] [0.47549728]\n",
      "loss on epoch  841  :  1.36283982021 [0.31776914] [0.4760738]\n",
      "loss on epoch  842  :  1.36217204067 [0.31647211] [0.4760738]\n",
      "loss on epoch  843  :  1.3623700451 [0.31647211] [0.47636205]\n",
      "loss on epoch  844  :  1.36278433932 [0.31647211] [0.47621793]\n",
      "loss on epoch  845  :  1.36222369141 [0.31647211] [0.4765062]\n",
      "loss on epoch  846  :  1.36210163876 [0.31776914] [0.4765062]\n",
      "loss on epoch  847  :  1.36135046791 [0.31776914] [0.47693861]\n",
      "loss on epoch  848  :  1.36199389122 [0.31776914] [0.47708273]\n",
      "loss on epoch  849  :  1.36135021625 [0.31647211] [0.47737101]\n",
      "loss on epoch  850  :  1.36065950217 [0.31776914] [0.47765926]\n",
      "loss on epoch  851  :  1.35986640277 [0.31776914] [0.47765926]\n",
      "loss on epoch  852  :  1.35967140728 [0.31647211] [0.47866821]\n",
      "loss on epoch  853  :  1.3592882642 [0.31647211] [0.47866821]\n",
      "loss on epoch  854  :  1.35929015389 [0.31517509] [0.47953302]\n",
      "loss on epoch  855  :  1.35946406258 [0.31776914] [0.47924474]\n",
      "loss on epoch  856  :  1.35810205892 [0.31776914] [0.47953302]\n",
      "loss on epoch  857  :  1.35773152775 [0.31647211] [0.47982126]\n",
      "loss on epoch  858  :  1.35824508579 [0.31647211] [0.47982126]\n",
      "loss on epoch  859  :  1.3574920981 [0.31647211] [0.47996542]\n",
      "loss on epoch  860  :  1.35755164314 [0.31647211] [0.48025367]\n",
      "loss on epoch  861  :  1.3578281226 [0.31647211] [0.48039782]\n",
      "loss on epoch  862  :  1.35700909738 [0.31517509] [0.48054194]\n",
      "loss on epoch  863  :  1.3572003731 [0.31387809] [0.48083022]\n",
      "loss on epoch  864  :  1.3554781874 [0.31517509] [0.48097435]\n",
      "loss on epoch  865  :  1.35549251239 [0.31517509] [0.48140675]\n",
      "loss on epoch  866  :  1.35592307868 [0.31517509] [0.48126262]\n",
      "loss on epoch  867  :  1.35524678451 [0.31647211] [0.48097435]\n",
      "loss on epoch  868  :  1.3551570751 [0.31517509] [0.48140675]\n",
      "loss on epoch  869  :  1.35498444019 [0.31387809] [0.48198327]\n",
      "loss on epoch  870  :  1.35344625182 [0.31517509] [0.48198327]\n",
      "loss on epoch  871  :  1.35360831685 [0.31387809] [0.48255983]\n",
      "loss on epoch  872  :  1.35331209721 [0.31387809] [0.48198327]\n",
      "loss on epoch  873  :  1.35201701853 [0.31387809] [0.48241568]\n",
      "loss on epoch  874  :  1.35336540143 [0.31387809] [0.48284808]\n",
      "loss on epoch  875  :  1.35260881539 [0.31387809] [0.48255983]\n",
      "loss on epoch  876  :  1.35157928423 [0.31258106] [0.48284808]\n",
      "loss on epoch  877  :  1.35163562607 [0.31258106] [0.48313636]\n",
      "loss on epoch  878  :  1.35075125871 [0.31387809] [0.48428941]\n",
      "loss on epoch  879  :  1.34974918101 [0.31387809] [0.48385701]\n",
      "loss on epoch  880  :  1.3512174465 [0.31387809] [0.48428941]\n",
      "loss on epoch  881  :  1.35030515326 [0.31387809] [0.48457769]\n",
      "loss on epoch  882  :  1.35047489625 [0.31517509] [0.48457769]\n",
      "loss on epoch  883  :  1.34979318027 [0.31387809] [0.48529837]\n",
      "loss on epoch  884  :  1.34974850549 [0.31517509] [0.48529837]\n",
      "loss on epoch  885  :  1.34990978682 [0.31517509] [0.48544249]\n",
      "loss on epoch  886  :  1.3495137537 [0.31517509] [0.48544249]\n",
      "loss on epoch  887  :  1.34877094958 [0.31258106] [0.48601902]\n",
      "loss on epoch  888  :  1.34790420532 [0.31387809] [0.48616317]\n",
      "loss on epoch  889  :  1.3476416292 [0.31258106] [0.48601902]\n",
      "loss on epoch  890  :  1.34840348253 [0.31387809] [0.48601902]\n",
      "loss on epoch  891  :  1.34761627515 [0.31517509] [0.48630729]\n",
      "loss on epoch  892  :  1.34686353472 [0.31517509] [0.48630729]\n",
      "loss on epoch  893  :  1.34723473037 [0.31517509] [0.48645142]\n",
      "loss on epoch  894  :  1.34692312391 [0.31387809] [0.4867397]\n",
      "loss on epoch  895  :  1.34553028919 [0.31517509] [0.48659557]\n",
      "loss on epoch  896  :  1.34614608685 [0.31258106] [0.4871721]\n",
      "loss on epoch  897  :  1.34572049424 [0.31258106] [0.4876045]\n",
      "loss on epoch  898  :  1.34518760443 [0.31128404] [0.48774862]\n",
      "loss on epoch  899  :  1.34562738074 [0.31128404] [0.48746037]\n",
      "loss on epoch  900  :  1.34490320418 [0.31258106] [0.48774862]\n",
      "loss on epoch  901  :  1.34411925519 [0.31258106] [0.48818102]\n",
      "loss on epoch  902  :  1.34443048415 [0.31128404] [0.4884693]\n",
      "loss on epoch  903  :  1.34394188722 [0.31258106] [0.4884693]\n",
      "loss on epoch  904  :  1.34346984713 [0.31128404] [0.48861343]\n",
      "loss on epoch  905  :  1.34180268756 [0.31258106] [0.48904583]\n",
      "loss on epoch  906  :  1.3420877258 [0.31258106] [0.48875758]\n",
      "loss on epoch  907  :  1.34311775808 [0.31387809] [0.48904583]\n",
      "loss on epoch  908  :  1.34161969247 [0.31258106] [0.48861343]\n",
      "loss on epoch  909  :  1.34148066794 [0.31258106] [0.48933411]\n",
      "loss on epoch  910  :  1.34216512353 [0.31258106] [0.48947823]\n",
      "loss on epoch  911  :  1.34210811059 [0.31258106] [0.48991063]\n",
      "loss on epoch  912  :  1.34090910134 [0.31387809] [0.48976651]\n",
      "loss on epoch  913  :  1.34112195836 [0.31517509] [0.49005476]\n",
      "loss on epoch  914  :  1.34045978387 [0.31387809] [0.49019891]\n",
      "loss on epoch  915  :  1.3398805349 [0.31258106] [0.49034303]\n",
      "loss on epoch  916  :  1.33874862945 [0.31258106] [0.49091956]\n",
      "loss on epoch  917  :  1.33918405021 [0.31647211] [0.49077544]\n",
      "loss on epoch  918  :  1.33830494351 [0.31387809] [0.49149612]\n",
      "loss on epoch  919  :  1.33940713935 [0.31517509] [0.49077544]\n",
      "loss on epoch  920  :  1.33750920826 [0.31128404] [0.49149612]\n",
      "loss on epoch  921  :  1.33876500306 [0.31517509] [0.49192852]\n",
      "loss on epoch  922  :  1.33801299113 [0.30998704] [0.49149612]\n",
      "loss on epoch  923  :  1.33723897846 [0.31258106] [0.49164024]\n",
      "loss on epoch  924  :  1.33675699985 [0.31258106] [0.49164024]\n",
      "loss on epoch  925  :  1.33677874009 [0.31387809] [0.49221677]\n",
      "loss on epoch  926  :  1.33690462731 [0.31647211] [0.49207264]\n",
      "loss on epoch  927  :  1.33556777018 [0.31387809] [0.49236092]\n",
      "loss on epoch  928  :  1.33599140688 [0.31258106] [0.49264917]\n",
      "loss on epoch  929  :  1.33484279447 [0.31387809] [0.49336985]\n",
      "loss on epoch  930  :  1.33484683434 [0.31517509] [0.49279332]\n",
      "loss on epoch  931  :  1.33524796036 [0.31258106] [0.49380225]\n",
      "loss on epoch  932  :  1.33421030309 [0.31387809] [0.49365813]\n",
      "loss on epoch  933  :  1.33553643801 [0.31387809] [0.49351397]\n",
      "loss on epoch  934  :  1.33396564369 [0.31647211] [0.49394637]\n",
      "loss on epoch  935  :  1.33344738572 [0.31387809] [0.49394637]\n",
      "loss on epoch  936  :  1.3332180734 [0.31517509] [0.49466705]\n",
      "loss on epoch  937  :  1.33308972032 [0.31387809] [0.49481118]\n",
      "loss on epoch  938  :  1.33309406704 [0.31517509] [0.49509946]\n",
      "loss on epoch  939  :  1.33233354489 [0.31776914] [0.49495533]\n",
      "loss on epoch  940  :  1.33234651442 [0.31647211] [0.49466705]\n",
      "loss on epoch  941  :  1.33153763745 [0.31647211] [0.49524358]\n",
      "loss on epoch  942  :  1.33182963398 [0.31517509] [0.49625254]\n",
      "loss on epoch  943  :  1.33081379643 [0.31647211] [0.49625254]\n",
      "loss on epoch  944  :  1.3311391826 [0.31647211] [0.49625254]\n",
      "loss on epoch  945  :  1.33158644703 [0.31647211] [0.49654078]\n",
      "loss on epoch  946  :  1.33005529863 [0.31776914] [0.49654078]\n",
      "loss on epoch  947  :  1.33019334078 [0.31776914] [0.49682906]\n",
      "loss on epoch  948  :  1.33003115875 [0.31776914] [0.49668491]\n",
      "loss on epoch  949  :  1.32972637592 [0.31647211] [0.49711731]\n",
      "loss on epoch  950  :  1.32916248286 [0.31776914] [0.49711731]\n",
      "loss on epoch  951  :  1.3301288044 [0.31906614] [0.49740559]\n",
      "loss on epoch  952  :  1.329040311 [0.31647211] [0.49798211]\n",
      "loss on epoch  953  :  1.3280399817 [0.31776914] [0.49783799]\n",
      "loss on epoch  954  :  1.32889213165 [0.31647211] [0.49855867]\n",
      "loss on epoch  955  :  1.32746021836 [0.31776914] [0.49884692]\n",
      "loss on epoch  956  :  1.32737788006 [0.31776914] [0.49870279]\n",
      "loss on epoch  957  :  1.32680279016 [0.31776914] [0.49927932]\n",
      "loss on epoch  958  :  1.32716405171 [0.31776914] [0.49870279]\n",
      "loss on epoch  959  :  1.32673701092 [0.31776914] [0.4991352]\n",
      "loss on epoch  960  :  1.32622472224 [0.31776914] [0.49971172]\n",
      "loss on epoch  961  :  1.32443095578 [0.31647211] [0.49971172]\n",
      "loss on epoch  962  :  1.32555895602 [0.31647211] [0.49971172]\n",
      "loss on epoch  963  :  1.32539332355 [0.31647211] [0.49985588]\n",
      "loss on epoch  964  :  1.32502301534 [0.31647211] [0.5]\n",
      "loss on epoch  965  :  1.32375030827 [0.31517509] [0.50028825]\n",
      "loss on epoch  966  :  1.32401113157 [0.31517509] [0.50028825]\n",
      "loss on epoch  967  :  1.32229227931 [0.31387809] [0.50028825]\n",
      "loss on epoch  968  :  1.32274108684 [0.31647211] [0.5008648]\n",
      "loss on epoch  969  :  1.32248736311 [0.31776914] [0.5008648]\n",
      "loss on epoch  970  :  1.32310914773 [0.31647211] [0.50115305]\n",
      "loss on epoch  971  :  1.32252829825 [0.31647211] [0.5008648]\n",
      "loss on epoch  972  :  1.32215393693 [0.31776914] [0.50144136]\n",
      "loss on epoch  973  :  1.32259095819 [0.31647211] [0.50172961]\n",
      "loss on epoch  974  :  1.32167822123 [0.31517509] [0.50187373]\n",
      "loss on epoch  975  :  1.32113817886 [0.31517509] [0.50187373]\n",
      "loss on epoch  976  :  1.32129802969 [0.31647211] [0.50172961]\n",
      "loss on epoch  977  :  1.32116594579 [0.31647211] [0.50273854]\n",
      "loss on epoch  978  :  1.32093857615 [0.31517509] [0.50259441]\n",
      "loss on epoch  979  :  1.31973238124 [0.31517509] [0.50288266]\n",
      "loss on epoch  980  :  1.31945364343 [0.31776914] [0.50273854]\n",
      "loss on epoch  981  :  1.31946320004 [0.31906614] [0.50331509]\n",
      "loss on epoch  982  :  1.32001396241 [0.31647211] [0.50345922]\n",
      "loss on epoch  983  :  1.31789695113 [0.31517509] [0.50360334]\n",
      "loss on epoch  984  :  1.31806797893 [0.31647211] [0.50374746]\n",
      "loss on epoch  985  :  1.31861220245 [0.31517509] [0.50403577]\n",
      "loss on epoch  986  :  1.31797169314 [0.31776914] [0.50417989]\n",
      "loss on epoch  987  :  1.31689181151 [0.31517509] [0.50432402]\n",
      "loss on epoch  988  :  1.31663691335 [0.31517509] [0.50432402]\n",
      "loss on epoch  989  :  1.31651604838 [0.31517509] [0.50490057]\n",
      "loss on epoch  990  :  1.31648701871 [0.31776914] [0.50475639]\n",
      "loss on epoch  991  :  1.31608579556 [0.31647211] [0.50490057]\n",
      "loss on epoch  992  :  1.31664846782 [0.31387809] [0.50533295]\n",
      "loss on epoch  993  :  1.31527806653 [0.31776914] [0.50605363]\n",
      "loss on epoch  994  :  1.31559607497 [0.31906614] [0.506486]\n",
      "loss on epoch  995  :  1.31594583723 [0.31517509] [0.50619775]\n",
      "loss on epoch  996  :  1.31579856961 [0.31647211] [0.50619775]\n",
      "loss on epoch  997  :  1.31430296986 [0.31906614] [0.5073508]\n",
      "loss on epoch  998  :  1.31429321678 [0.31776914] [0.50720668]\n",
      "loss on epoch  999  :  1.31451841637 [0.32036316] [0.5073508]\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "para_n_epoch = 1000\n",
    "\n",
    "# tunable parameters\n",
    "para_n_hidden_list = [ 256,128,32 ]\n",
    "para_batch_size = 128\n",
    "para_n_embedding = 2\n",
    "para_lr=0.008\n",
    "para_keep_prob=0.8\n",
    "para_l2=0.09\n",
    "\n",
    "# fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = 8\n",
    "\n",
    "tot_cnt= np.shape(cxtrain)[0]\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    mlp_clf = mlp_demo( para_batch_size, para_n_conti, para_n_disc, para_n_class, \\\n",
    "                        para_n_disc_voca, para_n_embedding, sess, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2 )\n",
    "    mlp_clf.train_ini()\n",
    "    mlp_clf.inference_ini()\n",
    "    \n",
    "    para_cur_lr=para_lr\n",
    "    total_batch = int(tot_cnt/para_batch_size)\n",
    "\n",
    "    tot_idx=range(tot_cnt)\n",
    "\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc=0.0\n",
    "        \n",
    "#       shuffle traning dataset each epoch  \n",
    "        np.random.shuffle(tot_idx)\n",
    "   \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = tot_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += mlp_clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                       para_cur_lr)\n",
    "        \n",
    "#       learning rate decaying\n",
    "        para_cur_lr = para_cur_lr*0.99\n",
    "        \n",
    "        tmp_test_acc = mlp_clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = mlp_clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wide demoMLP\n",
    "## without embedding\n",
    "class wide_mlp_demo():\n",
    "    \n",
    "    def __init__(self, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, session, n_hidden_list, lr, l2):\n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list) \n",
    "        self.sess=session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "#       linnear part on categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "        \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"disc\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_uniform([self.N_DISC_VOCA, self.N_CLASS],-1.0, 1.0) )\n",
    "#                         tf.random_normal([self.N_DISC_VOCA, self.N_CLASS],\\\n",
    "#                         stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "                \n",
    "                if i==0:\n",
    "                    dx_wsum = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                else:\n",
    "                    dx_wsum = dx_wsum + tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "        \n",
    "        with tf.variable_scope(\"wide\"):\n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))            \n",
    "            dx_wsum = tf.add(dx_wsum, b)\n",
    "\n",
    "#----unit test---------------------------------------------------------\n",
    "#         print self.N_TOTAL\n",
    "#         self.tmpshape= tf.shape(x_concate)\n",
    "        \n",
    "#         self.tmpshape1= tf.shape(tmp_embeded[0])\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# #     hidden layers\n",
    "        with tf.variable_scope(\"h0\"):\n",
    "            w= tf.Variable(tf.random_normal([self.N_CONTI, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_CONTI)))) \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            h = tf.nn.relu( tf.add( tf.matmul(self.cx, w),b) )\n",
    "        \n",
    "                \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"layer\"+str(i)):\n",
    "                w= tf.Variable(tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b= tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                h = tf.nn.relu( tf.add( tf.matmul(h, w),b) )\n",
    "        \n",
    "#       Regularization\n",
    "#       dropout\n",
    "#         h = tf.nn.dropout(h, self.keep_prob)\n",
    "#       L2\n",
    "        self.regularizer = tf.nn.l2_loss(w)\n",
    "        \n",
    "#       output layer  \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w= tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1],\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            h = tf.add( tf.matmul(h, w),b)\n",
    "            \n",
    "#           L2  \n",
    "            self.regularizer = self.regularizer + tf.nn.l2_loss(w)\n",
    "            \n",
    "            self.logit = h + dx_wsum\n",
    "    \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y) \\\n",
    "                                  + self.L2*self.regularizer)\n",
    "        self.optimizer = \\\n",
    "        tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#       tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                       feed_dict={self.dx:dx_batch, \\\n",
    "                                  self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                  self.lr:lr,\\\n",
    "                                  self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "    \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                  self.cx:cx_test, self.y:y_test,\\\n",
    "                                                  self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        return sess.run( [self.tmpshape], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "para_n_epoch = 800\n",
    "\n",
    "# tunable parameters\n",
    "para_n_hidden_list = [ 16,8 ]\n",
    "para_batch_size = 128\n",
    "para_lr=0.001\n",
    "para_keep_prob=0.9\n",
    "para_l2=0.1\n",
    "\n",
    "# fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = 8\n",
    "\n",
    "tot_cnt= np.shape(cxtrain)[0]\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    wide_mlp_clf = wide_mlp_demo( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                        para_n_disc_voca, sess, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2 )\n",
    "    wide_mlp_clf.train_ini()\n",
    "    wide_mlp_clf.inference_ini()\n",
    "    \n",
    "    para_cur_lr=para_lr\n",
    "    total_batch = int(tot_cnt/para_batch_size)\n",
    "\n",
    "    tot_idx=range(tot_cnt)\n",
    "\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc=0.0\n",
    "        \n",
    "#       shuffle traning dataset each epoch  \n",
    "        np.random.shuffle(tot_idx)\n",
    "   \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = tot_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += wide_mlp_clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                       para_cur_lr)\n",
    "        \n",
    "#       learning rate decaying\n",
    "        para_cur_lr = para_cur_lr*0.98\n",
    "        \n",
    "        tmp_test_acc = wide_mlp_clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = wide_mlp_clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wide and interaction demoMLP\n",
    "## without embedding\n",
    "class wide_mlp_demo():\n",
    "    \n",
    "    def __init__(self, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, session, n_hidden_list, lr, l2):\n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list) \n",
    "        self.sess=session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "#       linnear part on categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "        \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"disc\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_uniform([self.N_DISC_VOCA, self.N_CLASS],-1.0, 1.0) )\n",
    "#                         tf.random_normal([self.N_DISC_VOCA, self.N_CLASS],\\\n",
    "#                         stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "                if i==0:\n",
    "                    dx_wsum = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                else:\n",
    "                    dx_wsum = dx_wsum + tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "        \n",
    "        with tf.variable_scope(\"wide\"):\n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))            \n",
    "            dx_wsum = tf.add(dx_wsum, b)\n",
    "            \n",
    "            \n",
    "#       interaction of categorical features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----unit test---------------------------------------------------------\n",
    "#         print self.N_TOTAL\n",
    "#         self.tmpshape= tf.shape(x_concate)\n",
    "        \n",
    "#         self.tmpshape1= tf.shape(tmp_embeded[0])\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# #     hidden layers\n",
    "        with tf.variable_scope(\"h0\"):\n",
    "            w= tf.Variable(tf.random_normal([self.N_CONTI, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_CONTI)))) \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            h = tf.nn.relu( tf.add( tf.matmul(self.cx, w),b) )\n",
    "        \n",
    "                \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"layer\"+str(i)):\n",
    "                w= tf.Variable(tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b= tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                h = tf.nn.relu( tf.add( tf.matmul(h, w),b) )\n",
    "        \n",
    "#       Regularization\n",
    "#       dropout\n",
    "#         h = tf.nn.dropout(h, self.keep_prob)\n",
    "#       L2\n",
    "        self.regularizer = tf.nn.l2_loss(w)\n",
    "        \n",
    "#       output layer  \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w= tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1],\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            h = tf.add( tf.matmul(h, w),b)\n",
    "            \n",
    "#           L2  \n",
    "            self.regularizer = self.regularizer + tf.nn.l2_loss(w)\n",
    "            \n",
    "            self.logit = h + dx_wsum\n",
    "    \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y) \\\n",
    "                                  + self.L2*self.regularizer)\n",
    "        self.optimizer = \\\n",
    "        tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#       tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                       feed_dict={self.dx:dx_batch, \\\n",
    "                                  self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                  self.lr:lr,\\\n",
    "                                  self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "    \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                  self.cx:cx_test, self.y:y_test,\\\n",
    "                                                  self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        return sess.run( [self.tmpshape], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wide and embedding, \n",
    "# wide co-occurrence\n",
    "# wide co-occurrence & interaction \n",
    "\n",
    "\n",
    "\n",
    "# ordinal regression enhancment \n",
    "class mlp_demo_or():\n",
    "    \n",
    "# feature interaction + ordinal regression enhancment \n",
    "class mlp_demo_or_fi():"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
