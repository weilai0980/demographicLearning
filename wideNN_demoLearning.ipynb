{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TO DO:\n",
    "\n",
    "# ordinal \n",
    "# categorical+continuous\n",
    "\n",
    "# feature interaction\n",
    "\n",
    "# batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from six.moves import urllib\n",
    "# from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6938, 186) (771, 186) (6938, 1) (771, 1)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "files_list=[\"../dataset/dataset_demo/xtrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/xtest.csv\",\\\n",
    "            \"../dataset/dataset_demo/ytrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/ytest.csv\"]\n",
    "\n",
    "xtrain_df=pd.read_csv( files_list[0] ,sep=',', header=None)\n",
    "xtest_df=pd.read_csv( files_list[1] ,sep=',', header=None)\n",
    "ytrain_df=pd.read_csv( files_list[2] ,sep=',', header=None)\n",
    "ytest_df=pd.read_csv( files_list[3] ,sep=',', header=None)\n",
    "\n",
    "print xtrain_df.shape, xtest_df.shape, ytrain_df.shape, ytest_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# continuous and categorical feature split\n",
    "\n",
    "# features on columns [0:split_idx] are continuous\n",
    "# the rest are categorical\n",
    "def conti_cate_split(dta_df, split_idx):\n",
    "    \n",
    "    col_cnt= dta_df.shape[1]\n",
    "    conti_cols= range(split_idx)\n",
    "    dis_cols=range(split_idx, col_cnt)\n",
    "    \n",
    "    return dta_df[conti_cols], dta_df[dis_cols]\n",
    "\n",
    "def conti_normalization_train_dta(dta_df):\n",
    "    \n",
    "    return preprocessing.scale(dta_df)\n",
    "\n",
    "def conti_normalization_test_dta(dta_df, train_df):\n",
    "    \n",
    "    mean_dim = np.mean(train_df, axis=0)\n",
    "    std_dim = np.std(train_df, axis=0)\n",
    "        \n",
    "    df=pd.DataFrame()\n",
    "    cols = train_df.columns\n",
    "    idx=0\n",
    "    \n",
    "    for i in cols:\n",
    "        df[i] = (dta_df[i]- mean_dim[idx])*1.0/std_dim[idx]\n",
    "        idx=idx+1\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6938, 106) (6938, 80)\n",
      "(771, 106) (771, 80)\n"
     ]
    }
   ],
   "source": [
    "# get training and testing data prepared\n",
    "\n",
    "cxtrain, dxtrain = conti_cate_split(xtrain_df, 106)\n",
    "cxtest, dxtest = conti_cate_split(xtest_df, 106)\n",
    "\n",
    "cxtest = conti_normalization_test_dta(cxtest, cxtrain)\n",
    "cxtrain = conti_normalization_train_dta(cxtrain)\n",
    "\n",
    "print cxtrain.shape, dxtrain.shape\n",
    "print cxtest.shape, dxtest.shape\n",
    "\n",
    "# cxtrain = cxtrain.as_matrix() \n",
    "dxtrain = dxtrain.as_matrix().astype(int)\n",
    "cxtest = cxtest.as_matrix()\n",
    "dxtest = dxtest.as_matrix().astype(int)\n",
    "\n",
    "ytest = np_utils.to_categorical( ytest_df.as_matrix() )\n",
    "ytrain =np_utils.to_categorical( ytrain_df.as_matrix() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only embedding\n",
    "\n",
    "class mlp_demo():\n",
    "    \n",
    "    def __init__(self, batch_size, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, n_embedding, session, n_hidden_list, lr, l2):\n",
    "        \n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.N_EMBED = n_embedding\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list) \n",
    "        self.N_TOTAL = self.N_CONTI+ self.N_DISC*self.N_EMBED\n",
    "        \n",
    "        self.sess=session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "#       embedding categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "        \n",
    "        tmp_embeded = []                \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"disc\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA, self.N_EMBED],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "                              \n",
    "                tmp_embeded.append( tf.nn.embedding_lookup( w, self.dx_trans[i] ) )\n",
    "                \n",
    "        dx_embeded = tf.concat(tmp_embeded, 1)\n",
    "        x_concate =  tf.concat( [self.cx,dx_embeded], 1 )\n",
    "\n",
    "#----unit test---------------------------------------------------------\n",
    "#         print self.N_TOTAL\n",
    "#         self.tmpshape= tf.shape(x_concate)\n",
    "        \n",
    "#         self.tmpshape1= tf.shape(tmp_embeded[0])\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# #     hidden layers\n",
    "        with tf.variable_scope(\"h0\"):\n",
    "            w= tf.Variable(tf.random_normal([self.N_TOTAL, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_TOTAL)))) \n",
    "            \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            \n",
    "            h = tf.nn.relu( tf.add( tf.matmul(x_concate, w),b) )\n",
    "        \n",
    "        \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"layer\"+str(i)):\n",
    "                \n",
    "                w= tf.Variable(tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                \n",
    "                b= tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                \n",
    "                h = tf.nn.relu( tf.add( tf.matmul(h, w),b) )\n",
    "        \n",
    "#   Regularization  \n",
    "#       dropout\n",
    "        h = tf.nn.dropout(h, self.keep_prob)\n",
    "\n",
    "#       L2\n",
    "        self.regularizer = tf.nn.l2_loss(w)\n",
    "    \n",
    "        \n",
    "#       output layer  \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w= tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1],\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "                \n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            \n",
    "            h = tf.add( tf.matmul(h, w),b)\n",
    "            \n",
    "#           L2  \n",
    "            self.regularizer = self.regularizer + tf.nn.l2_loss(w)\n",
    "            \n",
    "            \n",
    "            self.logit = h\n",
    "    \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        \n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y) \\\n",
    "                                   )\n",
    "#                                   + self.L2*self.regularizer)\n",
    "        \n",
    "        self.optimizer = \\\n",
    "        tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                       feed_dict={self.dx:dx_batch, \\\n",
    "                                  self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                  self.lr:lr,\\\n",
    "                                  self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "    \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        \n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                  self.cx:cx_test, self.y:y_test,\\\n",
    "                                                  self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "        return sess.run( [self.tmpshape], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch  0  :  2.61422722582 [0.08690013] [0.084462382]\n",
      "loss on epoch  1  :  2.49386830606 [0.10246433] [0.087633327]\n",
      "loss on epoch  2  :  2.40369938249 [0.10505836] [0.097002015]\n",
      "loss on epoch  3  :  2.32592620193 [0.12581064] [0.11790141]\n",
      "loss on epoch  4  :  2.26486413375 [0.13618676] [0.13058518]\n",
      "loss on epoch  5  :  2.21472775159 [0.15953307] [0.14153935]\n",
      "loss on epoch  6  :  2.17865108234 [0.16990921] [0.16215047]\n",
      "loss on epoch  7  :  2.14756187321 [0.17509727] [0.17541078]\n",
      "loss on epoch  8  :  2.10958231878 [0.18677042] [0.18261747]\n",
      "loss on epoch  9  :  2.08552519567 [0.18028535] [0.18535601]\n",
      "loss on epoch  10  :  2.06685491403 [0.19844358] [0.19991352]\n",
      "loss on epoch  11  :  2.04117549243 [0.19584955] [0.20265207]\n",
      "loss on epoch  12  :  2.02940414263 [0.20492867] [0.21274142]\n",
      "loss on epoch  13  :  2.01158284709 [0.1919585] [0.21202075]\n",
      "loss on epoch  14  :  1.99941817574 [0.21919584] [0.22138944]\n",
      "loss on epoch  15  :  1.98532693282 [0.21400778] [0.22268665]\n",
      "loss on epoch  16  :  1.970428364 [0.21530479] [0.22643413]\n",
      "loss on epoch  17  :  1.95943478657 [0.21271077] [0.22556932]\n",
      "loss on epoch  18  :  1.95040682606 [0.19844358] [0.23147881]\n",
      "loss on epoch  19  :  1.94116616422 [0.21400778] [0.23364082]\n",
      "loss on epoch  20  :  1.93450188205 [0.21919584] [0.23681176]\n",
      "loss on epoch  21  :  1.92922003459 [0.21530479] [0.24272124]\n",
      "loss on epoch  22  :  1.91551972645 [0.21400778] [0.23839723]\n",
      "loss on epoch  23  :  1.90681860499 [0.21660182] [0.24416259]\n",
      "loss on epoch  24  :  1.90527622164 [0.22438392] [0.24373019]\n",
      "loss on epoch  25  :  1.89479471981 [0.23476005] [0.24286538]\n",
      "loss on epoch  26  :  1.88681508493 [0.22178988] [0.24445085]\n",
      "loss on epoch  27  :  1.88398860244 [0.21660182] [0.24762179]\n",
      "loss on epoch  28  :  1.87418638796 [0.22568093] [0.25093687]\n",
      "loss on epoch  29  :  1.86902366687 [0.23994812] [0.24690112]\n",
      "loss on epoch  30  :  1.85822703527 [0.21789883] [0.2528106]\n",
      "loss on epoch  31  :  1.8534215585 [0.23346303] [0.25468436]\n",
      "loss on epoch  32  :  1.83902623774 [0.23735408] [0.25136927]\n",
      "loss on epoch  33  :  1.84152160866 [0.21530479] [0.24949554]\n",
      "loss on epoch  34  :  1.83938114453 [0.24513619] [0.25180167]\n",
      "loss on epoch  35  :  1.83359429715 [0.25291827] [0.25612569]\n",
      "loss on epoch  36  :  1.8241494488 [0.23994812] [0.25554916]\n",
      "loss on epoch  37  :  1.81742702014 [0.23735408] [0.25886422]\n",
      "loss on epoch  38  :  1.81930239995 [0.22957198] [0.25886422]\n",
      "loss on epoch  39  :  1.81010535748 [0.24513619] [0.26635918]\n",
      "loss on epoch  40  :  1.80726657916 [0.23735408] [0.26246756]\n",
      "loss on epoch  41  :  1.80655316166 [0.25162128] [0.26030555]\n",
      "loss on epoch  42  :  1.8053623697 [0.24254215] [0.26405305]\n",
      "loss on epoch  43  :  1.8007925913 [0.24254215] [0.25944075]\n",
      "loss on epoch  44  :  1.80164214494 [0.24773023] [0.26434129]\n",
      "loss on epoch  45  :  1.78981894082 [0.25032425] [0.25929663]\n",
      "loss on epoch  46  :  1.78740447673 [0.25680932] [0.26145864]\n",
      "loss on epoch  47  :  1.78715371308 [0.22308689] [0.26535025]\n",
      "loss on epoch  48  :  1.77901055847 [0.23735408] [0.26808879]\n",
      "loss on epoch  49  :  1.77977057989 [0.23735408] [0.2702508]\n",
      "loss on epoch  50  :  1.77956776861 [0.23216602] [0.26722398]\n",
      "loss on epoch  51  :  1.76444555106 [0.22308689] [0.26333237]\n",
      "loss on epoch  52  :  1.77577356757 [0.24513619] [0.27125973]\n",
      "loss on epoch  53  :  1.76994730424 [0.23476005] [0.26318824]\n",
      "loss on epoch  54  :  1.76955749418 [0.230869] [0.26909772]\n",
      "loss on epoch  55  :  1.76536952153 [0.24773023] [0.27140385]\n",
      "loss on epoch  56  :  1.76179415506 [0.2542153] [0.2711156]\n",
      "loss on epoch  57  :  1.76048218426 [0.24513619] [0.27082732]\n",
      "loss on epoch  58  :  1.75456513097 [0.230869] [0.27688095]\n",
      "loss on epoch  59  :  1.75259230707 [0.24902724] [0.27486306]\n",
      "loss on epoch  60  :  1.75556410568 [0.24902724] [0.26679158]\n",
      "loss on epoch  61  :  1.75497958366 [0.24902724] [0.26722398]\n",
      "loss on epoch  62  :  1.75122288085 [0.25032425] [0.27183625]\n",
      "loss on epoch  63  :  1.74525252632 [0.24124514] [0.27515134]\n",
      "loss on epoch  64  :  1.74325222122 [0.22697794] [0.27688095]\n",
      "loss on epoch  65  :  1.74560496773 [0.24383917] [0.27587202]\n",
      "loss on epoch  66  :  1.73978167168 [0.25291827] [0.27284521]\n",
      "loss on epoch  67  :  1.7391307665 [0.25162128] [0.27414241]\n",
      "loss on epoch  68  :  1.73621884032 [0.24254215] [0.2826463]\n",
      "loss on epoch  69  :  1.73697915716 [0.25551233] [0.27832228]\n",
      "loss on epoch  70  :  1.73305163522 [0.25810635] [0.28091669]\n",
      "loss on epoch  71  :  1.73230051908 [0.24383917] [0.27486306]\n",
      "loss on epoch  72  :  1.72892993343 [0.25291827] [0.27601615]\n",
      "loss on epoch  73  :  1.72756560125 [0.23994812] [0.27702507]\n",
      "loss on epoch  74  :  1.72633773693 [0.22568093] [0.2776016]\n",
      "loss on epoch  75  :  1.7250714233 [0.2386511] [0.28524071]\n",
      "loss on epoch  76  :  1.7245498386 [0.2464332] [0.28120497]\n",
      "loss on epoch  77  :  1.7253584283 [0.25032425] [0.28192562]\n",
      "loss on epoch  78  :  1.72253868632 [0.24383917] [0.28423178]\n",
      "loss on epoch  79  :  1.71978227643 [0.24902724] [0.28178149]\n",
      "loss on epoch  80  :  1.72009952777 [0.25291827] [0.28668204]\n",
      "loss on epoch  81  :  1.72050525313 [0.25162128] [0.28423178]\n",
      "loss on epoch  82  :  1.71664421714 [0.25680932] [0.28480831]\n",
      "loss on epoch  83  :  1.71465025777 [0.24902724] [0.27918708]\n",
      "loss on epoch  84  :  1.71126086556 [0.22697794] [0.28206977]\n",
      "loss on epoch  85  :  1.71045970917 [0.22438392] [0.28769097]\n",
      "loss on epoch  86  :  1.71390926406 [0.25162128] [0.28697032]\n",
      "loss on epoch  87  :  1.70894393109 [0.26588845] [0.29028538]\n",
      "loss on epoch  88  :  1.71021868958 [0.24773023] [0.28495243]\n",
      "loss on epoch  89  :  1.71148903733 [0.24124514] [0.28668204]\n",
      "loss on epoch  90  :  1.70101175792 [0.24773023] [0.28466415]\n",
      "loss on epoch  91  :  1.6974319185 [0.25291827] [0.28942057]\n",
      "loss on epoch  92  :  1.70477889586 [0.24902724] [0.29042953]\n",
      "loss on epoch  93  :  1.71027487171 [0.23735408] [0.29042953]\n",
      "loss on epoch  94  :  1.70083088806 [0.24513619] [0.29071778]\n",
      "loss on epoch  95  :  1.69831745434 [0.24254215] [0.29028538]\n",
      "loss on epoch  96  :  1.70295568048 [0.23994812] [0.28985298]\n",
      "loss on epoch  97  :  1.70005551715 [0.25680932] [0.2830787]\n",
      "loss on epoch  98  :  1.69905696313 [0.27107653] [0.29273567]\n",
      "loss on epoch  99  :  1.68922085952 [0.24254215] [0.28970885]\n",
      "loss on epoch  100  :  1.69521312696 [0.24383917] [0.29475352]\n",
      "loss on epoch  101  :  1.6901545516 [0.25940338] [0.29042953]\n",
      "loss on epoch  102  :  1.68992905945 [0.22827497] [0.29331219]\n",
      "loss on epoch  103  :  1.68812953994 [0.26070037] [0.28624964]\n",
      "loss on epoch  104  :  1.69065090625 [0.25291827] [0.29230326]\n",
      "loss on epoch  105  :  1.68259632069 [0.26070037] [0.29345632]\n",
      "loss on epoch  106  :  1.68658695446 [0.26459143] [0.29158258]\n",
      "loss on epoch  107  :  1.68642134943 [0.24513619] [0.29749209]\n",
      "loss on epoch  108  :  1.68449893205 [0.2697795] [0.30123955]\n",
      "loss on epoch  109  :  1.68571563303 [0.24513619] [0.29619488]\n",
      "loss on epoch  110  :  1.68117628754 [0.25291827] [0.29100606]\n",
      "loss on epoch  111  :  1.68321034183 [0.26329443] [0.29763621]\n",
      "loss on epoch  112  :  1.67956350679 [0.2619974] [0.29806861]\n",
      "loss on epoch  113  :  1.68410028668 [0.26459143] [0.29936582]\n",
      "loss on epoch  114  :  1.6812430672 [0.24773023] [0.29533008]\n",
      "loss on epoch  115  :  1.67432038248 [0.24773023] [0.29965407]\n",
      "loss on epoch  116  :  1.67479011546 [0.26588845] [0.30282503]\n",
      "loss on epoch  117  :  1.67291004934 [0.27626458] [0.29778033]\n",
      "loss on epoch  118  :  1.67324330669 [0.25162128] [0.29821274]\n",
      "loss on epoch  119  :  1.67615422304 [0.24902724] [0.29633901]\n",
      "loss on epoch  120  :  1.67492169315 [0.2775616] [0.30412224]\n",
      "loss on epoch  121  :  1.67083613682 [0.27496758] [0.30210435]\n",
      "loss on epoch  122  :  1.66654620913 [0.26459143] [0.29950994]\n",
      "loss on epoch  123  :  1.67006113737 [0.2542153] [0.29792449]\n",
      "loss on epoch  124  :  1.66945139913 [0.27237353] [0.30095127]\n",
      "loss on epoch  125  :  1.6678916501 [0.2464332] [0.29994234]\n",
      "loss on epoch  126  :  1.66611844474 [0.26070037] [0.30023062]\n",
      "loss on epoch  127  :  1.670025632 [0.25940338] [0.30642837]\n",
      "loss on epoch  128  :  1.66472513866 [0.25291827] [0.30441049]\n",
      "loss on epoch  129  :  1.66167618399 [0.2464332] [0.29749209]\n",
      "loss on epoch  130  :  1.66368335831 [0.26459143] [0.30282503]\n",
      "loss on epoch  131  :  1.66460313572 [0.27496758] [0.29936582]\n",
      "loss on epoch  132  :  1.66144398848 [0.29053178] [0.29878926]\n",
      "loss on epoch  133  :  1.66340365185 [0.26848248] [0.30066302]\n",
      "loss on epoch  134  :  1.66320372146 [0.27367055] [0.30628422]\n",
      "loss on epoch  135  :  1.65455690415 [0.27367055] [0.30051887]\n",
      "loss on epoch  136  :  1.65585200856 [0.2697795] [0.30729318]\n",
      "loss on epoch  137  :  1.66074991831 [0.25551233] [0.30729318]\n",
      "loss on epoch  138  :  1.65490908986 [0.27496758] [0.30642837]\n",
      "loss on epoch  139  :  1.65650673275 [0.26718548] [0.30945519]\n",
      "loss on epoch  140  :  1.66073810881 [0.25680932] [0.30513117]\n",
      "loss on epoch  141  :  1.65161540629 [0.25680932] [0.3065725]\n",
      "loss on epoch  142  :  1.65395997141 [0.24383917] [0.31305853]\n",
      "loss on epoch  143  :  1.6552448195 [0.27496758] [0.30902278]\n",
      "loss on epoch  144  :  1.65322914676 [0.2619974] [0.30570769]\n",
      "loss on epoch  145  :  1.65212599436 [0.27107653] [0.31507638]\n",
      "loss on epoch  146  :  1.65259484709 [0.2697795] [0.30729318]\n",
      "loss on epoch  147  :  1.64482027897 [0.27237353] [0.30412224]\n",
      "loss on epoch  148  :  1.64610056601 [0.27367055] [0.31277025]\n",
      "loss on epoch  149  :  1.6470984998 [0.27626458] [0.30729318]\n",
      "loss on epoch  150  :  1.64978679453 [0.25940338] [0.31132892]\n",
      "loss on epoch  151  :  1.64683401757 [0.26848248] [0.30513117]\n",
      "loss on epoch  152  :  1.64378582308 [0.2697795] [0.31363505]\n",
      "loss on epoch  153  :  1.64196273987 [0.27626458] [0.30945519]\n",
      "loss on epoch  154  :  1.65037150487 [0.27367055] [0.31449986]\n",
      "loss on epoch  155  :  1.64072564979 [0.26848248] [0.30815798]\n",
      "loss on epoch  156  :  1.6456297636 [0.26848248] [0.31219372]\n",
      "loss on epoch  157  :  1.643377571 [0.27626458] [0.30844623]\n",
      "loss on epoch  158  :  1.63993541313 [0.26848248] [0.31075239]\n",
      "loss on epoch  159  :  1.64460514507 [0.25810635] [0.31277025]\n",
      "loss on epoch  160  :  1.64305499585 [0.28145266] [0.30873451]\n",
      "loss on epoch  161  :  1.63953856627 [0.27367055] [0.31204957]\n",
      "loss on epoch  162  :  1.64047178928 [0.27107653] [0.30916691]\n",
      "loss on epoch  163  :  1.64398846592 [0.28274968] [0.30758142]\n",
      "loss on epoch  164  :  1.63921382375 [0.28274968] [0.3116172]\n",
      "loss on epoch  165  :  1.64019210356 [0.28145266] [0.31363505]\n",
      "loss on epoch  166  :  1.62776404878 [0.25680932] [0.31738254]\n",
      "loss on epoch  167  :  1.64101356613 [0.26588845] [0.31536466]\n",
      "loss on epoch  168  :  1.63252607895 [0.27367055] [0.30859038]\n",
      "loss on epoch  169  :  1.63815864273 [0.25291827] [0.3194004]\n",
      "loss on epoch  170  :  1.63270210097 [0.27367055] [0.32271549]\n",
      "loss on epoch  171  :  1.63517226603 [0.27367055] [0.31795907]\n",
      "loss on epoch  172  :  1.63880319699 [0.25940338] [0.31738254]\n",
      "loss on epoch  173  :  1.63147490958 [0.27107653] [0.31435573]\n",
      "loss on epoch  174  :  1.62989330637 [0.29701686] [0.31089652]\n",
      "loss on epoch  175  :  1.6278641388 [0.26329443] [0.3194004]\n",
      "loss on epoch  176  :  1.62908899957 [0.2697795] [0.31651774]\n",
      "loss on epoch  177  :  1.63313287928 [0.25810635] [0.31464398]\n",
      "loss on epoch  178  :  1.62522724832 [0.27496758] [0.32660708]\n",
      "loss on epoch  179  :  1.63129894025 [0.27107653] [0.31017584]\n",
      "loss on epoch  180  :  1.62656145338 [0.2619974] [0.31723839]\n",
      "loss on epoch  181  :  1.62877973785 [0.2619974] [0.31997693]\n",
      "loss on epoch  182  :  1.62233376935 [0.2697795] [0.31637359]\n",
      "loss on epoch  183  :  1.6276209985 [0.26848248] [0.32502162]\n",
      "loss on epoch  184  :  1.62201510305 [0.29701686] [0.32329202]\n",
      "loss on epoch  185  :  1.61847600125 [0.26329443] [0.31968868]\n",
      "loss on epoch  186  :  1.62231313837 [0.27496758] [0.32069761]\n",
      "loss on epoch  187  :  1.62613297113 [0.28793773] [0.32040933]\n",
      "loss on epoch  188  :  1.62880422499 [0.26459143] [0.32040933]\n",
      "loss on epoch  189  :  1.61994261327 [0.28664073] [0.32314789]\n",
      "loss on epoch  190  :  1.61482667923 [0.27626458] [0.3304987]\n",
      "loss on epoch  191  :  1.61692957688 [0.28534371] [0.32113001]\n",
      "loss on epoch  192  :  1.61877404175 [0.27496758] [0.3244451]\n",
      "loss on epoch  193  :  1.62038492897 [0.2619974] [0.31882387]\n",
      "loss on epoch  194  :  1.61936412946 [0.27107653] [0.32069761]\n",
      "loss on epoch  195  :  1.6168661394 [0.26329443] [0.32430094]\n",
      "loss on epoch  196  :  1.61880313745 [0.27626458] [0.32300374]\n",
      "loss on epoch  197  :  1.61818910854 [0.27496758] [0.32458922]\n",
      "loss on epoch  198  :  1.61886279393 [0.28923476] [0.32502162]\n",
      "loss on epoch  199  :  1.61514697472 [0.28274968] [0.32833669]\n",
      "loss on epoch  200  :  1.61196388715 [0.28534371] [0.32660708]\n",
      "loss on epoch  201  :  1.6097901118 [0.29182878] [0.32819256]\n",
      "loss on epoch  202  :  1.61432593802 [0.29312581] [0.32833669]\n",
      "loss on epoch  203  :  1.60803401643 [0.28274968] [0.32285962]\n",
      "loss on epoch  204  :  1.6140108359 [0.28274968] [0.33078697]\n",
      "loss on epoch  205  :  1.61180584327 [0.27496758] [0.32271549]\n",
      "loss on epoch  206  :  1.61233669692 [0.27107653] [0.32703948]\n",
      "loss on epoch  207  :  1.60779411724 [0.26588845] [0.32242721]\n",
      "loss on epoch  208  :  1.61105318519 [0.27496758] [0.32992217]\n",
      "loss on epoch  209  :  1.60927555267 [0.29053178] [0.3248775]\n",
      "loss on epoch  210  :  1.61038536093 [0.27107653] [0.32862496]\n",
      "loss on epoch  211  :  1.60661830591 [0.27626458] [0.32761604]\n",
      "loss on epoch  212  :  1.60598229498 [0.2775616] [0.33583164]\n",
      "loss on epoch  213  :  1.60343729413 [0.28534371] [0.32516575]\n",
      "loss on epoch  214  :  1.60293952538 [0.26718548] [0.32920149]\n",
      "loss on epoch  215  :  1.6065662175 [0.28015563] [0.33194005]\n",
      "loss on epoch  216  :  1.61023265383 [0.29442284] [0.33309311]\n",
      "loss on epoch  217  :  1.60671417523 [0.28664073] [0.33395791]\n",
      "loss on epoch  218  :  1.60029305755 [0.30350193] [0.32747188]\n",
      "loss on epoch  219  :  1.60230076227 [0.27626458] [0.32660708]\n",
      "loss on epoch  220  :  1.60310762689 [0.29571983] [0.33208418]\n",
      "loss on epoch  221  :  1.60611775042 [0.2775616] [0.33366963]\n",
      "loss on epoch  222  :  1.60767434473 [0.28274968] [0.3368406]\n",
      "loss on epoch  223  :  1.59885677825 [0.26588845] [0.33986738]\n",
      "loss on epoch  224  :  1.60215993463 [0.28145266] [0.33611992]\n",
      "loss on epoch  225  :  1.59910532789 [0.27496758] [0.32948977]\n",
      "loss on epoch  226  :  1.6067588623 [0.28923476] [0.33828193]\n",
      "loss on epoch  227  :  1.60493328433 [0.26070037] [0.33669645]\n",
      "loss on epoch  228  :  1.59537817862 [0.29961088] [0.33021045]\n",
      "loss on epoch  229  :  1.59717476368 [0.28404668] [0.33669645]\n",
      "loss on epoch  230  :  1.59583705836 [0.30090791] [0.33871433]\n",
      "loss on epoch  231  :  1.5980478888 [0.26848248] [0.33237243]\n",
      "loss on epoch  232  :  1.60065011443 [0.28404668] [0.33309311]\n",
      "loss on epoch  233  :  1.59516813703 [0.28404668] [0.33842605]\n",
      "loss on epoch  234  :  1.59394969716 [0.28534371] [0.33078697]\n",
      "loss on epoch  235  :  1.59720753587 [0.28923476] [0.33467859]\n",
      "loss on epoch  236  :  1.59560884255 [0.29701686] [0.33496684]\n",
      "loss on epoch  237  :  1.59542596945 [0.28015563] [0.33251658]\n",
      "loss on epoch  238  :  1.59220009351 [0.27367055] [0.3377054]\n",
      "loss on epoch  239  :  1.5917280731 [0.30220494] [0.33381379]\n",
      "loss on epoch  240  :  1.59273186888 [0.27626458] [0.34116459]\n",
      "loss on epoch  241  :  1.59274913003 [0.27367055] [0.337273]\n",
      "loss on epoch  242  :  1.59249239383 [0.29182878] [0.34116459]\n",
      "loss on epoch  243  :  1.59151257553 [0.30090791] [0.33669645]\n",
      "loss on epoch  244  :  1.5942455798 [0.2775616] [0.33914673]\n",
      "loss on epoch  245  :  1.58795188907 [0.29182878] [0.33395791]\n",
      "loss on epoch  246  :  1.58512260776 [0.29961088] [0.34102046]\n",
      "loss on epoch  247  :  1.58765042865 [0.29571983] [0.3368406]\n",
      "loss on epoch  248  :  1.58421827658 [0.28664073] [0.34303835]\n",
      "loss on epoch  249  :  1.58485727293 [0.28664073] [0.34447968]\n",
      "loss on epoch  250  :  1.58729444984 [0.29312581] [0.33914673]\n",
      "loss on epoch  251  :  1.58321696606 [0.26848248] [0.34217355]\n",
      "loss on epoch  252  :  1.58843778009 [0.28923476] [0.33972326]\n",
      "loss on epoch  253  :  1.5850086765 [0.29442284] [0.34116459]\n",
      "loss on epoch  254  :  1.58840860318 [0.2775616] [0.33712885]\n",
      "loss on epoch  255  :  1.58502191219 [0.28015563] [0.33943498]\n",
      "loss on epoch  256  :  1.58078161357 [0.28404668] [0.34404728]\n",
      "loss on epoch  257  :  1.58022428855 [0.28923476] [0.34520036]\n",
      "loss on epoch  258  :  1.5830756901 [0.28145266] [0.34505621]\n",
      "loss on epoch  259  :  1.58032094309 [0.28793773] [0.34246179]\n",
      "loss on epoch  260  :  1.57987393504 [0.27885863] [0.34520036]\n",
      "loss on epoch  261  :  1.58377049107 [0.28015563] [0.33929086]\n",
      "loss on epoch  262  :  1.57781318651 [0.28145266] [0.3359758]\n",
      "loss on epoch  263  :  1.57776725638 [0.29442284] [0.33525512]\n",
      "loss on epoch  264  :  1.58158685677 [0.29312581] [0.34563276]\n",
      "loss on epoch  265  :  1.57752667907 [0.28793773] [0.3441914]\n",
      "loss on epoch  266  :  1.57728689823 [0.28404668] [0.35211876]\n",
      "loss on epoch  267  :  1.57402402681 [0.28534371] [0.34217355]\n",
      "loss on epoch  268  :  1.57194823113 [0.27626458] [0.34447968]\n",
      "loss on epoch  269  :  1.57641290582 [0.29442284] [0.34981263]\n",
      "loss on epoch  270  :  1.57785174812 [0.29053178] [0.343759]\n",
      "loss on epoch  271  :  1.57768320346 [0.2697795] [0.34822714]\n",
      "loss on epoch  272  :  1.57103791271 [0.27237353] [0.35644278]\n",
      "loss on epoch  273  :  1.57321587162 [0.28793773] [0.343759]\n",
      "loss on epoch  274  :  1.5729787056 [0.29312581] [0.34707409]\n",
      "loss on epoch  275  :  1.57188535341 [0.27107653] [0.33900261]\n",
      "loss on epoch  276  :  1.56832791325 [0.28404668] [0.35211876]\n",
      "loss on epoch  277  :  1.56911313966 [0.27367055] [0.34822714]\n",
      "loss on epoch  278  :  1.57296860995 [0.25940338] [0.34563276]\n",
      "loss on epoch  279  :  1.57148840116 [0.29312581] [0.34592101]\n",
      "loss on epoch  280  :  1.57051600408 [0.28664073] [0.34231767]\n",
      "loss on epoch  281  :  1.57291408898 [0.29961088] [0.34707409]\n",
      "loss on epoch  282  :  1.565831098 [0.28145266] [0.35038915]\n",
      "loss on epoch  283  :  1.56775942834 [0.28534371] [0.34361488]\n",
      "loss on epoch  284  :  1.56816553119 [0.28664073] [0.35096571]\n",
      "loss on epoch  285  :  1.56778172911 [0.27107653] [0.3548573]\n",
      "loss on epoch  286  :  1.57114019601 [0.27626458] [0.33943498]\n",
      "loss on epoch  287  :  1.56453608689 [0.29053178] [0.34433556]\n",
      "loss on epoch  288  :  1.56929924091 [0.27885863] [0.3446238]\n",
      "loss on epoch  289  :  1.56798517531 [0.28145266] [0.34491208]\n",
      "loss on epoch  290  :  1.56731935515 [0.26848248] [0.35514557]\n",
      "loss on epoch  291  :  1.56746249873 [0.28145266] [0.34808302]\n",
      "loss on epoch  292  :  1.56237046097 [0.28793773] [0.3552897]\n",
      "loss on epoch  293  :  1.56561315146 [0.28404668] [0.35139811]\n",
      "loss on epoch  294  :  1.56622335099 [0.28534371] [0.34822714]\n",
      "loss on epoch  295  :  1.56062916051 [0.28534371] [0.34938022]\n",
      "loss on epoch  296  :  1.56249964324 [0.29571983] [0.35629866]\n",
      "loss on epoch  297  :  1.56672552098 [0.29053178] [0.35038915]\n",
      "loss on epoch  298  :  1.56542755987 [0.29312581] [0.35384837]\n",
      "loss on epoch  299  :  1.56129760155 [0.28664073] [0.35543385]\n",
      "loss on epoch  300  :  1.56401248859 [0.27885863] [0.34909195]\n",
      "loss on epoch  301  :  1.56115893264 [0.2775616] [0.35226291]\n",
      "loss on epoch  302  :  1.56184436446 [0.29701686] [0.35125396]\n",
      "loss on epoch  303  :  1.55947515325 [0.30739298] [0.35644278]\n",
      "loss on epoch  304  :  1.56075630845 [0.28534371] [0.34995675]\n",
      "loss on epoch  305  :  1.56276087744 [0.29053178] [0.35255116]\n",
      "loss on epoch  306  :  1.55964823046 [0.30998704] [0.35557798]\n",
      "loss on epoch  307  :  1.55843533727 [0.27626458] [0.34952435]\n",
      "loss on epoch  308  :  1.55898785678 [0.28534371] [0.35716346]\n",
      "loss on epoch  309  :  1.55279939676 [0.27367055] [0.36278465]\n",
      "loss on epoch  310  :  1.55777931473 [0.29053178] [0.34765062]\n",
      "loss on epoch  311  :  1.55623694779 [0.28534371] [0.35154223]\n",
      "loss on epoch  312  :  1.55949163005 [0.28274968] [0.35428077]\n",
      "loss on epoch  313  :  1.55914581254 [0.30350193] [0.35543385]\n",
      "loss on epoch  314  :  1.55631851977 [0.28274968] [0.35125396]\n",
      "loss on epoch  315  :  1.56181482388 [0.30220494] [0.35168636]\n",
      "loss on epoch  316  :  1.55464390568 [0.2775616] [0.35442489]\n",
      "loss on epoch  317  :  1.55178000702 [0.29182878] [0.35356009]\n",
      "loss on epoch  318  :  1.55952101341 [0.29053178] [0.35831651]\n",
      "loss on epoch  319  :  1.55095859282 [0.29442284] [0.35961372]\n",
      "loss on epoch  320  :  1.55107389326 [0.28664073] [0.35889307]\n",
      "loss on epoch  321  :  1.55601706107 [0.29312581] [0.34894782]\n",
      "loss on epoch  322  :  1.55168117734 [0.27496758] [0.3561545]\n",
      "loss on epoch  323  :  1.54954182843 [0.2775616] [0.35946959]\n",
      "loss on epoch  324  :  1.54744014014 [0.28793773] [0.35687518]\n",
      "loss on epoch  325  :  1.55227681606 [0.27367055] [0.36292881]\n",
      "loss on epoch  326  :  1.54635335656 [0.28793773] [0.35687518]\n",
      "loss on epoch  327  :  1.54941915861 [0.28534371] [0.3561545]\n",
      "loss on epoch  328  :  1.54976532079 [0.29442284] [0.36379361]\n",
      "loss on epoch  329  :  1.55072136806 [0.29053178] [0.36451426]\n",
      "loss on epoch  330  :  1.54385475922 [0.28534371] [0.3561545]\n",
      "loss on epoch  331  :  1.54393426664 [0.27367055] [0.36451426]\n",
      "loss on epoch  332  :  1.54236346656 [0.30479896] [0.35903719]\n",
      "loss on epoch  333  :  1.5477453142 [0.29053178] [0.36134332]\n",
      "loss on epoch  334  :  1.54560586421 [0.26848248] [0.35817239]\n",
      "loss on epoch  335  :  1.5491548267 [0.28015563] [0.36321706]\n",
      "loss on epoch  336  :  1.54694283009 [0.26329443] [0.36494666]\n",
      "loss on epoch  337  :  1.54167940478 [0.27885863] [0.35961372]\n",
      "loss on epoch  338  :  1.54421128829 [0.28015563] [0.36105505]\n",
      "loss on epoch  339  :  1.54505009323 [0.28015563] [0.36393774]\n",
      "loss on epoch  340  :  1.5457862702 [0.26070037] [0.36321706]\n",
      "loss on epoch  341  :  1.54346239653 [0.28015563] [0.36177573]\n",
      "loss on epoch  342  :  1.54649431982 [0.29961088] [0.3607668]\n",
      "loss on epoch  343  :  1.54171908506 [0.2697795] [0.362064]\n",
      "loss on epoch  344  :  1.54643293135 [0.27107653] [0.36782935]\n",
      "loss on epoch  345  :  1.540095883 [0.26588845] [0.36437014]\n",
      "loss on epoch  346  :  1.54471881839 [0.28793773] [0.362064]\n",
      "loss on epoch  347  :  1.53662310994 [0.29053178] [0.36552322]\n",
      "loss on epoch  348  :  1.53683151024 [0.27107653] [0.37071201]\n",
      "loss on epoch  349  :  1.53400001146 [0.2775616] [0.36264053]\n",
      "loss on epoch  350  :  1.54328190589 [0.29701686] [0.36782935]\n",
      "loss on epoch  351  :  1.54193830576 [0.27626458] [0.36552322]\n",
      "loss on epoch  352  :  1.53721495815 [0.30090791] [0.36364946]\n",
      "loss on epoch  353  :  1.53751960008 [0.28664073] [0.36451426]\n",
      "loss on epoch  354  :  1.53647109305 [0.27626458] [0.36105505]\n",
      "loss on epoch  355  :  1.53651830079 [0.27885863] [0.36883828]\n",
      "loss on epoch  356  :  1.5369222121 [0.2775616] [0.35773998]\n",
      "loss on epoch  357  :  1.53331676255 [0.29182878] [0.35932547]\n",
      "loss on epoch  358  :  1.53749199881 [0.29961088] [0.36609975]\n",
      "loss on epoch  359  :  1.53799132333 [0.2775616] [0.36264053]\n",
      "loss on epoch  360  :  1.5341967638 [0.28274968] [0.36509082]\n",
      "loss on epoch  361  :  1.53634656858 [0.29571983] [0.36883828]\n",
      "loss on epoch  362  :  1.53538244185 [0.29571983] [0.36336121]\n",
      "loss on epoch  363  :  1.53189126132 [0.28145266] [0.36509082]\n",
      "loss on epoch  364  :  1.53451344414 [0.29053178] [0.36609975]\n",
      "loss on epoch  365  :  1.53408959033 [0.28404668] [0.36336121]\n",
      "loss on epoch  366  :  1.53450494659 [0.30739298] [0.38036898]\n",
      "loss on epoch  367  :  1.5317440629 [0.28923476] [0.37200922]\n",
      "loss on epoch  368  :  1.5333330493 [0.27107653] [0.36984721]\n",
      "loss on epoch  369  :  1.53109200104 [0.28145266] [0.36710867]\n",
      "loss on epoch  370  :  1.52549788053 [0.28923476] [0.36264053]\n",
      "loss on epoch  371  :  1.53090870726 [0.27367055] [0.37186509]\n",
      "loss on epoch  372  :  1.52685344565 [0.29831389] [0.3722975]\n",
      "loss on epoch  373  :  1.52789336616 [0.28793773] [0.36422601]\n",
      "loss on epoch  374  :  1.52847247711 [0.27367055] [0.37676564]\n",
      "loss on epoch  375  :  1.53084224808 [0.27626458] [0.37388295]\n",
      "loss on epoch  376  :  1.52843056641 [0.27626458] [0.36984721]\n",
      "loss on epoch  377  :  1.52930054803 [0.27367055] [0.37200922]\n",
      "loss on epoch  378  :  1.52681452468 [0.28534371] [0.37532431]\n",
      "loss on epoch  379  :  1.52257849168 [0.27885863] [0.3689824]\n",
      "loss on epoch  380  :  1.52531223971 [0.28145266] [0.37460363]\n",
      "loss on epoch  381  :  1.5275093844 [0.28404668] [0.38036898]\n",
      "loss on epoch  382  :  1.52625300919 [0.30090791] [0.36739695]\n",
      "loss on epoch  383  :  1.52466591372 [0.29701686] [0.36970308]\n",
      "loss on epoch  384  :  1.52335652407 [0.27367055] [0.3731623]\n",
      "loss on epoch  385  :  1.52454504414 [0.27885863] [0.37950417]\n",
      "loss on epoch  386  :  1.52013548215 [0.28274968] [0.37503603]\n",
      "loss on epoch  387  :  1.52317686012 [0.27367055] [0.3727299]\n",
      "loss on epoch  388  :  1.5243087791 [0.28015563] [0.38022485]\n",
      "loss on epoch  389  :  1.52405286958 [0.28015563] [0.37763044]\n",
      "loss on epoch  390  :  1.51812897299 [0.28664073] [0.37417123]\n",
      "loss on epoch  391  :  1.52378608265 [0.26070037] [0.36999136]\n",
      "loss on epoch  392  :  1.52483972777 [0.28404668] [0.37532431]\n",
      "loss on epoch  393  :  1.51841678222 [0.28145266] [0.37633324]\n",
      "loss on epoch  394  :  1.51755234642 [0.28923476] [0.36754107]\n",
      "loss on epoch  395  :  1.52047690965 [0.28534371] [0.37518016]\n",
      "loss on epoch  396  :  1.51762422617 [0.2775616] [0.37806284]\n",
      "loss on epoch  397  :  1.52061503866 [0.28274968] [0.37590083]\n",
      "loss on epoch  398  :  1.51954705041 [0.28923476] [0.38137791]\n",
      "loss on epoch  399  :  1.51609612634 [0.27885863] [0.38541368]\n",
      "loss on epoch  400  :  1.51879321838 [0.28534371] [0.36970308]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2b267ccb88f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mbatch_y\u001b[0m  \u001b[1;33m=\u001b[0m  \u001b[0mytrain\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0mtmpc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmlp_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mbatch_dx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_cx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpara_keep_prob\u001b[0m\u001b[1;33m,\u001b[0m                                       \u001b[0mpara_cur_lr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;31m#       learning rate decaying\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-e5cabb94d7a7>\u001b[0m in \u001b[0;36mtrain_batch\u001b[1;34m(self, dx_batch, cx_batch, y_batch, keep_prob, lr)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdx_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m                       \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdx_batch\u001b[0m\u001b[1;33m,\u001b[0m                                   \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mkeep_prob\u001b[0m                                 \u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "para_n_epoch = 1000\n",
    "\n",
    "# tunable parameters\n",
    "# representation ability\n",
    "para_n_hidden_list = [ 256,128,32 ]\n",
    "para_n_embedding = 2\n",
    "para_lr=0.007\n",
    "# regularization\n",
    "para_batch_size = 50\n",
    "para_keep_prob=0.9\n",
    "para_l2= 0.1\n",
    "\n",
    "\n",
    "# fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = 8\n",
    "\n",
    "tot_cnt= np.shape(cxtrain)[0]\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    mlp_clf = mlp_demo( para_batch_size, para_n_conti, para_n_disc, para_n_class, \\\n",
    "                        para_n_disc_voca, para_n_embedding, sess, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2 )\n",
    "    mlp_clf.train_ini()\n",
    "    mlp_clf.inference_ini()\n",
    "    \n",
    "    para_cur_lr=para_lr\n",
    "    total_batch = int(tot_cnt/para_batch_size)\n",
    "\n",
    "    tot_idx=range(tot_cnt)\n",
    "\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc=0.0\n",
    "        \n",
    "#       shuffle traning dataset each epoch  \n",
    "        np.random.shuffle(tot_idx)\n",
    "   \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = tot_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += mlp_clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                       para_cur_lr)\n",
    "        \n",
    "#       learning rate decaying\n",
    "        para_cur_lr = para_cur_lr*0.99\n",
    "        \n",
    "        tmp_test_acc = mlp_clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = mlp_clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only wide\n",
    "class wide_mlp_demo():\n",
    "    \n",
    "    def __init__(self, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, session, n_hidden_list, lr, l2):\n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list) \n",
    "        self.sess=session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        \n",
    "#       linnear part on categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "        \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"disc\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_uniform([self.N_DISC_VOCA, self.N_CLASS],-1.0, 1.0) )\n",
    "#                         tf.random_normal([self.N_DISC_VOCA, self.N_CLASS],\\\n",
    "#                         stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "                \n",
    "                if i==0:\n",
    "                    dx_wsum = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                else:\n",
    "                    dx_wsum = dx_wsum + tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "        \n",
    "        with tf.variable_scope(\"wide\"):\n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))            \n",
    "            dx_wsum = tf.add(dx_wsum, b)\n",
    "\n",
    "#----unit test---------------------------------------------------------\n",
    "#         print self.N_TOTAL\n",
    "#         self.tmpshape= tf.shape(x_concate)\n",
    "        \n",
    "#         self.tmpshape1= tf.shape(tmp_embeded[0])\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# #     hidden layers\n",
    "        with tf.variable_scope(\"h0\"):\n",
    "            w= tf.Variable(tf.random_normal([self.N_CONTI, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_CONTI)))) \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            h = tf.nn.relu( tf.add( tf.matmul(self.cx, w),b) )\n",
    "        \n",
    "                \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"layer\"+str(i)):\n",
    "                w= tf.Variable(tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b= tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                h = tf.nn.relu( tf.add( tf.matmul(h, w),b) )\n",
    "        \n",
    "#       Regularization\n",
    "#       dropout\n",
    "#         h = tf.nn.dropout(h, self.keep_prob)\n",
    "#       L2\n",
    "        self.regularizer = tf.nn.l2_loss(w)\n",
    "        \n",
    "#       output layer  \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w= tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1],\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            h = tf.add( tf.matmul(h, w),b)\n",
    "            \n",
    "#           L2  \n",
    "            self.regularizer = self.regularizer + tf.nn.l2_loss(w)\n",
    "            \n",
    "            self.logit = h + dx_wsum\n",
    "    \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y) \\\n",
    "                                  + self.L2*self.regularizer)\n",
    "        self.optimizer = \\\n",
    "        tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#       tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                       feed_dict={self.dx:dx_batch, \\\n",
    "                                  self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                  self.lr:lr,\\\n",
    "                                  self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "    \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                  self.cx:cx_test, self.y:y_test,\\\n",
    "                                                  self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        return sess.run( [self.tmpshape], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "para_n_epoch = 800\n",
    "\n",
    "# tunable parameters\n",
    "# representation ability\n",
    "para_n_hidden_list = [ 16,8 ]\n",
    "para_lr = 0.001\n",
    "# regularization\n",
    "para_batch_size = 128\n",
    "para_keep_prob = 0.9\n",
    "para_l2 = 0.1\n",
    "\n",
    "# fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = 8\n",
    "\n",
    "\n",
    "tot_cnt= np.shape(cxtrain)[0]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    wide_mlp_clf = wide_mlp_demo( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                        para_n_disc_voca, sess, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2 )\n",
    "    wide_mlp_clf.train_ini()\n",
    "    wide_mlp_clf.inference_ini()\n",
    "    \n",
    "    para_cur_lr=para_lr\n",
    "    total_batch = int(tot_cnt/para_batch_size)\n",
    "\n",
    "    tot_idx=range(tot_cnt)\n",
    "\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc=0.0\n",
    "        \n",
    "#       shuffle traning dataset each epoch  \n",
    "        np.random.shuffle(tot_idx)\n",
    "   \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = tot_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += wide_mlp_clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                       para_cur_lr)\n",
    "        \n",
    "#       learning rate decaying\n",
    "        para_cur_lr = para_cur_lr*0.98\n",
    "        \n",
    "        tmp_test_acc = wide_mlp_clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = wide_mlp_clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wide and embedding\n",
    "class wide_embed_NN():\n",
    "    \n",
    "    def __init__(self, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, session,n_embedding, n_hidden_list, lr, l2):\n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.N_EMBED = n_embedding\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list)\n",
    "        self.N_TOTAL = self.N_CONTI+ self.N_DISC*self.N_EMBED\n",
    "\n",
    "        self.sess=session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "#       wide part on categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "        \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"wide\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_uniform([self.N_DISC_VOCA, self.N_CLASS],-1.0, 1.0) )\n",
    "#                         tf.random_normal([self.N_DISC_VOCA, self.N_CLASS],\\\n",
    "#                         stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "                if i==0:\n",
    "                    dx_wsum = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                    \n",
    "#                   L2\n",
    "                    self.regularizer_wide = tf.nn.l2_loss(w)\n",
    "                    \n",
    "                else:\n",
    "                    dx_wsum = dx_wsum + tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                    \n",
    "#                   L2  \n",
    "                    self.regularizer_wide = self.regularizer_wide + tf.nn.l2_loss(w)\n",
    "                    \n",
    "#       nonlinear    \n",
    "        dx_wsum = tf.nn.relu( dx_wsum )\n",
    "\n",
    "            \n",
    "#       embedding categorical features\n",
    "#         self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "        \n",
    "        tmp_embeded = []                \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"embedding\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA, self.N_EMBED],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "                              \n",
    "                tmp_embeded.append( tf.nn.embedding_lookup( w, self.dx_trans[i] ) )\n",
    "                \n",
    "        dx_embeded = tf.concat(tmp_embeded, 1)\n",
    "        x_concate =  tf.concat( [self.cx,dx_embeded], 1 )\n",
    "\n",
    "#----unit test---------------------------------------------------------\n",
    "#         print self.N_TOTAL\n",
    "#         self.tmpshape= tf.shape(x_concate)\n",
    "        \n",
    "#         self.tmpshape1= tf.shape(tmp_embeded[0])\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# #     hidden layers\n",
    "        with tf.variable_scope(\"h0\"):\n",
    "            w= tf.Variable(tf.random_normal([self.N_TOTAL, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_CONTI)))) \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            h = tf.nn.relu( tf.add( tf.matmul(x_concate, w),b) )\n",
    "        \n",
    "                \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"layer\"+str(i)):\n",
    "                w= tf.Variable(tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b= tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                h = tf.nn.relu( tf.add( tf.matmul(h, w),b) )\n",
    "                \n",
    "                \n",
    "#       Regularization\n",
    "\n",
    "#       dropout\n",
    "#         if not (abs(self.keep_prob - 1.0) <= 1e-4):\n",
    "#             h = tf.nn.dropout(h, self.keep_prob)\n",
    "#       L2\n",
    "        self.regularizer = tf.nn.l2_loss(w)\n",
    "        \n",
    "        \n",
    "#       output layer  \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w= tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1],\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            \n",
    "            \n",
    "#             h = tf.add( tf.matmul(h, w), b )\n",
    "            \n",
    "            h = tf.add( tf.matmul(h, w), dx_wsum )\n",
    "            h = tf.add( h, b )\n",
    "#             \n",
    "            self.logit = h\n",
    "     \n",
    "    \n",
    "#           L2  \n",
    "            self.regularizer = self.regularizer + tf.nn.l2_loss(w)\n",
    "    \n",
    "#       overall L2\n",
    "        self.regularizer= self.regularizer + self.regularizer_wide\n",
    "         \n",
    "    \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y) \\\n",
    "                                  + self.L2*self.regularizer )\n",
    "        self.optimizer = \\\n",
    "        tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#       tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                       feed_dict={self.dx:dx_batch, \\\n",
    "                                  self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                  self.lr:lr,\\\n",
    "                                  self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "    \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                  self.cx:cx_test, self.y:y_test,\\\n",
    "                                                  self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        return sess.run( [self.tmpshape], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch  0  :  266.250372286 [0.1465629] [0.14557509]\n",
      "loss on epoch  1  :  261.815340819 [0.14915694] [0.14701644]\n",
      "loss on epoch  2  :  256.887796402 [0.15564202] [0.14889017]\n",
      "loss on epoch  3  :  251.546624996 [0.15823606] [0.15191698]\n",
      "loss on epoch  4  :  245.874232751 [0.15953307] [0.15249351]\n",
      "loss on epoch  5  :  239.944819627 [0.16601816] [0.15624098]\n",
      "loss on epoch  6  :  233.809127525 [0.16342412] [0.16070914]\n",
      "loss on epoch  7  :  227.512261921 [0.16990921] [0.16474488]\n",
      "loss on epoch  8  :  221.098424276 [0.1763943] [0.16849236]\n",
      "loss on epoch  9  :  214.603237788 [0.1841764] [0.17368117]\n",
      "loss on epoch  10  :  208.04042802 [0.18936446] [0.17858173]\n",
      "loss on epoch  11  :  201.449299565 [0.19584955] [0.1801672]\n",
      "loss on epoch  12  :  194.84480434 [0.19844358] [0.18319401]\n",
      "loss on epoch  13  :  188.249538987 [0.19844358] [0.18665321]\n",
      "loss on epoch  14  :  181.681809814 [0.19714656] [0.19025655]\n",
      "loss on epoch  15  :  175.16517399 [0.20233463] [0.19184203]\n",
      "loss on epoch  16  :  168.704715799 [0.20752271] [0.19371577]\n",
      "loss on epoch  17  :  162.321240037 [0.20752271] [0.19544537]\n",
      "loss on epoch  18  :  156.027536463 [0.20752271] [0.19904871]\n",
      "loss on epoch  19  :  149.830811324 [0.21141374] [0.20135486]\n",
      "loss on epoch  20  :  143.739254492 [0.21271077] [0.20337273]\n",
      "loss on epoch  21  :  137.769945215 [0.21011673] [0.20625541]\n",
      "loss on epoch  22  :  131.916074258 [0.21141374] [0.20928222]\n",
      "loss on epoch  23  :  126.19791098 [0.21271077] [0.21115595]\n",
      "loss on epoch  24  :  120.617001675 [0.21141374] [0.21375036]\n",
      "loss on epoch  25  :  115.17630895 [0.21271077] [0.2160565]\n",
      "loss on epoch  26  :  109.881666678 [0.21789883] [0.21749784]\n",
      "loss on epoch  27  :  104.733998157 [0.21400778] [0.21951571]\n",
      "loss on epoch  28  :  99.74093031 [0.21141374] [0.22355145]\n",
      "loss on epoch  29  :  94.9004372138 [0.21400778] [0.22744307]\n",
      "loss on epoch  30  :  90.2148337541 [0.21141374] [0.2288844]\n",
      "loss on epoch  31  :  85.6873033665 [0.21400778] [0.23061401]\n",
      "loss on epoch  32  :  81.3175946342 [0.21660182] [0.23176707]\n",
      "loss on epoch  33  :  77.1004679998 [0.21400778] [0.23493803]\n",
      "loss on epoch  34  :  73.0387799298 [0.22049287] [0.23839723]\n",
      "loss on epoch  35  :  69.1348547759 [0.22049287] [0.24099164]\n",
      "loss on epoch  36  :  65.3829796049 [0.22438392] [0.24127991]\n",
      "loss on epoch  37  :  61.7836162956 [0.22827497] [0.24416259]\n",
      "loss on epoch  38  :  58.330643389 [0.230869] [0.24603632]\n",
      "loss on epoch  39  :  55.0297629745 [0.22827497] [0.24877486]\n",
      "loss on epoch  40  :  51.8756488517 [0.22827497] [0.24863073]\n",
      "loss on epoch  41  :  48.8660451218 [0.23476005] [0.24906313]\n",
      "loss on epoch  42  :  45.9980938346 [0.23605707] [0.25223407]\n",
      "loss on epoch  43  :  43.2631058693 [0.23605707] [0.2536754]\n",
      "loss on epoch  44  :  40.6659099967 [0.2386511] [0.2541078]\n",
      "loss on epoch  45  :  38.2001419597 [0.24383917] [0.25511676]\n",
      "loss on epoch  46  :  35.8589774238 [0.2464332] [0.25569329]\n",
      "loss on epoch  47  :  33.6428507876 [0.24902724] [0.2587201]\n",
      "loss on epoch  48  :  31.5492672567 [0.25291827] [0.25857595]\n",
      "loss on epoch  49  :  29.5697661506 [0.25680932] [0.26117036]\n",
      "loss on epoch  50  :  27.7076123997 [0.25680932] [0.26203516]\n",
      "loss on epoch  51  :  25.9513109172 [0.25680932] [0.26261172]\n",
      "loss on epoch  52  :  24.300324387 [0.25940338] [0.26419717]\n",
      "loss on epoch  53  :  22.7496404118 [0.2542153] [0.26578265]\n",
      "loss on epoch  54  :  21.295801719 [0.26070037] [0.26679158]\n",
      "loss on epoch  55  :  19.9369566794 [0.25940338] [0.26909772]\n",
      "loss on epoch  56  :  18.6660475024 [0.25680932] [0.2702508]\n",
      "loss on epoch  57  :  17.4805783784 [0.25551233] [0.2711156]\n",
      "loss on epoch  58  :  16.3746979325 [0.25680932] [0.27356586]\n",
      "loss on epoch  59  :  15.3469971904 [0.25810635] [0.27515134]\n",
      "loss on epoch  60  :  14.3916551846 [0.26070037] [0.27558374]\n",
      "loss on epoch  61  :  13.5064362288 [0.25551233] [0.27601615]\n",
      "loss on epoch  62  :  12.6866427881 [0.25940338] [0.27933121]\n",
      "loss on epoch  63  :  11.9282277602 [0.25940338] [0.27961949]\n",
      "loss on epoch  64  :  11.2262423833 [0.26070037] [0.28062841]\n",
      "loss on epoch  65  :  10.5830551827 [0.25940338] [0.28163737]\n",
      "loss on epoch  66  :  9.98951971089 [0.26070037] [0.28452003]\n",
      "loss on epoch  67  :  9.44582614634 [0.26329443] [0.28653792]\n",
      "loss on epoch  68  :  8.94618994218 [0.26329443] [0.28682616]\n",
      "loss on epoch  69  :  8.48926302239 [0.26459143] [0.28884405]\n",
      "loss on epoch  70  :  8.0691047355 [0.2619974] [0.28970885]\n",
      "loss on epoch  71  :  7.68488612661 [0.26070037] [0.29042953]\n",
      "loss on epoch  72  :  7.33225009618 [0.25810635] [0.29100606]\n",
      "loss on epoch  73  :  7.01021119621 [0.25940338] [0.29230326]\n",
      "loss on epoch  74  :  6.71712841811 [0.2542153] [0.29345632]\n",
      "loss on epoch  75  :  6.45132233699 [0.25551233] [0.29403287]\n",
      "loss on epoch  76  :  6.20545895453 [0.2542153] [0.29561833]\n",
      "loss on epoch  77  :  5.9825397774 [0.25940338] [0.29619488]\n",
      "loss on epoch  78  :  5.77767104793 [0.26070037] [0.30008647]\n",
      "loss on epoch  79  :  5.58821836887 [0.26070037] [0.30037475]\n",
      "loss on epoch  80  :  5.41436618567 [0.26329443] [0.30412224]\n",
      "loss on epoch  81  :  5.25397858796 [0.2619974] [0.30498701]\n",
      "loss on epoch  82  :  5.10565768348 [0.26329443] [0.30570769]\n",
      "loss on epoch  83  :  4.96832243381 [0.2697795] [0.30570769]\n",
      "loss on epoch  84  :  4.84037453378 [0.26588845] [0.30772558]\n",
      "loss on epoch  85  :  4.71977538533 [0.2697795] [0.30859038]\n",
      "loss on epoch  86  :  4.60758111874 [0.27107653] [0.30931103]\n",
      "loss on epoch  87  :  4.50123034362 [0.26848248] [0.3116172]\n",
      "loss on epoch  88  :  4.40004554281 [0.27107653] [0.31190544]\n",
      "loss on epoch  89  :  4.30421536609 [0.27367055] [0.31147304]\n",
      "loss on epoch  90  :  4.21257011316 [0.27496758] [0.31204957]\n",
      "loss on epoch  91  :  4.12521211417 [0.27367055] [0.31291437]\n",
      "loss on epoch  92  :  4.0408831018 [0.27367055] [0.31334677]\n",
      "loss on epoch  93  :  3.9627907519 [0.27367055] [0.31435573]\n",
      "loss on epoch  94  :  3.88848259493 [0.2775616] [0.31594118]\n",
      "loss on epoch  95  :  3.81524017122 [0.28015563] [0.31651774]\n",
      "loss on epoch  96  :  3.74588399022 [0.28145266] [0.31709427]\n",
      "loss on epoch  97  :  3.67991291925 [0.28404668] [0.31767079]\n",
      "loss on epoch  98  :  3.61593278801 [0.28274968] [0.3185356]\n",
      "loss on epoch  99  :  3.55526578426 [0.28015563] [0.31997693]\n",
      "loss on epoch  100  :  3.49701403026 [0.28274968] [0.32213894]\n",
      "loss on epoch  101  :  3.43919699501 [0.28534371] [0.32213894]\n",
      "loss on epoch  102  :  3.38607642606 [0.28404668] [0.32415682]\n",
      "loss on epoch  103  :  3.33383821117 [0.28015563] [0.32574227]\n",
      "loss on epoch  104  :  3.28393958675 [0.27626458] [0.32588643]\n",
      "loss on epoch  105  :  3.23553480594 [0.28534371] [0.32718363]\n",
      "loss on epoch  106  :  3.18980812254 [0.28534371] [0.32732776]\n",
      "loss on epoch  107  :  3.14455063696 [0.28404668] [0.32675123]\n",
      "loss on epoch  108  :  3.10142491261 [0.28793773] [0.32718363]\n",
      "loss on epoch  109  :  3.05929990609 [0.28923476] [0.32862496]\n",
      "loss on epoch  110  :  3.01741515155 [0.29053178] [0.32876909]\n",
      "loss on epoch  111  :  2.97961251714 [0.29571983] [0.3309311]\n",
      "loss on epoch  112  :  2.94262862426 [0.29442284] [0.32848084]\n",
      "loss on epoch  113  :  2.90531787607 [0.29831389] [0.32948977]\n",
      "loss on epoch  114  :  2.87046425541 [0.29701686] [0.33150765]\n",
      "loss on epoch  115  :  2.8346338813 [0.29571983] [0.33194005]\n",
      "loss on epoch  116  :  2.80176892545 [0.29961088] [0.33064285]\n",
      "loss on epoch  117  :  2.76863281153 [0.29831389] [0.33453444]\n",
      "loss on epoch  118  :  2.73631371392 [0.29701686] [0.33525512]\n",
      "loss on epoch  119  :  2.70521993107 [0.30090791] [0.33611992]\n",
      "loss on epoch  120  :  2.67516867651 [0.29831389] [0.33482271]\n",
      "loss on epoch  121  :  2.64507335314 [0.29701686] [0.3364082]\n",
      "loss on epoch  122  :  2.61591774004 [0.30090791] [0.3368406]\n",
      "loss on epoch  123  :  2.5884393277 [0.29831389] [0.337273]\n",
      "loss on epoch  124  :  2.56046641977 [0.29701686] [0.33943498]\n",
      "loss on epoch  125  :  2.53499245202 [0.30350193] [0.33784953]\n",
      "loss on epoch  126  :  2.50907237331 [0.29831389] [0.34087634]\n",
      "loss on epoch  127  :  2.48589309829 [0.29442284] [0.34246179]\n",
      "loss on epoch  128  :  2.46250923916 [0.29831389] [0.33900261]\n",
      "loss on epoch  129  :  2.43766833235 [0.29571983] [0.343759]\n",
      "loss on epoch  130  :  2.41657265248 [0.29571983] [0.34217355]\n",
      "loss on epoch  131  :  2.3938429168 [0.29571983] [0.34073219]\n",
      "loss on epoch  132  :  2.37281596109 [0.29312581] [0.3428942]\n",
      "loss on epoch  133  :  2.35180556995 [0.29571983] [0.34275007]\n",
      "loss on epoch  134  :  2.3323153158 [0.29831389] [0.34563276]\n",
      "loss on epoch  135  :  2.31301133721 [0.29831389] [0.34476796]\n",
      "loss on epoch  136  :  2.2943920747 [0.29961088] [0.34563276]\n",
      "loss on epoch  137  :  2.27687181919 [0.29831389] [0.34476796]\n",
      "loss on epoch  138  :  2.25960453738 [0.29831389] [0.34563276]\n",
      "loss on epoch  139  :  2.24212590357 [0.29831389] [0.34649754]\n",
      "loss on epoch  140  :  2.22660008956 [0.29571983] [0.34664169]\n",
      "loss on epoch  141  :  2.21063197873 [0.29701686] [0.34635341]\n",
      "loss on epoch  142  :  2.1951756505 [0.29571983] [0.34750649]\n",
      "loss on epoch  143  :  2.18066845172 [0.29831389] [0.34808302]\n",
      "loss on epoch "
     ]
    }
   ],
   "source": [
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "#   representation ability\n",
    "para_n_hidden_list = [ 128,64,8 ]\n",
    "#     128,16,8 ]\n",
    "para_lr = 0.045\n",
    "# 0.05\n",
    "para_n_embedding = 2\n",
    "#   regularization\n",
    "para_batch_size = 32\n",
    "para_keep_prob = 1.0\n",
    "para_l2 = 0.3\n",
    "# 0.15\n",
    "\n",
    "# fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = 8\n",
    "\n",
    "tot_cnt= np.shape(cxtrain)[0]\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    wide_mlp_clf = wide_embed_NN( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                      para_n_disc_voca, sess, para_n_embedding, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2 )\n",
    "    wide_mlp_clf.train_ini()\n",
    "    wide_mlp_clf.inference_ini()\n",
    "    \n",
    "    para_cur_lr=para_lr\n",
    "    total_batch = int(tot_cnt/para_batch_size)\n",
    "\n",
    "    tot_idx=range(tot_cnt)\n",
    "\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc=0.0\n",
    "        \n",
    "#       shuffle traning dataset each epoch  \n",
    "        np.random.shuffle(tot_idx)\n",
    "   \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = tot_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += wide_mlp_clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                       para_cur_lr)\n",
    "        \n",
    "#       learning rate decaying\n",
    "#         para_cur_lr = para_cur_lr*0.98\n",
    "        \n",
    "        tmp_test_acc = wide_mlp_clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = wide_mlp_clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
