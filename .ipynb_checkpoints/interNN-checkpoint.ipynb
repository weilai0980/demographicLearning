{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wide and embedding, \n",
    "# wide co-occurrence\n",
    "# wide co-occurrence & interaction \n",
    "\n",
    "# TO DO\n",
    "# batch normalization   https://r2rt.com/implementing-batch-normalization-in-tensorflow.html\n",
    "# batch norm specific for training and testing\n",
    "# https://gist.github.com/tomokishii/0ce3bdac1588b5cca9fa5fbdf6e1c412\n",
    "# weight normalization\n",
    "\n",
    "# optimize the efficiency of codes \n",
    "\n",
    "# start with low regularization, large loss\n",
    "#  more parameters, high learning rate\n",
    "\n",
    "# reguliarization slows down the convergence rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTES:\n",
    "\n",
    "# ---Ini\n",
    "\n",
    "# Xavier  1/n_in\n",
    "# He's 2/n_in\n",
    "# Orthogonal\n",
    "\n",
    "# ---Activation\n",
    "\n",
    "\n",
    "# ---Optimizer\n",
    "\n",
    "# Adam\n",
    "# Adadelta\n",
    "\n",
    "# ---Regularization\n",
    "# dropout + max norm\n",
    "# batch normalization\n",
    "# weight normalization\n",
    "\n",
    "# regularization:  closeness of traning and validation performance, training stableness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from six.moves import urllib\n",
    "# from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# continuous and categorical feature split\n",
    "\n",
    "# features on columns [0:split_idx) are continuous\n",
    "# the rest are categorical\n",
    "def conti_cate_split(dta_df, split_idx):\n",
    "    \n",
    "    col_cnt= dta_df.shape[1]\n",
    "    conti_cols= range(split_idx)\n",
    "    dis_cols=range(split_idx, col_cnt)\n",
    "    \n",
    "    return dta_df[conti_cols], dta_df[dis_cols]\n",
    "\n",
    "def conti_normalization_train_dta(dta_df):\n",
    "    \n",
    "    return preprocessing.scale(dta_df)\n",
    "\n",
    "def conti_normalization_test_dta(dta_df, train_df):\n",
    "    \n",
    "    mean_dim = np.mean(train_df, axis=0)\n",
    "    std_dim = np.std(train_df, axis=0)\n",
    "        \n",
    "    df=pd.DataFrame()\n",
    "    cols = train_df.columns\n",
    "    idx=0\n",
    "    \n",
    "    for i in cols:\n",
    "        df[i] = (dta_df[i]- mean_dim[idx])*1.0/std_dim[idx]\n",
    "        idx=idx+1\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6938, 186) (771, 186) (6938, 1) (771, 1)\n",
      "(6938, 106) (6938, 80)\n",
      "(771, 106) (771, 80)\n"
     ]
    }
   ],
   "source": [
    "# Demographic data\n",
    "# ------------data prepro-----------------------\n",
    "\n",
    "files_list=[\"../dataset/dataset_demo/xtrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/xtest.csv\",\\\n",
    "            \"../dataset/dataset_demo/ytrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/ytest.csv\"]\n",
    "\n",
    "xtrain_df=pd.read_csv( files_list[0] ,sep=',', header=None)\n",
    "xtest_df=pd.read_csv( files_list[1] ,sep=',', header=None)\n",
    "ytrain_df=pd.read_csv( files_list[2] ,sep=',', header=None)\n",
    "ytest_df=pd.read_csv( files_list[3] ,sep=',', header=None)\n",
    "\n",
    "\n",
    "# get training and testing data prepared\n",
    "\n",
    "cxtrain, dxtrain = conti_cate_split(xtrain_df, 106)\n",
    "cxtest, dxtest = conti_cate_split(xtest_df, 106)\n",
    "\n",
    "cxtest = conti_normalization_test_dta(cxtest, cxtrain)\n",
    "cxtrain = conti_normalization_train_dta(cxtrain)\n",
    "\n",
    "# cxtrain = cxtrain.as_matrix() \n",
    "dxtrain = dxtrain.as_matrix().astype(int)\n",
    "cxtest = cxtest.as_matrix()\n",
    "dxtest = dxtest.as_matrix().astype(int)\n",
    "\n",
    "ytest = np_utils.to_categorical( ytest_df.as_matrix() )\n",
    "ytrain =np_utils.to_categorical( ytrain_df.as_matrix() )\n",
    "\n",
    "\n",
    "print np.shape(cxtrain), np.shape(dxtrain), np.shape(ytrain)\n",
    "print np.shape(cxtest), np.shape(dxtest), np.shape(ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6938, 186) (771, 186) (6938, 1) (771, 1)\n",
      "(6938, 20) (6938, 166)\n",
      "(771, 20) (771, 166)\n"
     ]
    }
   ],
   "source": [
    "# Default data\n",
    "# ------------data prepro-----------------------\n",
    "\n",
    "files_list=[\"../dataset/dataset_demo/xtrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/xtest.csv\",\\\n",
    "            \"../dataset/dataset_demo/ytrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/ytest.csv\"]\n",
    "\n",
    "xtrain_df=pd.read_csv( files_list[0] ,sep=',', header=None)\n",
    "xtest_df=pd.read_csv( files_list[1] ,sep=',', header=None)\n",
    "ytrain_df=pd.read_csv( files_list[2] ,sep=',', header=None)\n",
    "ytest_df=pd.read_csv( files_list[3] ,sep=',', header=None)\n",
    "\n",
    "# get training and testing data prepared\n",
    "\n",
    "cxtrain, dxtrain = conti_cate_split(xtrain_df, 20)\n",
    "cxtest, dxtest = conti_cate_split(xtest_df, 20)\n",
    "\n",
    "cxtest = conti_normalization_test_dta(cxtest, cxtrain)\n",
    "cxtrain = conti_normalization_train_dta(cxtrain)\n",
    "\n",
    "# cxtrain = cxtrain.as_matrix() \n",
    "dxtrain = dxtrain.as_matrix().astype(int)\n",
    "cxtest = cxtest.as_matrix()\n",
    "dxtest = dxtest.as_matrix().astype(int)\n",
    "\n",
    "ytest = np_utils.to_categorical( ytest_df.as_matrix() )\n",
    "ytrain =np_utils.to_categorical( ytrain_df.as_matrix() )\n",
    "\n",
    "print np.shape(cxtrain), np.shape(dxtrain), np.shape(ytrain)\n",
    "print np.shape(cxtest), np.shape(dxtest), np.shape(ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wide, embedding, co-occurrence and feature interaction \n",
    "# version 1: interaction as external hiddens\n",
    "\n",
    "class wide_embed_coocc_NN():\n",
    "    \n",
    "#   build the network graph\n",
    "    def __init__(self, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, session,n_embedding, n_hidden_list, lr, l2):\n",
    "        \n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.N_EMBED = n_embedding\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list)\n",
    "        self.N_TOTAL = self.N_CONTI+ self.N_DISC*self.N_EMBED\n",
    "\n",
    "        self.sess=session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "#       wide part on categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "        \n",
    "#         for i in range(self.N_DISC):\n",
    "            \n",
    "#             with tf.variable_scope(\"wide\"+str(i)):\n",
    "#                 w= tf.Variable(\\\n",
    "#                 tf.random_uniform([self.N_DISC_VOCA[i], self.N_CLASS],-1.0, 1.0) )\n",
    "# #                         tf.random_normal([self.N_DISC_VOCA, self.N_CLASS],\\\n",
    "# #                         stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "\n",
    "#                 b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "\n",
    "#                 if i==0:\n",
    "#                     dx_wsum = tf.nn.embedding_lookup( w, self.dx_trans[i] ) + b\n",
    "                    \n",
    "# #                   L2\n",
    "#                     self.regularizer_wide = tf.nn.l2_loss(w)\n",
    "                    \n",
    "#                 else:\n",
    "#                     dx_wsum = dx_wsum + tf.nn.embedding_lookup( w, self.dx_trans[i] ) + b\n",
    "                    \n",
    "# #                   L2  \n",
    "#                     self.regularizer_wide = self.regularizer_wide + tf.nn.l2_loss(w)\n",
    "        \n",
    "        \n",
    "# #       non-linear\n",
    "#         dx_wsum = tf.nn.relu( dx_wsum )\n",
    "\n",
    "        \n",
    "#       co-occurrence of categorical features\n",
    "        for i in range(20):\n",
    "            \n",
    "            tmpvoca1 = self.N_DISC_VOCA[i] \n",
    "            \n",
    "            for j in range(i+1,20):\n",
    "                \n",
    "                tmpvoca2 = self.N_DISC_VOCA[j]\n",
    "                \n",
    "                with tf.variable_scope(\"cooccur\"+str(i)+str(j)):\n",
    "                    \n",
    "#                   vector approximate of co-occurrence matrix \n",
    "                    w1 = tf.Variable(tf.random_normal([self.N_CLASS, tmpvoca1, 1],\\\n",
    "                                            stddev=math.sqrt(2.0/float( tmpvoca1 ))) )\n",
    "                    \n",
    "                    w2 = tf.Variable(tf.random_normal([self.N_CLASS, 1, tmpvoca2],\\\n",
    "                                            stddev=math.sqrt(2.0/float( tmpvoca2 ))) )\n",
    "            \n",
    "                    \n",
    "                    b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "\n",
    "                    \n",
    "                    cooc_mat = []\n",
    "                    for k in range(self.N_CLASS):\n",
    "                        cooc_mat.append( tf.reshape( tf.matmul(w1[k],w2[k]), [-1,1] ) )\n",
    "                        \n",
    "#                   n_class by voca1*voca2  \n",
    "                    cooc_vec = tf.stack(cooc_mat)\n",
    "                    cooc_vec = tf.reshape(cooc_vec, [self.N_CLASS, -1] )\n",
    "                    cooc_vec = tf.transpose(cooc_vec,[1,0])\n",
    "                    \n",
    "#                   build the idx of co-occurrence features\n",
    "                    cooc_idx = self.dx_trans[i] * tmpvoca2 + self.dx_trans[j]\n",
    "    \n",
    "                    if i==0 and j==1:\n",
    "                        cooc_wsum  = tf.nn.embedding_lookup( cooc_vec, cooc_idx ) + b\n",
    "#                       L2\n",
    "                        self.regularizer_cooc   = tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2)\n",
    "                    else:\n",
    "                        cooc_wsum += tf.nn.embedding_lookup( cooc_vec, cooc_idx ) + b\n",
    "#                       L2\n",
    "                        self.regularizer_cooc += ( tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2) )\n",
    "    \n",
    "        \n",
    "#       non-linear\n",
    "        cooc_wsum = tf.nn.relu( cooc_wsum )\n",
    "\n",
    "    \n",
    "#       feature interaction\n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"interact\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA[i], self.N_CONTI],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "        \n",
    "                if i==0:\n",
    "                    inter_wsum  = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "#                   L2 \n",
    "                    self.regularizer_inter  = tf.nn.l2_loss(w)\n",
    "            \n",
    "                else:\n",
    "                    inter_wsum += tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "#                   L2 \n",
    "                    self.regularizer_inter += tf.nn.l2_loss(w)\n",
    "\n",
    "    \n",
    "#       add noise bias\n",
    "        singleCol_h = tf.reduce_sum( self.cx * inter_wsum, 1 )\n",
    "        singleCol_h = tf.reshape( singleCol_h, [-1,1] )\n",
    "        \n",
    "#       interaction hidden layers\n",
    "        with tf.variable_scope(\"inter_h0\"):\n",
    "            w = tf.Variable(tf.random_normal([1, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(n_hidden_list[0])))) \n",
    "                        \n",
    "            b = tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            \n",
    "            h_inter = tf.matmul( singleCol_h, w )\n",
    "            h_inter = tf.nn.relu( h_inter + b)\n",
    "#           L2  \n",
    "            self.regularizer_inter += tf.nn.l2_loss(w)\n",
    "            \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"inter_h\"+str(i)):\n",
    "                w = tf.Variable(\\\n",
    "                        tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b = tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                h_inter = tf.nn.relu( tf.add( tf.matmul(h_inter, w),b) )\n",
    "#               L2\n",
    "                self.regularizer_inter += tf.nn.l2_loss(w)\n",
    "    \n",
    "    \n",
    "#       embedding of categorical features        \n",
    "        tmp_embeded = []                \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"embedding\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA[i], self.N_EMBED],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "                              \n",
    "                tmp_embeded.append( tf.nn.embedding_lookup( w, self.dx_trans[i] ) )\n",
    "                \n",
    "        dx_embeded = tf.concat(tmp_embeded, 1)\n",
    "        x_concate =  tf.concat( [self.cx,dx_embeded], 1 )\n",
    "        \n",
    "#       embedding + continuous hidden layers\n",
    "        with tf.variable_scope(\"h0\"):\n",
    "            w= tf.Variable(tf.random_normal([self.N_TOTAL, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_CONTI)))) \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            h = tf.nn.relu( tf.add( tf.matmul(x_concate, w),b) )\n",
    "#           L2\n",
    "            self.regularizer = tf.nn.l2_loss(w)\n",
    "        \n",
    "                \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"layer\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b= tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                h = tf.nn.relu( tf.add( tf.matmul(h, w),b) )\n",
    "                \n",
    "#               L2\n",
    "                self.regularizer += tf.nn.l2_loss(w)\n",
    "        \n",
    "                \n",
    "#       dropout\n",
    "#       h = tf.nn.dropout(h, self.keep_prob)\n",
    "\n",
    "        \n",
    "#       output layer  \n",
    "        h = tf.concat([h, h_inter], 1)\n",
    "    \n",
    "        with tf.variable_scope(\"temp\"):\n",
    "            w= tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1]*2,\\\n",
    "                                             n_hidden_list[self.N_HIDDEN_LAYERS-1] ],\\\n",
    "                        stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1]))))\n",
    "                           \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[self.N_HIDDEN_LAYERS-1] ] ))\n",
    "            h = tf.nn.relu( tf.add( tf.matmul(h, w),b) )\n",
    "                           \n",
    "#           L2  \n",
    "            self.regularizer += tf.nn.l2_loss(w) \n",
    "\n",
    "    \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w= tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1],\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            \n",
    "            h = tf.matmul(h, w)\n",
    "#             h = tf.add( h, dx_wsum ) #wide\n",
    "            h = tf.add( h, cooc_wsum ) #co-occurrence\n",
    "            h = tf.add( h, b )\n",
    "\n",
    "            self.logit = h\n",
    "     \n",
    "    \n",
    "#           L2  \n",
    "            self.regularizer += tf.nn.l2_loss(w) \n",
    "    \n",
    "#       overall L2\n",
    "        self.regularizer += (self.regularizer_cooc + self.regularizer_inter)\n",
    "#     self.regularizer_wide + \n",
    "         \n",
    "    \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y)) \\\n",
    "                                  + self.L2*self.regularizer\n",
    "        self.optimizer = \\\n",
    "        tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#       tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                       feed_dict={self.dx:dx_batch, \\\n",
    "                                  self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                  self.lr:lr,\\\n",
    "                                  self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "    \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                  self.cx:cx_test, self.y:y_test,\\\n",
    "                                                  self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        return sess.run( [self.tmpshape], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch  0  :  125.562676889 [0.19714656] [0.19184203]\n",
      "loss on epoch  10  :  103.0058379 [0.1919585] [0.20985875]\n",
      "loss on epoch  20  :  78.8748071459 [0.21011673] [0.23508215]\n",
      "loss on epoch  30  :  56.2327757765 [0.22568093] [0.24488325]\n",
      "loss on epoch  40  :  38.4282539332 [0.23735408] [0.24819833]\n",
      "loss on epoch  50  :  25.7996697426 [0.24254215] [0.26462957]\n",
      "loss on epoch  60  :  17.4615739098 [0.24124514] [0.26924187]\n",
      "loss on epoch  70  :  12.2084200956 [0.24513619] [0.27616027]\n",
      "loss on epoch  80  :  9.03294288229 [0.25551233] [0.29360047]\n",
      "loss on epoch  90  :  7.09382124521 [0.25551233] [0.29749209]\n",
      "loss on epoch  100  :  5.86688907941 [0.25940338] [0.3083021]\n",
      "loss on epoch  110  :  5.0558954102 [0.2775616] [0.31291437]\n",
      "loss on epoch  120  :  4.49443837007 [0.28664073] [0.32228309]\n",
      "loss on epoch  130  :  4.09488211407 [0.28923476] [0.32732776]\n",
      "loss on epoch  140  :  3.77220187364 [0.29312581] [0.33165178]\n",
      "loss on epoch  150  :  3.51076422245 [0.30090791] [0.33669645]\n",
      "loss on epoch  160  :  3.30103076829 [0.30350193] [0.34029979]\n",
      "loss on epoch  170  :  3.13012642441 [0.30609599] [0.34404728]\n",
      "loss on epoch  180  :  2.98648472737 [0.31258106] [0.34822714]\n",
      "loss on epoch  190  :  2.86373199247 [0.31517509] [0.35370424]\n",
      "loss on epoch  200  :  2.75809692785 [0.30739298] [0.3552897]\n",
      "loss on epoch  210  :  2.66571122739 [0.30220494] [0.35889307]\n",
      "loss on epoch  220  :  2.58515745953 [0.30090791] [0.36292881]\n",
      "loss on epoch  230  :  2.51335038079 [0.29442284] [0.36797348]\n",
      "loss on epoch  240  :  2.45003745953 [0.28793773] [0.3722975]\n",
      "loss on epoch  250  :  2.39266953535 [0.29312581] [0.37748632]\n",
      "loss on epoch  260  :  2.34294281182 [0.28664073] [0.38036898]\n",
      "loss on epoch  270  :  2.29684470539 [0.28793773] [0.38916114]\n",
      "loss on epoch  280  :  2.25517235365 [0.28923476] [0.39391756]\n",
      "loss on epoch  290  :  2.21810523614 [0.29312581] [0.40155664]\n",
      "loss on epoch  300  :  2.18201676949 [0.29312581] [0.40804267]\n",
      "loss on epoch  310  :  2.14757333972 [0.28404668] [0.41654655]\n",
      "loss on epoch  320  :  2.11779287568 [0.27107653] [0.42533872]\n",
      "loss on epoch  330  :  2.08685795135 [0.2619974] [0.43283367]\n",
      "loss on epoch  340  :  2.05778278576 [0.26459143] [0.44090515]\n",
      "loss on epoch  350  :  2.0290976818 [0.26070037] [0.44883251]\n",
      "loss on epoch  360  :  2.00155798484 [0.26459143] [0.45892188]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c584f32abbf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mbatch_y\u001b[0m  \u001b[1;33m=\u001b[0m  \u001b[0mytrain\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0mtmpc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mbatch_dx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_cx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpara_keep_prob\u001b[0m\u001b[1;33m,\u001b[0m                                     \u001b[0mpara_cur_lr\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mpara_eval_byepoch\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-db66b3f63355>\u001b[0m in \u001b[0;36mtrain_batch\u001b[1;34m(self, dx_batch, cx_batch, y_batch, keep_prob, lr)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdx_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m                       \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdx_batch\u001b[0m\u001b[1;33m,\u001b[0m                                   \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mkeep_prob\u001b[0m                                 \u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# wide, embedding, co-occurrence and feature interaction \n",
    "# version 1: interaction as external hiddens\n",
    "\n",
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "\n",
    "#   representation ability\n",
    "para_n_hidden_list = [ 32,32,16 ]\n",
    "#     128,16,8 ]\n",
    "para_lr = 0.045\n",
    "para_n_embedding = 2\n",
    "#   regularization\n",
    "para_batch_size = 32\n",
    "para_keep_prob = 1.0\n",
    "para_l2 = 0.01\n",
    "\n",
    "# fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = [8]*para_n_disc\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 10\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    clf = wide_embed_coocc_NN( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                      para_n_disc_voca, sess, para_n_embedding, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2)\n",
    "    clf.train_ini()\n",
    "    clf.inference_ini()\n",
    "    \n",
    "    para_cur_lr=para_lr\n",
    "    \n",
    "    total_cnt= np.shape(cxtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "    \n",
    "    total_idx=range(total_cnt)\n",
    "    \n",
    "    \n",
    "#   training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc=0.0\n",
    "        \n",
    "#       shuffle traning instances each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "        \n",
    "#       Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                     para_cur_lr )\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "        \n",
    "        tmp_test_acc  = clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "        \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wide, embedding, co-occurrence and feature interaction \n",
    "# version 2: individual hidden layers on embedding and interaction\n",
    "\n",
    "class InterNN_IndiH():\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, session,n_embedding, n_hidden_list, lr, l2, batch_size):\n",
    "        \n",
    "#       build the network graph \n",
    "        self.LEARNING_RATE = lr\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.N_EMBED = n_embedding\n",
    "        self.N_BATCH = batch_size\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list)\n",
    "        self.N_TOTAL = self.N_CONTI+ self.N_DISC*self.N_EMBED\n",
    "        self.N_TOTAL_EMBED = self.N_DISC*self.N_EMBED\n",
    "\n",
    "        self.sess=session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "#       wide part on categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "    \n",
    "    \n",
    "    \n",
    "#         for i in range(self.N_DISC):\n",
    "            \n",
    "#             with tf.variable_scope(\"wide\"+str(i)):\n",
    "#                 w= tf.Variable(\\\n",
    "#                 tf.random_uniform([self.N_DISC_VOCA[i], self.N_CLASS],-1.0, 1.0) )\n",
    "# #                         tf.random_normal([self.N_DISC_VOCA, self.N_CLASS],\\\n",
    "# #                         stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "#                 if i==0:\n",
    "#                     dx_wsum = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                    \n",
    "# #                   L2\n",
    "#                     self.regularizer_wide = tf.nn.l2_loss(w)\n",
    "                    \n",
    "#                 else:\n",
    "#                     dx_wsum = dx_wsum + tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                    \n",
    "# #                   L2  \n",
    "#                     self.regularizer_wide = self.regularizer_wide + tf.nn.l2_loss(w)\n",
    "               \n",
    "    \n",
    "    \n",
    "    \n",
    "#       co-occurrence of categorical features\n",
    "        for i in range(20):\n",
    "            \n",
    "            tmpvoca1 = self.N_DISC_VOCA[i] \n",
    "            \n",
    "            for j in range(i+1,20):\n",
    "                \n",
    "                tmpvoca2 = self.N_DISC_VOCA[j]\n",
    "                \n",
    "                with tf.variable_scope(\"cooccur\"+str(i)+str(j)):\n",
    "                    \n",
    "                    \n",
    "#                   vector approximate of co-occurrence matrix \n",
    "                    w1 = tf.Variable(tf.random_normal([self.N_CLASS, tmpvoca1, 1],\\\n",
    "                                            stddev=math.sqrt(2.0/float( tmpvoca1 ))) )\n",
    "                    \n",
    "                    w2 = tf.Variable(tf.random_normal([self.N_CLASS, 1, tmpvoca2],\\\n",
    "                                            stddev=math.sqrt(2.0/float( tmpvoca2 ))) )\n",
    "            \n",
    "                    \n",
    "                    b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "\n",
    "                    \n",
    "                    cooc_mat = []\n",
    "                    for k in range(self.N_CLASS):\n",
    "                        cooc_mat.append( tf.reshape( tf.matmul(w1[k],w2[k]), [-1,1] ) )\n",
    "                        \n",
    "#                   n_class by voca1*voca2  \n",
    "        \n",
    "                    cooc_vec = tf.stack(cooc_mat)\n",
    "                    cooc_vec = tf.reshape(cooc_vec, [self.N_CLASS, -1] )\n",
    "                    cooc_vec = tf.transpose(cooc_vec,[1,0])\n",
    "                    \n",
    "#                   build the idx of co-occurrence features\n",
    "                    cooc_idx = self.dx_trans[i] * tmpvoca2 + self.dx_trans[j]\n",
    "    \n",
    "                    if i==0 and j==1:\n",
    "                        cooc_wsum  = tf.nn.embedding_lookup( cooc_vec, cooc_idx ) + b\n",
    "#                       L2 \n",
    "                        self.regularizer_cooc   = tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2) \n",
    "                    else:\n",
    "                        cooc_wsum += tf.nn.embedding_lookup( cooc_vec, cooc_idx ) + b\n",
    "#                       L2\n",
    "                        self.regularizer_cooc += (tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2))\n",
    "    \n",
    "#       non-linear \n",
    "        cooc_wsum = tf.nn.relu( cooc_wsum )\n",
    "        \n",
    "        \n",
    "        \n",
    "#       embedding of categorical features        \n",
    "        tmp_embeded = []                \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"embedding\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA[i], self.N_EMBED],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "                              \n",
    "                tmp_embeded.append( tf.nn.embedding_lookup( w, self.dx_trans[i] ) )\n",
    "                \n",
    "        dx_embeded = tf.concat(tmp_embeded, 1)\n",
    "        \n",
    "        \n",
    "#       embedding hidden layers\n",
    "        with tf.variable_scope(\"embed_h0\"):\n",
    "            w= tf.Variable(tf.random_normal([self.N_TOTAL_EMBED, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_CONTI)))) \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            h_embed = tf.nn.relu( tf.add( tf.matmul(dx_embeded, w),b) )\n",
    "            \n",
    "#           L2   \n",
    "            self.regularizer = tf.nn.l2_loss(w)\n",
    "                \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"embed_h\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b= tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                h_embed = tf.nn.relu( tf.add( tf.matmul(h_embed, w),b) )       \n",
    "            \n",
    "#               L2   \n",
    "                self.regularizer += tf.nn.l2_loss(w)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#       feature interaction\n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"interact\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA[i], self.N_CONTI],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "        \n",
    "                if i==0:\n",
    "                    inter_wsum  = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "#                   L2 \n",
    "                    self.regularizer_inter  = tf.nn.l2_loss(w)\n",
    "            \n",
    "                else:\n",
    "                    inter_wsum += tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "#                   L2 \n",
    "                    self.regularizer_inter += tf.nn.l2_loss(w)\n",
    "\n",
    "#       add noise bias\n",
    "        singleCol_h = tf.reduce_sum( self.cx * inter_wsum, 1   )\n",
    "        tmp_h=[]\n",
    "        for i in range( n_hidden_list[0] ):\n",
    "            with tf.variable_scope(\"noise\"+str( i )):\n",
    "                noise_b = tf.Variable(tf.random_normal([ ],\\\n",
    "                        stddev=math.sqrt(2.0/float(n_hidden_list[0]))))\n",
    "                tmp_h.append( singleCol_h + noise_b ) \n",
    "                \n",
    "#               L2  \n",
    "                self.regularizer_inter += tf.nn.l2_loss(noise_b)\n",
    "        \n",
    "#         tmp_h=[]\n",
    "#         for i in range( n_hidden_list[0] ):\n",
    "#             with tf.variable_scope(\"noise\"+str( i )):\n",
    "#                 noise_b = tf.Variable(tf.random_normal([ self.N_CONTI ],\\\n",
    "#                         stddev=math.sqrt(2.0/float(n_hidden_list[0]))))\n",
    "#                 tmp_h.append( tf.reduce_sum( self.cx * (inter_wsum + noise_b), 1 ) ) \n",
    "        \n",
    "        tmp_h = tf.stack( tmp_h )\n",
    "        tmp_h = tf.transpose( tmp_h, [1,0])\n",
    "            \n",
    "        \n",
    "#       interaction hidden layers\n",
    "\n",
    "\n",
    "#         singleCol_h = tf.reduce_sum( self.cx * inter_wsum, 1   )\n",
    "        \n",
    "#         tmp_h = [ singleCol_h ]*n_hidden_list[0]\n",
    "        \n",
    "#         tmp_h = tf.stack( tmp_h )\n",
    "#         tmp_h = tf.transpose( tmp_h, [1,0])\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope(\"inter_h0\"):\n",
    "            w = tf.Variable(tf.random_normal([self.N_CONTI, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_CONTI)))) \n",
    "                        \n",
    "            b = tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            \n",
    "            h_inter = tf.matmul(self.cx, w)\n",
    "            h_inter = h_inter + tmp_h\n",
    "            h_inter = tf.nn.relu( h_inter + b)\n",
    "                             \n",
    "#           L2  \n",
    "            self.regularizer += tf.nn.l2_loss(w)\n",
    "            \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"inter_h\"+str(i)):\n",
    "                w = tf.Variable(\\\n",
    "                        tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b = tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                h_inter = tf.nn.relu( tf.add( tf.matmul(h_inter, w),b) )\n",
    "        \n",
    "#               L2\n",
    "                self.regularizer += tf.nn.l2_loss(w)\n",
    "    \n",
    "    \n",
    "#       dropout\n",
    "#       h = tf.nn.dropout(h, self.keep_prob)\n",
    "        \n",
    "        \n",
    "#       output layer  \n",
    "        h = tf.concat([h_embed, h_inter], 1)\n",
    "        \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w = tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1]*2,\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "            b = tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            \n",
    "            h = tf.matmul(h, w)\n",
    "#             h = tf.add( h, dx_wsum )\n",
    "            h = tf.add( h, cooc_wsum )\n",
    "            h = tf.add( h, b )\n",
    "        \n",
    "            self.logit = h\n",
    "            \n",
    "#           L2  \n",
    "            self.regularizer += tf.nn.l2_loss(w) \n",
    "    \n",
    "    \n",
    "#       overall L2\n",
    "        self.regularizer += (self.regularizer_cooc + self.regularizer_inter)\n",
    "#       self.regularizer_wide + \n",
    "         \n",
    "        \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y) ) \\\n",
    "                                  + self.L2*self.regularizer\n",
    "        self.optimizer = \\\n",
    "        tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#       tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                       feed_dict={self.dx:dx_batch, \\\n",
    "                                  self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                  self.lr:lr,\\\n",
    "                                  self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "        \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                  self.cx:cx_test, self.y:y_test,\\\n",
    "                                                  self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        return sess.run( [self.tmp1], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wide, embedding, co-occurrence and feature interaction \n",
    "# version 2: individual hidden layers on embedding and interaction\n",
    "\n",
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "\n",
    "#   representation ability\n",
    "para_n_hidden_list = [ 128,64,32,16 ]\n",
    "#     128,16,8 ]\n",
    "para_lr = 0.075\n",
    "para_n_embedding = 2\n",
    "#   regularization\n",
    "para_batch_size = 32\n",
    "para_keep_prob = 1.0\n",
    "para_l2 = 0.25\n",
    "\n",
    "# fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = [8]*para_n_disc\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 10\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    clf = InterNN_IndiH( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                      para_n_disc_voca, sess, para_n_embedding, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2, para_batch_size)\n",
    "    clf.train_ini()\n",
    "    clf.inference_ini()\n",
    "    \n",
    "    para_cur_lr=para_lr\n",
    "    \n",
    "    total_cnt= np.shape(cxtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "    \n",
    "    total_idx=range(total_cnt)\n",
    "    \n",
    "    \n",
    "#   training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc=0.0\n",
    "        \n",
    "#       shuffle traning instances each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "        \n",
    "#       Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                     para_cur_lr )\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "        \n",
    "        tmp_test_acc  = clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "        \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 5 [5]\n",
      "1.0\n",
      "2.08166817117e-17\n",
      "0.813187183618\n",
      "0.194308854259\n"
     ]
    }
   ],
   "source": [
    "def orthogonal(shape):\n",
    "\n",
    "        flat_shape = (shape[0], np.prod(shape[1:]))\n",
    "        \n",
    "        print shape[0],np.prod(shape[1:]), shape[1:]\n",
    "        \n",
    "        a = np.random.normal(0.0, 1.0, flat_shape)\n",
    "        \n",
    "        u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "        q = u if u.shape == flat_shape else v\n",
    "        \n",
    "        return q.reshape(shape)\n",
    "    \n",
    "tmp = orthogonal([3,5])\n",
    "\n",
    "print sum( [tmp[0][i]*tmp[0][i] for i in range(5)] )\n",
    "\n",
    "print sum( [tmp[0][i]*tmp[2][i] for i in range(5)] )\n",
    "\n",
    "print sum( [tmp[i][0]*tmp[i][0] for i in range(3)] )\n",
    "\n",
    "print sum( [tmp[i][0]*tmp[i][2] for i in range(3)] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wide, embedding, co-occurrence and feature interaction\n",
    "# version 3: fuse all into hidden layers\n",
    "\n",
    "class InterNN_fuse():\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, session,n_embedding, n_hidden_list, lr, l2, batch_size, \\\n",
    "                 max_norm ):\n",
    "        \n",
    "#       build the network graph \n",
    "        self.LEARNING_RATE = lr\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC  = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.N_EMBED = n_embedding\n",
    "        self.N_BATCH = batch_size\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list)\n",
    "        self.N_TOTAL = self.N_CONTI+ self.N_DISC*self.N_EMBED\n",
    "        self.N_TOTAL_EMBED = self.N_DISC*self.N_EMBED\n",
    "   \n",
    "        self.MAX_NORM = max_norm\n",
    "        self.epsilon = 1e-3\n",
    "        \n",
    "        self.sess = session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y  = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "#       wide part on categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "    \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"wide\"+str(i)):\n",
    "#               !change\n",
    "                w= tf.Variable(\\\n",
    "                               tf.random_normal([self.N_DISC_VOCA[i], self.N_CLASS],\\\n",
    "                        stddev = math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "#               tf.random_uniform([self.N_DISC_VOCA[i], self.N_CLASS],-1.0, 1.0) )\n",
    "\n",
    "#               max norm constraint\n",
    "#                 w = tf.clip_by_norm( w, self.MAX_NORM, axes = 1)\n",
    "\n",
    "#               !! change\n",
    "                b  = tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "\n",
    "                if i==0:\n",
    "                    dx_wsum = tf.nn.embedding_lookup( w, self.dx_trans[i] ) + b\n",
    "#                   L2\n",
    "                    self.regularizer_wide  =  tf.nn.l2_loss(w)\n",
    "                    \n",
    "                else:\n",
    "                    dx_wsum = dx_wsum + tf.nn.embedding_lookup( w, self.dx_trans[i] ) + b\n",
    "#                   L2  \n",
    "                    self.regularizer_wide += tf.nn.l2_loss(w)\n",
    "        \n",
    "    \n",
    "#       nonlinear activation\n",
    "#         dx_wsum = tf.nn.relu( dx_wsum )\n",
    "        dx_wsum = tf.maximum( 0.01*dx_wsum, dx_wsum )\n",
    "               \n",
    "    \n",
    "    \n",
    "#       co-occurrence of categorical features\n",
    "        for i in range(20):\n",
    "            \n",
    "            tmpvoca1 = self.N_DISC_VOCA[i] \n",
    "            \n",
    "            for j in range(i+1,20):\n",
    "                \n",
    "                tmpvoca2 = self.N_DISC_VOCA[j]\n",
    "                \n",
    "                with tf.variable_scope(\"cooccur\"+str(i)+str(j)):\n",
    "                    \n",
    "#                   vector approximate of co-occurrence matrix \n",
    "                    w1 = tf.Variable(tf.random_normal([self.N_CLASS, tmpvoca1, 1],\\\n",
    "                                            stddev=math.sqrt(2.0/float( tmpvoca1 ))) )\n",
    "                    \n",
    "                    w2 = tf.Variable(tf.random_normal([self.N_CLASS, 1, tmpvoca2],\\\n",
    "                                            stddev=math.sqrt(2.0/float( tmpvoca2 ))) )\n",
    "                    \n",
    "#                   !!! change\n",
    "                    b  = tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "                    \n",
    "                    cooc_mat = []\n",
    "                    for k in range(self.N_CLASS):\n",
    "                        cooc_mat.append( tf.reshape( tf.matmul(w1[k],w2[k]), [-1,1] ) )\n",
    "                        \n",
    "#                   n_class by voca1*voca2  \n",
    "                    cooc_vec = tf.stack(cooc_mat)\n",
    "                    cooc_vec = tf.reshape(cooc_vec, [self.N_CLASS, -1] )\n",
    "                    cooc_vec = tf.transpose(cooc_vec,[1,0])\n",
    "            \n",
    "#                   max norm constraint\n",
    "#                     cooc_vec = tf.clip_by_norm( cooc_vec, self.MAX_NORM, axes = 0)\n",
    "    \n",
    "                    \n",
    "#                   build the idx of co-occurrence features\n",
    "                    cooc_idx = self.dx_trans[i] * tmpvoca2 + self.dx_trans[j]\n",
    "    \n",
    "                    if i==0 and j==1:\n",
    "#                       co-occurrence weighted sum\n",
    "\n",
    "#                       !!! change\n",
    "                        cooc_wsum  = tf.nn.embedding_lookup( cooc_vec, cooc_idx ) + b\n",
    "#                       L2 \n",
    "                        self.regularizer_cooc  = tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2) \n",
    "                    else:\n",
    "            \n",
    "#                       !!! change\n",
    "                        cooc_wsum += (tf.nn.embedding_lookup( cooc_vec, cooc_idx ) + b) \n",
    "#                       L2\n",
    "                        self.regularizer_cooc += (tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2))\n",
    "    \n",
    "#       nonlinear activation\n",
    "#         cooc_wsum = tf.nn.relu( cooc_wsum )\n",
    "        cooc_wsum = tf.maximum( 0.01*cooc_wsum, cooc_wsum )\n",
    "        \n",
    "        \n",
    "#       embedding of categorical features        \n",
    "        tmp_embeded = []                \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"embedding\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA[i], self.N_EMBED],\\\n",
    "                        stddev = math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "                              \n",
    "                tmp_embeded.append( tf.nn.embedding_lookup( w, self.dx_trans[i] ) )\n",
    "                \n",
    "        dx_embeded = tf.concat(tmp_embeded, 1)\n",
    "        x_concate  = tf.concat( [self.cx,dx_embeded], 1 )\n",
    "        \n",
    "        \n",
    "#       feature interaction\n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"interact\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA[i], self.N_CONTI],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "        \n",
    "#               max norm constraint\n",
    "#                 w = tf.clip_by_norm( w, self.MAX_NORM, axes = 1)\n",
    "            \n",
    "            \n",
    "                if i==0:\n",
    "                    inter_wsum  = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "#                   L2 \n",
    "                    self.regularizer_inter  = tf.nn.l2_loss(w)\n",
    "            \n",
    "                else:\n",
    "                    inter_wsum += tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "#                   L2 \n",
    "                    self.regularizer_inter += tf.nn.l2_loss(w)\n",
    "\n",
    "    \n",
    "#       add noise bias\n",
    "        singleCol_h = tf.reduce_sum( self.cx * inter_wsum, 1   )\n",
    "        tmp_h=[]\n",
    "        for i in range( n_hidden_list[0] ):\n",
    "            with tf.variable_scope(\"noise\"+str( i )):\n",
    "                noise_b = tf.Variable(tf.random_normal([ ],\\\n",
    "                        stddev=math.sqrt(2.0/float(n_hidden_list[0]))))\n",
    "                \n",
    "                tmp_h.append( singleCol_h + noise_b ) \n",
    "        \n",
    "#         tmp_h=[]\n",
    "#         for i in range( n_hidden_list[0] ):\n",
    "#             with tf.variable_scope(\"noise\"+str( i )):\n",
    "#                 noise_b = tf.Variable(tf.random_normal([ self.N_CONTI ],\\\n",
    "#                         stddev=math.sqrt(2.0/float(n_hidden_list[0]))))\n",
    "#                 tmp_h.append( tf.reduce_sum( self.cx * (inter_wsum + noise_b), 1 ) ) \n",
    "        \n",
    "        \n",
    "        tmp_h = tf.stack( tmp_h )\n",
    "        tmp_h = tf.transpose( tmp_h, [1,0])\n",
    "        \n",
    "        \n",
    "#       dropout\n",
    "#         h_inter = tf.nn.dropout(h_inter, self.keep_prob)\n",
    "        \n",
    "        \n",
    "#       hidden layers on fusion\n",
    "        with tf.variable_scope(\"inter_h0\"):\n",
    "            \n",
    "#          conventional one   \n",
    "            w= tf.Variable(tf.random_normal([self.N_TOTAL, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_TOTAL)))) \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "        \n",
    "#           max norm constraint\n",
    "#             w = tf.clip_by_norm( w, self.MAX_NORM, axes = 0) \n",
    "            \n",
    "            h_inter = tf.matmul(x_concate, w)\n",
    "            h_inter = h_inter + tmp_h\n",
    "#           h_inter = tf.add( h_inter, b )\n",
    "            \n",
    "            \n",
    "#           batch normalization \n",
    "            batch_m, batch_v = tf.nn.moments(h_inter,[0])\n",
    "            scale = tf.Variable( tf.ones( [ n_hidden_list[0] ]) )\n",
    "            beta  = tf.Variable( tf.zeros([ n_hidden_list[0] ]) )\n",
    "            \n",
    "            h_inter = \\\n",
    "            tf.nn.batch_normalization(h_inter, batch_m, batch_v, beta, scale,\\\n",
    "                                      self.epsilon)\n",
    "            \n",
    "#           nonlinear activation   \n",
    "#             h_inter = tf.nn.relu( h_inter )\n",
    "            h_inter = tf.maximum( 0.01*h_inter, h_inter )\n",
    "            \n",
    "#           L2   \n",
    "            self.regularizer = tf.nn.l2_loss(w)\n",
    "        \n",
    "#       dropout\n",
    "#         h_inter = tf.nn.dropout(h_inter, self.keep_prob)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"inter_h\"+str(i)):\n",
    "                w = tf.Variable(\\\n",
    "                        tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b = tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                \n",
    "#               max norm constraint\n",
    "#                 w = tf.clip_by_norm( w, self.MAX_NORM,axes = 0)\n",
    "        \n",
    "                h_inter = tf.matmul(h_inter, w)\n",
    "#               h_inter = tf.add(h_inter,b)\n",
    "                \n",
    "                \n",
    "#               batch normalization \n",
    "                batch_m, batch_v = tf.nn.moments(h_inter,[0])\n",
    "                scale = tf.Variable( tf.ones( [ n_hidden_list[i] ]) )\n",
    "                beta  = tf.Variable( tf.zeros([ n_hidden_list[i] ]) )\n",
    "            \n",
    "                h_inter = \\\n",
    "                tf.nn.batch_normalization(h_inter, batch_m, batch_v, beta, scale,\\\n",
    "                                          self.epsilon)\n",
    "\n",
    "#               nonlinear activation  \n",
    "                h_inter = tf.maximum( 0.01*h_inter, h_inter )\n",
    "#                 h_inter = tf.nn.relu( h_inter )\n",
    "        \n",
    "#               L2\n",
    "                self.regularizer += tf.nn.l2_loss(w)\n",
    "    \n",
    "#               dropout\n",
    "#                 h_inter = tf.nn.dropout(h_inter, self.keep_prob)\n",
    "        \n",
    "        \n",
    "#       output layer  \n",
    "        h = h_inter\n",
    "        \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w = tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1],\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev = math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "            b = tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            \n",
    "            h = tf.matmul(h, w)\n",
    "            \n",
    "            h = tf.add( h, dx_wsum )\n",
    "            h = tf.add( h, cooc_wsum )\n",
    "            h = tf.add( h, b )\n",
    "        \n",
    "            self.logit = h\n",
    "            \n",
    "#           L2  \n",
    "            self.regularizer += tf.nn.l2_loss(w) \n",
    "    \n",
    "#       overall L2\n",
    "        self.regularizer += \\\n",
    "        (self.regularizer_cooc + self.regularizer_inter + self.regularizer_wide)\n",
    "\n",
    "        \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y)) \\\n",
    "                                   + self.L2*self.regularizer\n",
    "            \n",
    "        self.optimizer = \\\n",
    "        tf.train.AdamOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                        feed_dict={self.dx:dx_batch,\\\n",
    "                                   self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                   self.lr:lr,\\\n",
    "                                   self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "        \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                    self.cx:cx_test, self.y:y_test,\\\n",
    "                                                    self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        return sess.run( [self.tmp1], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception KeyboardInterrupt in <bound method TF_Buffer.<lambda> of <tensorflow.python.pywrap_tensorflow.TF_Buffer; proxy of <Swig Object of type 'TF_Buffer *' at 0x7f4b490cbd20> >> ignored\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[1;34m(self, code_obj, result)\u001b[0m\n\u001b[0;32m   2892\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2893\u001b[0m             \u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"To exit: use 'exit', 'quit', or Ctrl-D.\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2894\u001b[1;33m         \u001b[1;32mexcept\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcustom_exceptions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2895\u001b[0m             \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from interNN import *\n",
    "\n",
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "\n",
    "#   representation ability\n",
    "para_n_hidden_list = [ 256, 128, 32 ]\n",
    "#     128,16,8\n",
    "para_lr = 0.015\n",
    "\n",
    "# !!! change\n",
    "para_n_embedding = 3\n",
    "\n",
    "#   regularization\n",
    "para_batch_size = 32\n",
    "para_l2 = 0.015\n",
    "\n",
    "# !!!change\n",
    "para_keep_prob = 0.8\n",
    "para_max_norm = 7\n",
    "\n",
    "#  fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = [8]*para_n_disc\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    clf = InterNN_fuse( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                        para_n_disc_voca, sess, para_n_embedding, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2, para_batch_size, para_max_norm)\n",
    "    \n",
    "    \n",
    "#   change  \n",
    "    clf.inference_ini()\n",
    "    clf.train_ini()\n",
    "    \n",
    "    \n",
    "    para_cur_lr = para_lr\n",
    "\n",
    "    total_cnt   = np.shape(cxtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "    \n",
    "    total_idx = range(total_cnt)\n",
    "    \n",
    "#   training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc = 0.0\n",
    "        \n",
    "#       shuffle traning instances each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "    \n",
    "#       Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                     para_cur_lr )\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "    \n",
    "        tmp_test_acc  = clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wide, embedding, co-occurrence and feature interaction\n",
    "# version 3: fuse all into hidden layers, with orthogonal initialization\n",
    "\n",
    "class InterNN_fuse():\n",
    "    \n",
    "    \n",
    "#   orthogonal initialization of weights \n",
    "#   return row vector  \n",
    "    def orthogonal(self, shape):\n",
    "\n",
    "        flat_shape = (shape[0], np.prod(shape[1:]))\n",
    "        a = np.random.normal(0.0, 1.0, flat_shape)\n",
    "        \n",
    "        u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "        q = u if u.shape == flat_shape else v\n",
    "        \n",
    "        return q.reshape(shape)\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, session,n_embedding, n_hidden_list, lr, l2, batch_size, \\\n",
    "                 max_norm ):\n",
    "        \n",
    "#       build the network graph \n",
    "        self.LEARNING_RATE = lr\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC  = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.N_EMBED = n_embedding\n",
    "        self.N_BATCH = batch_size\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list)\n",
    "        self.N_TOTAL = self.N_CONTI+ self.N_DISC*self.N_EMBED\n",
    "        self.N_TOTAL_EMBED = self.N_DISC*self.N_EMBED\n",
    "   \n",
    "        self.MAX_NORM = max_norm\n",
    "        self.epsilon = 1e-3\n",
    "        \n",
    "        self.sess = session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y  = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "#       wide part on categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "    \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"wide\"+str(i)):\n",
    "#               !change\n",
    "                w= tf.Variable(\\\n",
    "                               tf.random_normal([self.N_DISC_VOCA[i], self.N_CLASS],\\\n",
    "                        stddev = math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "#               tf.constant(orthogonal( [self.N_DISC_VOCA[i], self.N_CLASS] )) )\n",
    "#               tf.random_uniform([self.N_DISC_VOCA[i], self.N_CLASS],-1.0, 1.0) )\n",
    "\n",
    "#               max norm constraint\n",
    "#                 w = tf.clip_by_norm( w, self.MAX_NORM, axes = 1)\n",
    "\n",
    "                if i==0:\n",
    "                    dx_wsum = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "#                   L2\n",
    "                    self.regularizer_wide  =  tf.nn.l2_loss(w)\n",
    "                    \n",
    "                else:\n",
    "                    dx_wsum = dx_wsum + tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "#                   L2  \n",
    "                    self.regularizer_wide += tf.nn.l2_loss(w)\n",
    "        \n",
    "    \n",
    "#       nonlinear activation\n",
    "#         dx_wsum = tf.nn.relu( dx_wsum )\n",
    "        dx_wsum = tf.maximum( 0.01*dx_wsum, dx_wsum )\n",
    "               \n",
    "    \n",
    "    \n",
    "#       co-occurrence of categorical features\n",
    "        for i in range(20):\n",
    "            \n",
    "            tmpvoca1 = self.N_DISC_VOCA[i] \n",
    "            \n",
    "            for j in range(i+1,20):\n",
    "                \n",
    "                tmpvoca2 = self.N_DISC_VOCA[j]\n",
    "                \n",
    "                with tf.variable_scope(\"cooccur\"+str(i)+str(j)):\n",
    "                    \n",
    "#                   vector approximate of co-occurrence matrix \n",
    "                    w1 = tf.Variable(tf.random_normal([self.N_CLASS, tmpvoca1, 1],\\\n",
    "                                            stddev=math.sqrt(2.0/float( tmpvoca1 ))) )\n",
    "                    \n",
    "                    w2 = tf.Variable(tf.random_normal([self.N_CLASS, 1, tmpvoca2],\\\n",
    "                                            stddev=math.sqrt(2.0/float( tmpvoca2 ))) )\n",
    "                    \n",
    "#                   !!! change\n",
    "                    b  = tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "                    \n",
    "                    cooc_mat = []\n",
    "                    for k in range(self.N_CLASS):\n",
    "                        cooc_mat.append( tf.reshape( tf.matmul(w1[k],w2[k]), [-1,1] ) )\n",
    "                        \n",
    "#                   n_class by voca1*voca2  \n",
    "                    cooc_vec = tf.stack(cooc_mat)\n",
    "                    cooc_vec = tf.reshape(cooc_vec, [self.N_CLASS, -1] )\n",
    "                    cooc_vec = tf.transpose(cooc_vec,[1,0])\n",
    "            \n",
    "#                   max norm constraint\n",
    "#                     cooc_vec = tf.clip_by_norm( cooc_vec, self.MAX_NORM, axes = 0)\n",
    "    \n",
    "                    \n",
    "#                   build the idx of co-occurrence features\n",
    "                    cooc_idx = self.dx_trans[i] * tmpvoca2 + self.dx_trans[j]\n",
    "    \n",
    "                    if i==0 and j==1:\n",
    "#                       co-occurrence weighted sum\n",
    "\n",
    "#                       !!! change\n",
    "                        cooc_wsum  = tf.nn.embedding_lookup( cooc_vec, cooc_idx )\n",
    "#                       L2 \n",
    "                        self.regularizer_cooc  = tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2) \n",
    "                    else:\n",
    "            \n",
    "#                       !!! change\n",
    "                        cooc_wsum += tf.nn.embedding_lookup( cooc_vec, cooc_idx )\n",
    "#                       L2\n",
    "                        self.regularizer_cooc += (tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2))\n",
    "    \n",
    "#       nonlinear activation\n",
    "#         cooc_wsum = tf.nn.relu( cooc_wsum )\n",
    "        cooc_wsum = tf.maximum( 0.01*cooc_wsum, cooc_wsum )\n",
    "        \n",
    "        \n",
    "#       embedding of categorical features        \n",
    "        tmp_embeded = []                \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"embedding\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA[i], self.N_EMBED],\\\n",
    "                        stddev = math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "                              \n",
    "                tmp_embeded.append( tf.nn.embedding_lookup( w, self.dx_trans[i] ) )\n",
    "                \n",
    "        dx_embeded = tf.concat(tmp_embeded, 1)\n",
    "        x_concate  = tf.concat( [self.cx,dx_embeded], 1 )\n",
    "        \n",
    "        \n",
    "#       feature interaction\n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"interact\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA[i], self.N_CONTI],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "        \n",
    "#               max norm constraint\n",
    "#                 w = tf.clip_by_norm( w, self.MAX_NORM, axes = 1)\n",
    "            \n",
    "            \n",
    "                if i==0:\n",
    "                    inter_wsum  = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "#                   L2 \n",
    "                    self.regularizer_inter  = tf.nn.l2_loss(w)\n",
    "            \n",
    "                else:\n",
    "                    inter_wsum += tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "#                   L2 \n",
    "                    self.regularizer_inter += tf.nn.l2_loss(w)\n",
    "\n",
    "    \n",
    "#       add noise bias\n",
    "        singleCol_h = tf.reduce_sum( self.cx * inter_wsum, 1   )\n",
    "        tmp_h=[]\n",
    "        for i in range( n_hidden_list[0] ):\n",
    "            with tf.variable_scope(\"noise\"+str( i )):\n",
    "                noise_b = tf.Variable(tf.random_normal([ ],\\\n",
    "                        stddev=math.sqrt(2.0/float(n_hidden_list[0]))))\n",
    "                \n",
    "                tmp_h.append( singleCol_h + noise_b ) \n",
    "        \n",
    "#         tmp_h=[]\n",
    "#         for i in range( n_hidden_list[0] ):\n",
    "#             with tf.variable_scope(\"noise\"+str( i )):\n",
    "#                 noise_b = tf.Variable(tf.random_normal([ self.N_CONTI ],\\\n",
    "#                         stddev=math.sqrt(2.0/float(n_hidden_list[0]))))\n",
    "#                 tmp_h.append( tf.reduce_sum( self.cx * (inter_wsum + noise_b), 1 ) ) \n",
    "        \n",
    "        \n",
    "        tmp_h = tf.stack( tmp_h )\n",
    "        tmp_h = tf.transpose( tmp_h, [1,0])\n",
    "        \n",
    "        \n",
    "#       dropout\n",
    "#         h_inter = tf.nn.dropout(h_inter, self.keep_prob)\n",
    "        \n",
    "        \n",
    "#       hidden layers on fusion\n",
    "        with tf.variable_scope(\"inter_h0\"):\n",
    "                                    \n",
    "            ortho_ini = tf.constant(self.orthogonal([n_hidden_list[0], self.N_TOTAL]), dtype = tf.float32)\n",
    "            ortho_ini = tf.transpose(ortho_ini, [1,0])                        \n",
    "            w = tf.Variable( ortho_ini )\n",
    "#             tf.random_normal([self.N_TOTAL, n_hidden_list[0]],\\\n",
    "#                         stddev=math.sqrt(2.0/float(self.N_TOTAL)))) \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "        \n",
    "#           max norm constraint\n",
    "#             w = tf.clip_by_norm( w, self.MAX_NORM, axes = 0) \n",
    "            \n",
    "            h_inter = tf.matmul(x_concate, w)\n",
    "            h_inter = h_inter + tmp_h\n",
    "#           h_inter = tf.add( h_inter, b )\n",
    "            \n",
    "            \n",
    "#           batch normalization \n",
    "            batch_m, batch_v = tf.nn.moments(h_inter,[0])\n",
    "            scale = tf.Variable( tf.ones( [ n_hidden_list[0] ]) )\n",
    "            beta  = tf.Variable( tf.zeros([ n_hidden_list[0] ]) )\n",
    "            \n",
    "            h_inter = \\\n",
    "            tf.nn.batch_normalization(h_inter, batch_m, batch_v, beta, scale,\\\n",
    "                                      self.epsilon)\n",
    "            \n",
    "#           nonlinear activation   \n",
    "#             h_inter = tf.nn.relu( h_inter )\n",
    "            h_inter = tf.maximum( 0.01*h_inter, h_inter )\n",
    "            \n",
    "#           L2   \n",
    "            self.regularizer = tf.nn.l2_loss(w)\n",
    "        \n",
    "#       dropout\n",
    "#         h_inter = tf.nn.dropout(h_inter, self.keep_prob)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"inter_h\"+str(i)):\n",
    "                                    \n",
    "                ortho_ini = tf.constant(self.orthogonal([n_hidden_list[i], n_hidden_list[i-1]]), dtype = tf.float32)\n",
    "                ortho_ini = tf.transpose(ortho_ini, [1,0])                        \n",
    "                w = tf.Variable( ortho_ini )                    \n",
    "#                 w = tf.Variable(\\\n",
    "#                         tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "#                                 stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b = tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                \n",
    "#               max norm constraint\n",
    "#                 w = tf.clip_by_norm( w, self.MAX_NORM,axes = 0)\n",
    "        \n",
    "                h_inter = tf.matmul(h_inter, w)\n",
    "#               h_inter = tf.add(h_inter,b)\n",
    "                \n",
    "                \n",
    "#               batch normalization \n",
    "                batch_m, batch_v = tf.nn.moments(h_inter,[0])\n",
    "                scale = tf.Variable( tf.ones( [ n_hidden_list[i] ]) )\n",
    "                beta  = tf.Variable( tf.zeros([ n_hidden_list[i] ]) )\n",
    "            \n",
    "                h_inter = \\\n",
    "                tf.nn.batch_normalization(h_inter, batch_m, batch_v, beta, scale,\\\n",
    "                                          self.epsilon)\n",
    "\n",
    "#               nonlinear activation  \n",
    "                h_inter = tf.maximum( 0.01*h_inter, h_inter )\n",
    "#                 h_inter = tf.nn.relu( h_inter )\n",
    "        \n",
    "#               L2\n",
    "                self.regularizer += tf.nn.l2_loss(w)\n",
    "    \n",
    "#               dropout\n",
    "#                 h_inter = tf.nn.dropout(h_inter, self.keep_prob)\n",
    "        \n",
    "        \n",
    "#       output layer  \n",
    "        h = h_inter\n",
    "        \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w = tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1],\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev = math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "            b = tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            \n",
    "            h = tf.matmul(h, w)\n",
    "            \n",
    "            h = tf.add( h, dx_wsum )\n",
    "            h = tf.add( h, cooc_wsum )\n",
    "            h = tf.add( h, b )\n",
    "        \n",
    "            self.logit = h\n",
    "            \n",
    "#           L2  \n",
    "            self.regularizer += tf.nn.l2_loss(w) \n",
    "    \n",
    "#       overall L2\n",
    "        self.regularizer += \\\n",
    "        (self.regularizer_cooc + self.regularizer_inter + self.regularizer_wide)\n",
    "\n",
    "        \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y)) \\\n",
    "                                   + self.L2 * self.regularizer\n",
    "            \n",
    "        self.optimizer = \\\n",
    "        tf.train.AdamOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                        feed_dict={self.dx:dx_batch,\\\n",
    "                                   self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                   self.lr:lr,\\\n",
    "                                   self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "        \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                  self.cx:cx_test, self.y:y_test,\\\n",
    "                                                  self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        return sess.run( [self.tmp1], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a85b403ab56b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m#   change\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference_ini\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_ini\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-3299113fcd01>\u001b[0m in \u001b[0;36mtrain_ini\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m                                                    \u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                                    \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;31m#         tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[1;31m#         tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m         grad_loss=grad_loss)\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mcompute_gradients\u001b[1;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgate_gradients\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGATE_OP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m         colocate_gradients_with_ops=colocate_gradients_with_ops)\n\u001b[0m\u001b[0;32m    355\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgate_gradients\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGATE_GRAPH\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m       \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.pyc\u001b[0m in \u001b[0;36mgradients\u001b[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\u001b[0m\n\u001b[0;32m    489\u001b[0m               if gate_gradients and len(\n\u001b[0;32m    490\u001b[0m                   [x for x in in_grads if x is not None]) > 1:\n\u001b[1;32m--> 491\u001b[1;33m                 \u001b[0min_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m           \u001b[0m_LogOpGradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc\u001b[0m in \u001b[0;36mtuple\u001b[1;34m(tensors, name, control_inputs)\u001b[0m\n\u001b[0;32m   2812\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgating_ops\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2813\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Must have at least one Tensor: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2814\u001b[1;33m     \u001b[0mgate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgating_ops\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2815\u001b[0m     \u001b[0mtpl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2816\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc\u001b[0m in \u001b[0;36mgroup\u001b[1;34m(*inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2751\u001b[0m       \u001b[1;31m# 1-level tree. The root node is the returned NoOp node.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2752\u001b[0m       \u001b[1;33m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops_on_device\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2753\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0m_GroupControlDeps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2755\u001b[0m     \u001b[1;31m# 2-level tree. The root node is the returned NoOp node.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc\u001b[0m in \u001b[0;36m_GroupControlDeps\u001b[1;34m(dev, deps, name)\u001b[0m\n\u001b[0;32m   2709\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2710\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2711\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mno_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_control_flow_ops.pyc\u001b[0m in \u001b[0;36mno_op\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0mOperation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m   \"\"\"\n\u001b[1;32m--> 189\u001b[1;33m   \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"NoOp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    377\u001b[0m       \u001b[1;31m# Perform input type inference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m       \u001b[0minferred_from\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0minput_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m         \u001b[0minput_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minput_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mgetter\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m     \u001b[0mfield_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# Construct a new object to represent this field.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "\n",
    "#   representation ability\n",
    "para_n_hidden_list = [ 512, 256, 128 ]\n",
    "#  1024, 512, 512\n",
    "#     128,16,8\n",
    "para_lr = 0.085\n",
    "\n",
    "# !!! change\n",
    "para_n_embedding = 3\n",
    "\n",
    "#   regularization\n",
    "para_batch_size = 32\n",
    "para_l2 = 0.3\n",
    "\n",
    "# !!!change\n",
    "para_keep_prob = 0.8\n",
    "para_max_norm = 7\n",
    "\n",
    "#  fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = [8]*para_n_disc\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    clf = InterNN_fuse( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                        para_n_disc_voca, sess, para_n_embedding, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2, para_batch_size, para_max_norm)\n",
    "    \n",
    "    \n",
    "#   change  \n",
    "    clf.inference_ini()\n",
    "    clf.train_ini()\n",
    "    \n",
    "    \n",
    "    para_cur_lr = para_lr\n",
    "    re\n",
    "    total_cnt   = np.shape(cxtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "    \n",
    "    total_idx = range(total_cnt)\n",
    "    \n",
    "#   training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc = 0.0\n",
    "        \n",
    "#       shuffle traning instances each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "    \n",
    "#       Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                     para_cur_lr )\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "    \n",
    "        tmp_test_acc  = clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
