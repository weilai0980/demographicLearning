{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# batch normalization   https://r2rt.com/implementing-batch-normalization-in-tensorflow.html\n",
    "# batch norm specific for training and testing\n",
    "\n",
    "# https://gist.github.com/tomokishii/0ce3bdac1588b5cca9fa5fbdf6e1c412\n",
    "# weight normalization\n",
    "\n",
    "# optimize the efficiency of codes \n",
    "\n",
    "# start with low regularization, large loss\n",
    "#  more parameters, high learning rate\n",
    "\n",
    "# reguliarization slows down the convergence rate\n",
    "\n",
    "\n",
    "# TO DO:\n",
    "# observe regulairzation and objective losses individually\n",
    "# orthogonal ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTES:\n",
    "\n",
    "# ---Ini\n",
    "\n",
    "# Xavier 1/n_in\n",
    "# He's 2/n_in\n",
    "# Orthogonal\n",
    "\n",
    "# ---Activation\n",
    "\n",
    "\n",
    "# ---Optimizer\n",
    "\n",
    "# Adam\n",
    "# Adadelta\n",
    "\n",
    "# ---Regularization\n",
    "# dropout + max norm\n",
    "# batch normalization\n",
    "# weight normalization\n",
    "\n",
    "# regularization:  closeness of traning and validation performance, training stableness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from six.moves import urllib\n",
    "# from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from interNN import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# continuous and categorical feature split\n",
    "\n",
    "# features on columns [0:split_idx) are continuous\n",
    "# the rest are categorical\n",
    "def conti_cate_split(dta_df, split_idx):\n",
    "    \n",
    "    col_cnt= dta_df.shape[1]\n",
    "    conti_cols= range(split_idx)\n",
    "    dis_cols=range(split_idx, col_cnt)\n",
    "    \n",
    "    return dta_df[conti_cols], dta_df[dis_cols]\n",
    "\n",
    "def conti_normalization_train_dta(dta_df):\n",
    "    \n",
    "    return preprocessing.scale(dta_df)\n",
    "\n",
    "def conti_normalization_test_dta(dta_df, train_df):\n",
    "    \n",
    "    mean_dim = np.mean(train_df, axis=0)\n",
    "    std_dim = np.std(train_df, axis=0)\n",
    "        \n",
    "    df=pd.DataFrame()\n",
    "    cols = train_df.columns\n",
    "    idx=0\n",
    "    \n",
    "    for i in cols:\n",
    "        df[i] = (dta_df[i]- mean_dim[idx])*1.0/std_dim[idx]\n",
    "        idx=idx+1\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6938, 106) (6938, 80) (6938, 8)\n",
      "(771, 106) (771, 80) (771, 8)\n"
     ]
    }
   ],
   "source": [
    "# Demographic data\n",
    "# ------------data prepro-----------------------\n",
    "\n",
    "files_list=[\"../dataset/dataset_demo/xtrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/xtest.csv\",\\\n",
    "            \"../dataset/dataset_demo/ytrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/ytest.csv\"]\n",
    "\n",
    "xtrain_df=pd.read_csv( files_list[0] ,sep=',', header=None)\n",
    "xtest_df=pd.read_csv( files_list[1] ,sep=',', header=None)\n",
    "ytrain_df=pd.read_csv( files_list[2] ,sep=',', header=None)\n",
    "ytest_df=pd.read_csv( files_list[3] ,sep=',', header=None)\n",
    "\n",
    "\n",
    "# get training and testing data prepared\n",
    "\n",
    "cxtrain, dxtrain = conti_cate_split(xtrain_df, 106)\n",
    "cxtest, dxtest = conti_cate_split(xtest_df, 106)\n",
    "\n",
    "cxtest = conti_normalization_test_dta(cxtest, cxtrain)\n",
    "cxtrain = conti_normalization_train_dta(cxtrain)\n",
    "\n",
    "# cxtrain = cxtrain.as_matrix() \n",
    "dxtrain = dxtrain.as_matrix().astype(int)\n",
    "cxtest = cxtest.as_matrix()\n",
    "dxtest = dxtest.as_matrix().astype(int)\n",
    "\n",
    "ytest = np_utils.to_categorical( ytest_df.as_matrix() )\n",
    "ytrain =np_utils.to_categorical( ytrain_df.as_matrix() )\n",
    "\n",
    "\n",
    "print np.shape(cxtrain), np.shape(dxtrain), np.shape(ytrain)\n",
    "print np.shape(cxtest), np.shape(dxtest), np.shape(ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6938, 186) (771, 186) (6938, 1) (771, 1)\n",
      "(6938, 20) (6938, 166)\n",
      "(771, 20) (771, 166)\n"
     ]
    }
   ],
   "source": [
    "# Default data\n",
    "# ------------data prepro-----------------------\n",
    "\n",
    "files_list=[\"../dataset/dataset_demo/default_xtrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/default_xtest.csv\",\\\n",
    "            \"../dataset/dataset_demo/default_ytrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/default_ytest.csv\"]\n",
    "\n",
    "xtrain_df=pd.read_csv( files_list[0] ,sep=',', header=None)\n",
    "xtest_df=pd.read_csv( files_list[1] ,sep=',', header=None)\n",
    "ytrain_df=pd.read_csv( files_list[2] ,sep=',', header=None)\n",
    "ytest_df=pd.read_csv( files_list[3] ,sep=',', header=None)\n",
    "\n",
    "# get training and testing data prepared\n",
    "\n",
    "cxtrain, dxtrain = conti_cate_split(xtrain_df, 20)\n",
    "cxtest, dxtest = conti_cate_split(xtest_df, 20)\n",
    "\n",
    "cxtest = conti_normalization_test_dta(cxtest, cxtrain)\n",
    "cxtrain = conti_normalization_train_dta(cxtrain)\n",
    "\n",
    "# cxtrain = cxtrain.as_matrix() \n",
    "dxtrain = dxtrain.as_matrix().astype(int)\n",
    "cxtest = cxtest.as_matrix()\n",
    "dxtest = dxtest.as_matrix().astype(int)\n",
    "\n",
    "ytest = np_utils.to_categorical( ytest_df.as_matrix() )\n",
    "ytrain =np_utils.to_categorical( ytrain_df.as_matrix() )\n",
    "\n",
    "print np.shape(cxtrain), np.shape(dxtrain), np.shape(ytrain)\n",
    "print np.shape(cxtest), np.shape(dxtest), np.shape(ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wide, embedding, co-occurrence and feature interaction \n",
    "# version 1: interaction as external hiddens\n",
    "\n",
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "\n",
    "#   representation ability\n",
    "para_n_hidden_list = [ 32,32,16 ]\n",
    "#     128,16,8 ]\n",
    "para_lr = 0.045\n",
    "para_n_embedding = 2\n",
    "#   regularization\n",
    "para_batch_size = 32\n",
    "para_keep_prob = 1.0\n",
    "para_l2 = 0.01\n",
    "\n",
    "# fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = [8]*para_n_disc\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 10\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    clf = wide_embed_coocc_NN( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                      para_n_disc_voca, sess, para_n_embedding, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2)\n",
    "    clf.train_ini()\n",
    "    clf.inference_ini()\n",
    "    \n",
    "    para_cur_lr=para_lr\n",
    "    \n",
    "    total_cnt= np.shape(cxtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "    \n",
    "    total_idx=range(total_cnt)\n",
    "    \n",
    "    \n",
    "#   training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc=0.0\n",
    "        \n",
    "#       shuffle traning instances each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "        \n",
    "#       Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                     para_cur_lr )\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "        \n",
    "        tmp_test_acc  = clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "        \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wide, embedding, co-occurrence and feature interaction \n",
    "# version 2: individual hidden layers on embedding and interaction\n",
    "\n",
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "\n",
    "#   representation ability\n",
    "para_n_hidden_list = [ 128,64,32,16 ]\n",
    "#     128,16,8 ]\n",
    "para_lr = 0.005\n",
    "para_n_embedding = 2\n",
    "#   regularization\n",
    "para_batch_size = 32\n",
    "para_keep_prob = 1.0\n",
    "para_l2 = 0.01\n",
    "\n",
    "# fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = [8]*para_n_disc\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 10\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    clf = InterNN_IndiH( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                      para_n_disc_voca, sess, para_n_embedding, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2, para_batch_size)\n",
    "    clf.train_ini()\n",
    "    clf.inference_ini()\n",
    "    \n",
    "    para_cur_lr=para_lr\n",
    "    \n",
    "    total_cnt= np.shape(cxtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "    \n",
    "    total_idx=range(total_cnt)\n",
    "    \n",
    "    \n",
    "#   training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc=0.0\n",
    "        \n",
    "#       shuffle traning instances each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "        \n",
    "#       Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                     para_cur_lr )\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "        \n",
    "        tmp_test_acc  = clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "        \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch  0  :  2428.14847367 testing: [0.21141374, 14.136365, 2355.8481] training: [0.18160853, 13.672114, 2355.8481]\n",
      "loss on epoch  5  :  1941.31167716 testing: [0.19714656, 2.8874924, 1895.3241] training: [0.18607669, 2.9022205, 1895.3241]\n",
      "loss on epoch  10  :  1569.14640752 testing: [0.20881972, 2.0964427, 1533.0062] training: [0.21403863, 2.1158705, 1533.0062]\n",
      "loss on epoch  15  :  1272.26470721 testing: [0.22568093, 1.9657334, 1242.9178] training: [0.22211012, 1.9692196, 1242.9178]\n",
      "loss on epoch  20  :  1032.6619873 testing: [0.22568093, 1.9235015, 1008.5903] training: [0.23320842, 1.898248, 1008.5903]\n",
      "loss on epoch  25  :  838.299802427 testing: [0.23216602, 1.8551874, 818.4472] training: [0.24113578, 1.8442209, 818.4472]\n",
      "loss on epoch  30  :  680.224688495 testing: [0.23605707, 1.818863, 663.77185] training: [0.24373019, 1.7965426, 663.77185]\n",
      "loss on epoch  35  :  551.500232838 testing: [0.21271077, 1.7859875, 537.77484] training: [0.24185644, 1.7821738, 537.77484]\n",
      "loss on epoch  40  :  446.581500018 testing: [0.230869, 1.7819723, 435.07797] training: [0.24545978, 1.7678416, 435.07797]\n",
      "loss on epoch  45  :  361.069335938 testing: [0.25032425, 1.7709696, 351.37653] training: [0.24545978, 1.7576593, 351.37653]\n",
      "loss on epoch  50  :  291.407020851 testing: [0.22178988, 1.7610333, 283.1954] training: [0.24646872, 1.7537371, 283.1954]\n",
      "loss on epoch  55  :  234.71510541 testing: [0.2542153, 1.7480588, 227.71452] training: [0.25122514, 1.7470474, 227.71452]\n",
      "loss on epoch  60  :  188.64173211 testing: [0.24902724, 1.7490066, 182.6312] training: [0.24488325, 1.7440383, 182.6312]\n",
      "loss on epoch  65  :  151.26073145 testing: [0.24773023, 1.7383872, 146.06204] training: [0.25208995, 1.7400423, 146.06204]\n",
      "loss on epoch  70  :  120.994975902 testing: [0.24513619, 1.7424432, 116.46162] training: [0.25136927, 1.7382753, 116.46162]\n",
      "loss on epoch  75  :  96.5480389065 testing: [0.24383917, 1.7340827, 92.558693] training: [0.24603632, 1.7344161, 92.558693]\n",
      "loss on epoch  80  :  76.8537035342 testing: [0.25940338, 1.7282141, 73.308487] training: [0.25021619, 1.7311642, 73.308487]\n",
      "loss on epoch  85  :  61.0340526722 testing: [0.27626458, 1.7205462, 57.850338] training: [0.26390889, 1.7277441, 57.850338]\n",
      "loss on epoch  90  :  48.3607675058 testing: [0.28015563, 1.7184718, 45.477467] training: [0.26390889, 1.7216027, 45.477467]\n",
      "loss on epoch  95  :  38.2515904462 testing: [0.25810635, 1.7146825, 35.607204] training: [0.26304412, 1.7168314, 35.607204]\n",
      "loss on epoch  100  :  30.211667732 testing: [0.25162128, 1.7116929, 27.763248] training: [0.26448545, 1.7129804, 27.763248]\n",
      "loss on epoch  105  :  23.8450703798 testing: [0.25680932, 1.7090489, 21.553854] training: [0.271548, 1.7108897, 21.553854]\n",
      "loss on epoch  110  :  18.8253275553 testing: [0.25551233, 1.7094016, 16.659008] training: [0.27457482, 1.7082695, 16.659008]\n",
      "loss on epoch  115  :  14.8813034693 testing: [0.25810635, 1.7028056, 12.817459] training: [0.27356586, 1.7044535, 12.817459]\n",
      "loss on epoch  120  :  11.7851350926 testing: [0.28923476, 1.6801924, 9.827837] training: [0.28869992, 1.6743151, 9.827837]\n",
      "loss on epoch  125  :  9.37421470218 testing: [0.28664073, 1.6738498, 7.5010948] training: [0.30138367, 1.6502091, 7.5010948]\n",
      "loss on epoch  130  :  7.50923388093 testing: [0.29053178, 1.66854, 5.701664] training: [0.3083021, 1.6284374, 5.701664]\n",
      "loss on epoch  135  :  6.07006707015 testing: [0.29961088, 1.672523, 4.3199172] training: [0.30916691, 1.6070974, 4.3199172]\n",
      "loss on epoch  140  :  4.96660140709 testing: [0.29961088, 1.664878, 3.2643998] training: [0.32343614, 1.5919744, 3.2643998]\n",
      "loss on epoch  145  :  4.12041035405 testing: [0.29831389, 1.6867131, 2.4713142] training: [0.3428942, 1.5537198, 2.4713142]\n",
      "loss on epoch  150  :  3.47518885577 testing: [0.30090791, 1.6969864, 1.8693644] training: [0.35961372, 1.5299239, 1.8693644]\n",
      "loss on epoch  155  :  2.98543024946 testing: [0.29312581, 1.7338439, 1.4347793] training: [0.37445951, 1.487785, 1.4347793]\n",
      "loss on epoch  160  :  2.60919436702 testing: [0.29312581, 1.7479972, 1.1109222] training: [0.41222253, 1.4392382, 1.1109222]\n",
      "loss on epoch  165  :  2.33259412094 testing: [0.29182878, 1.8018916, 0.87282515] training: [0.41942924, 1.4044325, 0.87282515]\n",
      "loss on epoch  170  :  2.11256469621 testing: [0.28015563, 1.799505, 0.69462186] training: [0.46137217, 1.3624548, 0.69462186]\n",
      "loss on epoch  175  :  1.96808961144 testing: [0.29701686, 1.8383006, 0.57220238] training: [0.46555203, 1.3484067, 0.57220238]\n",
      "loss on epoch  180  :  1.83844829489 testing: [0.27496758, 1.8529735, 0.47256109] training: [0.47679445, 1.3261907, 0.47256109]\n",
      "loss on epoch  185  :  1.74656739941 testing: [0.28534371, 1.8584837, 0.40306434] training: [0.49207264, 1.3060449, 0.40306434]\n",
      "loss on epoch  190  :  1.67165565049 testing: [0.29182878, 1.8678303, 0.3575342] training: [0.50778323, 1.2790775, 0.3575342]\n",
      "loss on epoch  195  :  1.63764357125 testing: [0.27107653, 1.8938322, 0.31792426] training: [0.50778323, 1.2726207, 0.31792426]\n",
      "loss on epoch  200  :  1.58987117255 testing: [0.27367055, 1.890021, 0.292757] training: [0.5289709, 1.261025, 0.292757]\n",
      "loss on epoch  205  :  1.54531008226 testing: [0.28923476, 1.933877, 0.27322274] training: [0.54035747, 1.2359978, 0.27322274]\n",
      "loss on epoch  210  :  1.54179101962 testing: [0.28534371, 1.9211967, 0.2658616] training: [0.53012395, 1.2486409, 0.2658616]\n",
      "loss on epoch  215  :  1.54008312579 testing: [0.29182878, 1.905524, 0.25672802] training: [0.52234071, 1.2582043, 0.25672802]\n",
      "loss on epoch  220  :  1.52643612579 testing: [0.28793773, 1.9217751, 0.25008827] training: [0.52911502, 1.2394017, 0.25008827]\n",
      "loss on epoch  225  :  1.51482678784 testing: [0.29442284, 1.9136915, 0.24431741] training: [0.52579993, 1.2534584, 0.24431741]\n",
      "loss on epoch  230  :  1.50499312083 testing: [0.28793773, 1.9548652, 0.24671446] training: [0.51859325, 1.2316746, 0.24671446]\n",
      "loss on epoch  235  :  1.50418904093 testing: [0.28404668, 1.9201864, 0.24216452] training: [0.53372729, 1.2408909, 0.24216452]\n",
      "loss on epoch  240  :  1.48467155298 testing: [0.2775616, 1.9694312, 0.24438448] training: [0.5422312, 1.2162184, 0.24438448]\n",
      "loss on epoch  245  :  1.54009744856 testing: [0.29312581, 1.9392853, 0.23914094] training: [0.5050447, 1.26169, 0.23914094]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a4aa302cf08f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mbatch_y\u001b[0m  \u001b[1;33m=\u001b[0m  \u001b[0mytrain\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0mtmpc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mbatch_dx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_cx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpara_keep_prob\u001b[0m\u001b[1;33m,\u001b[0m                                     \u001b[0mpara_cur_lr\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mpara_eval_byepoch\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guo/nn_work/demo_learning/interNN.pyc\u001b[0m in \u001b[0;36mtrain_batch\u001b[1;34m(self, dx_batch, cx_batch, y_batch, keep_prob, lr)\u001b[0m\n\u001b[0;32m   1292\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1293\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1294\u001b[1;33m                                    \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1295\u001b[0m                                  })\n\u001b[0;32m   1296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# v3 fusion\n",
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "\n",
    "# learning speed\n",
    "para_lr = 0.0005\n",
    "para_batch_size = 256\n",
    "\n",
    "#   representation ability\n",
    "para_n_hidden_list = [ 128, 64 ]\n",
    "#     128,16,8\n",
    "# !!! change\n",
    "para_n_embedding = 3\n",
    "\n",
    "\n",
    "#   regularization\n",
    "para_l2 = 0.3\n",
    "# !!!change\n",
    "para_keep_prob = 0.7\n",
    "para_max_norm = 7\n",
    "\n",
    "\n",
    "#  fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = [8]*para_n_disc\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    clf = InterNN_fuse( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                        para_n_disc_voca, sess, para_n_embedding, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2, para_batch_size, para_max_norm)\n",
    "    \n",
    "    \n",
    "#   change  \n",
    "    clf.inference_ini()\n",
    "    clf.train_ini()\n",
    "    \n",
    "    \n",
    "    para_cur_lr = para_lr\n",
    "\n",
    "    total_cnt   = np.shape(cxtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "    \n",
    "    total_idx = range(total_cnt)\n",
    "    \n",
    "#   training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc = 0.0\n",
    "        \n",
    "#       shuffle traning instances each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "    \n",
    "#       Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                     para_cur_lr )\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "    \n",
    "        tmp_test_acc  = clf.inference(dxtest,  cxtest,  ytest,  para_keep_prob) \n",
    "        tmp_train_acc = clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, \"testing:\", tmp_test_acc,\\\n",
    "        \"training:\", tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 5 [5]\n",
      "1.0\n",
      "2.08166817117e-17\n",
      "0.813187183618\n",
      "0.194308854259\n"
     ]
    }
   ],
   "source": [
    "def orthogonal(shape):\n",
    "\n",
    "        flat_shape = (shape[0], np.prod(shape[1:]))\n",
    "        \n",
    "        print shape[0],np.prod(shape[1:]), shape[1:]\n",
    "        \n",
    "        a = np.random.normal(0.0, 1.0, flat_shape)\n",
    "        \n",
    "        u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "        q = u if u.shape == flat_shape else v\n",
    "        \n",
    "        return q.reshape(shape)\n",
    "    \n",
    "tmp = orthogonal([3,5])\n",
    "\n",
    "print sum( [tmp[0][i]*tmp[0][i] for i in range(5)] )\n",
    "\n",
    "print sum( [tmp[0][i]*tmp[2][i] for i in range(5)] )\n",
    "\n",
    "print sum( [tmp[i][0]*tmp[i][0] for i in range(3)] )\n",
    "\n",
    "print sum( [tmp[i][0]*tmp[i][2] for i in range(3)] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fusion, orthogonal ini\n",
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "\n",
    "\n",
    "# learning speed\n",
    "para_lr = 0.005\n",
    "para_batch_size = 32\n",
    "\n",
    "#   representation ability\n",
    "para_n_hidden_list = [ 128, 64 ]\n",
    "#  1024, 512, 512\n",
    "#     128,16,8\n",
    "\n",
    "# !!! change\n",
    "para_n_embedding = 3\n",
    "\n",
    "#   regularization\n",
    "para_l2 = 0.01\n",
    "\n",
    "# !!!change\n",
    "para_keep_prob = 0.8\n",
    "para_max_norm = 7\n",
    "\n",
    "\n",
    "#  fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = [8]*para_n_disc\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    clf = InterNN_fuse( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                        para_n_disc_voca, sess, para_n_embedding, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2, para_batch_size, para_max_norm)\n",
    "    \n",
    "    \n",
    "#   change  \n",
    "    clf.inference_ini()\n",
    "    clf.train_ini()\n",
    "    \n",
    "    \n",
    "    para_cur_lr = para_lr\n",
    "    re\n",
    "    total_cnt   = np.shape(cxtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "    \n",
    "    total_idx = range(total_cnt)\n",
    "    \n",
    "#   training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc = 0.0\n",
    "        \n",
    "#       shuffle traning instances each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "    \n",
    "#       Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                     para_cur_lr )\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "    \n",
    "        tmp_test_acc  = clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
