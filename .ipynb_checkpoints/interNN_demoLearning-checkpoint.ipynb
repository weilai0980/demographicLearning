{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wide and embedding, \n",
    "# wide co-occurrence\n",
    "# wide co-occurrence & interaction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from six.moves import urllib\n",
    "# from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6938, 186) (771, 186) (6938, 1) (771, 1)\n",
      "(6938, 106) (6938, 80)\n",
      "(771, 106) (771, 80)\n"
     ]
    }
   ],
   "source": [
    "# continuous and categorical feature split\n",
    "\n",
    "# features on columns [0:split_idx] are continuous\n",
    "# the rest are categorical\n",
    "def conti_cate_split(dta_df, split_idx):\n",
    "    \n",
    "    col_cnt= dta_df.shape[1]\n",
    "    conti_cols= range(split_idx)\n",
    "    dis_cols=range(split_idx, col_cnt)\n",
    "    \n",
    "    return dta_df[conti_cols], dta_df[dis_cols]\n",
    "\n",
    "def conti_normalization_train_dta(dta_df):\n",
    "    \n",
    "    return preprocessing.scale(dta_df)\n",
    "\n",
    "def conti_normalization_test_dta(dta_df, train_df):\n",
    "    \n",
    "    mean_dim = np.mean(train_df, axis=0)\n",
    "    std_dim = np.std(train_df, axis=0)\n",
    "        \n",
    "    df=pd.DataFrame()\n",
    "    cols = train_df.columns\n",
    "    idx=0\n",
    "    \n",
    "    for i in cols:\n",
    "        df[i] = (dta_df[i]- mean_dim[idx])*1.0/std_dim[idx]\n",
    "        idx=idx+1\n",
    "        \n",
    "    return df\n",
    "\n",
    "files_list=[\"../dataset/dataset_demo/xtrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/xtest.csv\",\\\n",
    "            \"../dataset/dataset_demo/ytrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/ytest.csv\"]\n",
    "\n",
    "xtrain_df=pd.read_csv( files_list[0] ,sep=',', header=None)\n",
    "xtest_df=pd.read_csv( files_list[1] ,sep=',', header=None)\n",
    "ytrain_df=pd.read_csv( files_list[2] ,sep=',', header=None)\n",
    "ytest_df=pd.read_csv( files_list[3] ,sep=',', header=None)\n",
    "\n",
    "print xtrain_df.shape, xtest_df.shape, ytrain_df.shape, ytest_df.shape\n",
    "\n",
    "\n",
    "# get training and testing data prepared\n",
    "\n",
    "cxtrain, dxtrain = conti_cate_split(xtrain_df, 106)\n",
    "cxtest, dxtest = conti_cate_split(xtest_df, 106)\n",
    "\n",
    "cxtest = conti_normalization_test_dta(cxtest, cxtrain)\n",
    "cxtrain = conti_normalization_train_dta(cxtrain)\n",
    "\n",
    "print cxtrain.shape, dxtrain.shape\n",
    "print cxtest.shape, dxtest.shape\n",
    "\n",
    "# cxtrain = cxtrain.as_matrix() \n",
    "dxtrain = dxtrain.as_matrix().astype(int)\n",
    "cxtest = cxtest.as_matrix()\n",
    "dxtest = dxtest.as_matrix().astype(int)\n",
    "\n",
    "ytest = np_utils.to_categorical( ytest_df.as_matrix() )\n",
    "ytrain =np_utils.to_categorical( ytrain_df.as_matrix() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wide, embedding and co-occurrence of features\n",
    "\n",
    "class wide_embed_coocc_NN():\n",
    "    \n",
    "    def __init__(self, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, session,n_embedding, n_hidden_list, lr, l2):\n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.N_EMBED = n_embedding\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list)\n",
    "        self.N_TOTAL = self.N_CONTI+ self.N_DISC*self.N_EMBED\n",
    "\n",
    "        self.sess=session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "#       wide part on categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "        \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"wide\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                tf.random_uniform([self.N_DISC_VOCA[i], self.N_CLASS],-1.0, 1.0) )\n",
    "#                         tf.random_normal([self.N_DISC_VOCA, self.N_CLASS],\\\n",
    "#                         stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "                if i==0:\n",
    "                    dx_wsum = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                    \n",
    "#                   L2\n",
    "                    self.regularizer_wide = tf.nn.l2_loss(w)\n",
    "                    \n",
    "                else:\n",
    "                    dx_wsum = dx_wsum + tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                    \n",
    "#                   L2  \n",
    "                    self.regularizer_wide = self.regularizer_wide + tf.nn.l2_loss(w)\n",
    "                    \n",
    "            \n",
    "#       embedding of categorical features        \n",
    "        tmp_embeded = []                \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"embedding\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA[i], self.N_EMBED],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "                              \n",
    "                tmp_embeded.append( tf.nn.embedding_lookup( w, self.dx_trans[i] ) )\n",
    "                \n",
    "        dx_embeded = tf.concat(tmp_embeded, 1)\n",
    "        x_concate =  tf.concat( [self.cx,dx_embeded], 1 )\n",
    "        \n",
    "#       co-occurrence of categorical features\n",
    "        for i in range(10):\n",
    "            \n",
    "            tmpvoca1 = self.N_DISC_VOCA[i] \n",
    "            \n",
    "            for j in range(i+1,10):\n",
    "                \n",
    "                tmpvoca2 = self.N_DISC_VOCA[j]\n",
    "                \n",
    "                with tf.variable_scope(\"cooccur\"+str(i)+str(j)):\n",
    "                    \n",
    "                    \n",
    "#                   vector approximate of co-occurrence matrix \n",
    "                    w1 = tf.Variable(tf.random_normal([self.N_CLASS, tmpvoca1, 1],\\\n",
    "                                            stddev=math.sqrt(2.0/float( tmpvoca1 ))) )\n",
    "                    \n",
    "                    w2 = tf.Variable(tf.random_normal([self.N_CLASS, 1, tmpvoca2],\\\n",
    "                                            stddev=math.sqrt(2.0/float( tmpvoca2 ))) )\n",
    "                    \n",
    "                    cooc_mat = []\n",
    "                    for k in range(self.N_CLASS):\n",
    "                        cooc_mat.append( tf.reshape( tf.matmul(w1[k],w2[k]), [-1,1] ) )\n",
    "                        \n",
    "#                   n_class by voca1*voca2  \n",
    "                    cooc_vec = tf.stack(cooc_mat)\n",
    "                    cooc_vec = tf.reshape(cooc_vec, [self.N_CLASS, -1] )\n",
    "                    cooc_vec = tf.transpose(cooc_vec,[1,0])\n",
    "                    \n",
    "#                   build the idx of co-occurrence features\n",
    "                    cooc_idx = self.dx_trans[i] * tmpvoca2 + self.dx_trans[j]\n",
    "    \n",
    "                    if i==0 and j==1:\n",
    "                        cooc_wsum  = tf.nn.embedding_lookup( cooc_vec, cooc_idx )\n",
    "                        self.regularizer_cooc   = tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2)\n",
    "                    else:\n",
    "                        cooc_wsum += tf.nn.embedding_lookup( cooc_vec, cooc_idx )\n",
    "                        self.regularizer_cooc += (tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2))\n",
    "        \n",
    "                    \n",
    "# #     hidden layers\n",
    "        with tf.variable_scope(\"h0\"):\n",
    "            w= tf.Variable(tf.random_normal([self.N_TOTAL, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_CONTI)))) \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            h = tf.nn.relu( tf.add( tf.matmul(x_concate, w),b) )\n",
    "        \n",
    "                \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"layer\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b= tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                h = tf.nn.relu( tf.add( tf.matmul(h, w),b) )\n",
    "                \n",
    "                \n",
    "#       Regularization\n",
    "\n",
    "#       dropout\n",
    "#       h = tf.nn.dropout(h, self.keep_prob)\n",
    "\n",
    "#       L2\n",
    "        self.regularizer = tf.nn.l2_loss(w)\n",
    "        \n",
    "#       output layer  \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w= tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1],\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            \n",
    "            h = tf.add( tf.matmul(h, w), dx_wsum ) #wide\n",
    "            h = tf.add( h, cooc_wsum ) #co-occurrence\n",
    "            h = tf.add( h, b )\n",
    "\n",
    "\n",
    "            self.logit = h\n",
    "     \n",
    "    \n",
    "#           L2  \n",
    "            self.regularizer = self.regularizer + tf.nn.l2_loss(w) \n",
    "    \n",
    "#       overall L2\n",
    "        self.regularizer=self.regularizer + self.regularizer_wide + self.regularizer_cooc\n",
    "         \n",
    "    \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y) \\\n",
    "                                  + self.L2*self.regularizer )\n",
    "        self.optimizer = \\\n",
    "        tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#       tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                       feed_dict={self.dx:dx_batch, \\\n",
    "                                  self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                  self.lr:lr,\\\n",
    "                                  self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "    \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                  self.cx:cx_test, self.y:y_test,\\\n",
    "                                                  self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        return sess.run( [self.tmpshape], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch  0  :  248.414669037 [0.072632946] [0.066301525]\n",
      "loss on epoch  10  :  182.041134516 [0.20881972] [0.20668781]\n",
      "loss on epoch  20  :  119.78852484 [0.21530479] [0.22629]\n",
      "loss on epoch  30  :  72.4136905317 [0.21011673] [0.2369559]\n",
      "loss on epoch  40  :  41.4497991491 [0.21919584] [0.24747767]\n",
      "loss on epoch  50  :  23.5136919728 [0.22049287] [0.2587201]\n",
      "loss on epoch  60  :  14.113192885 [0.21660182] [0.27371001]\n",
      "loss on epoch  70  :  9.43611409929 [0.22178988] [0.28106081]\n",
      "loss on epoch  80  :  6.98401696152 [0.22438392] [0.29403287]\n",
      "loss on epoch  90  :  5.54885720765 [0.22049287] [0.30570769]\n",
      "loss on epoch  100  :  4.6190436637 [0.230869] [0.31781495]\n",
      "loss on epoch  110  :  3.96468808033 [0.24513619] [0.32804844]\n",
      "loss on epoch  120  :  3.48226148884 [0.24902724] [0.33914673]\n",
      "loss on epoch  130  :  3.11341652385 [0.25162128] [0.34736234]\n",
      "loss on epoch  140  :  2.82550982303 [0.27237353] [0.35500145]\n",
      "loss on epoch  150  :  2.59551349062 [0.27496758] [0.3672528]\n",
      "loss on epoch  160  :  2.41185091933 [0.27367055] [0.37734216]\n",
      "loss on epoch  170  :  2.26293617045 [0.28274968] [0.38022485]\n",
      "loss on epoch  180  :  2.14334785552 [0.28793773] [0.38944942]\n",
      "loss on epoch  190  :  2.04488761006 [0.30090791] [0.39636782]\n",
      "loss on epoch  200  :  1.96420470542 [0.29312581] [0.40443933]\n",
      "loss on epoch "
     ]
    }
   ],
   "source": [
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "#   representation ability\n",
    "para_n_hidden_list = [ 128,64,8 ]\n",
    "#     128,16,8 ]\n",
    "para_lr = 0.055\n",
    "para_n_embedding = 2\n",
    "#   regularization\n",
    "para_batch_size = 32\n",
    "para_keep_prob = 1.0\n",
    "para_l2 = 0.15\n",
    "\n",
    "# fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = [8]*para_n_disc\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 10\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    wide_mlp_clf = wide_embed_coocc_NN( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                      para_n_disc_voca, sess, para_n_embedding, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2 )\n",
    "    wide_mlp_clf.train_ini()\n",
    "    wide_mlp_clf.inference_ini()\n",
    "    \n",
    "    para_cur_lr=para_lr\n",
    "    \n",
    "    total_cnt= np.shape(cxtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "\n",
    "    total_idx=range(total_cnt)\n",
    "\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc=0.0\n",
    "        \n",
    "#       shuffle traning dataset each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "   \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += wide_mlp_clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                       para_cur_lr)\n",
    "        \n",
    "#       learning rate decaying\n",
    "#         para_cur_lr = para_cur_lr*0.98\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "    \n",
    "        tmp_test_acc = wide_mlp_clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = wide_mlp_clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wide, embedding, co-occurrence and feature interaction \n",
    "\n",
    "class wide_embed_coInter_NN():\n",
    "    \n",
    "    def __init__(self, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, session,n_embedding, n_hidden_list, lr, l2):\n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.N_EMBED = n_embedding\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list)\n",
    "        self.N_TOTAL = self.N_CONTI+ self.N_DISC*self.N_EMBED\n",
    "        self.N_TOTAL_EMBED = self.N_DISC*self.N_EMBED\n",
    "\n",
    "        self.sess=session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "#       wide part on categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "        \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"wide\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                tf.random_uniform([self.N_DISC_VOCA[i], self.N_CLASS],-1.0, 1.0) )\n",
    "#                         tf.random_normal([self.N_DISC_VOCA, self.N_CLASS],\\\n",
    "#                         stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "                if i==0:\n",
    "                    dx_wsum = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                    \n",
    "#                   L2\n",
    "                    self.regularizer_wide = tf.nn.l2_loss(w)\n",
    "                    \n",
    "                else:\n",
    "                    dx_wsum = dx_wsum + tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                    \n",
    "#                   L2  \n",
    "                    self.regularizer_wide = self.regularizer_wide + tf.nn.l2_loss(w)\n",
    "                    \n",
    "            \n",
    "#       embedding of categorical features        \n",
    "        tmp_embeded = []                \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"embedding\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA[i], self.N_EMBED],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "                              \n",
    "                tmp_embeded.append( tf.nn.embedding_lookup( w, self.dx_trans[i] ) )\n",
    "                \n",
    "        dx_embeded = tf.concat(tmp_embeded, 1)\n",
    "        x_concate =  tf.concat( [self.cx,dx_embeded], 1 )\n",
    "        \n",
    "#       co-occurrence of categorical features\n",
    "        for i in range(20):\n",
    "            \n",
    "            tmpvoca1 = self.N_DISC_VOCA[i] \n",
    "            \n",
    "            for j in range(i+1,20):\n",
    "                \n",
    "                tmpvoca2 = self.N_DISC_VOCA[j]\n",
    "                \n",
    "                with tf.variable_scope(\"cooccur\"+str(i)+str(j)):\n",
    "                    \n",
    "                    \n",
    "#                   vector approximate of co-occurrence matrix \n",
    "                    w1 = tf.Variable(tf.random_normal([self.N_CLASS, tmpvoca1, 1],\\\n",
    "                                            stddev=math.sqrt(2.0/float( tmpvoca1 ))) )\n",
    "                    \n",
    "                    w2 = tf.Variable(tf.random_normal([self.N_CLASS, 1, tmpvoca2],\\\n",
    "                                            stddev=math.sqrt(2.0/float( tmpvoca2 ))) )\n",
    "                    \n",
    "                    cooc_mat = []\n",
    "                    for k in range(self.N_CLASS):\n",
    "                        cooc_mat.append( tf.reshape( tf.matmul(w1[k],w2[k]), [-1,1] ) )\n",
    "                        \n",
    "#                   n_class by voca1*voca2  \n",
    "                    cooc_vec = tf.stack(cooc_mat)\n",
    "                    cooc_vec = tf.transpose(cooc_mat,[1,0])\n",
    "                    \n",
    "#                   build the idx of co-occurrence features\n",
    "                    cooc_idx = self.dx_trans[i] * tmpvoca2 + self.dx_trans[j]\n",
    "    \n",
    "                    if i==0 and j==1:\n",
    "                        cooc_wsum  = tf.nn.embedding_lookup( cooc_vec, cooc_idx )\n",
    "                        self.regularizer_cooc   = tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2)\n",
    "                    else:\n",
    "                        cooc_wsum += tf.nn.embedding_lookup( cooc_vec, cooc_idx )\n",
    "                        self.regularizer_cooc += (tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2))\n",
    "        \n",
    "        \n",
    "#       feature interaction\n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"interact\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA[i], self.N_CONTI],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "\n",
    "                tmp_interact.append( tf.nn.embedding_lookup( w, self.dx_trans[i] ) )\n",
    "    \n",
    "                if i==0:\n",
    "                    inter_wsum   = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "            \n",
    "                else:\n",
    "                    inter_wsum + = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                    \n",
    "\n",
    "#       embedding hidden layers\n",
    "        with tf.variable_scope(\"embed_h0\"):\n",
    "            w= tf.Variable(tf.random_normal([self.N_TOTAL_EMBED, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_CONTI)))) \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            h_embed = tf.nn.relu( tf.add( tf.matmul(x_concate, w),b) )\n",
    "                \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"embed_h\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b= tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                h_embed = tf.nn.relu( tf.add( tf.matmul(h_embed, w),b) )\n",
    "                \n",
    "#       L2   \n",
    "        self.regularizer = tf.nn.l2_loss(w)\n",
    "\n",
    "        \n",
    "#       interaction hidden layers\n",
    "        inter_w = []\n",
    "        for i in range( n_hidden_list[0] ):\n",
    "            inter_w.append(inter_wsum)\n",
    "        \n",
    "        inter_w = tf.stack(inter_w)\n",
    "        inter_w = tf.transpose( inter_w, [1,0])\n",
    "\n",
    "        \n",
    "        with tf.variable_scope(\"inter_h0\"):\n",
    "            w= tf.Variable(tf.random_normal([self.N_CONTI, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_CONTI)))) \n",
    "            \n",
    "            w= w + inter_w\n",
    "            \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            h_inter ten= tf.nn.relu( tf.add( tf.matmul(x_concate, w),b) )\n",
    "                \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"inter_h\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b= tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                h_inter = tf.nn.relu( tf.add( tf.matmul(h_inter, w),b) )\n",
    "        \n",
    "#       dropout\n",
    "#       h = tf.nn.dropout(h, self.keep_prob)\n",
    "\n",
    "#       L2\n",
    "        self.regularizer + = tf.nn.l2_loss(w)\n",
    "        \n",
    "#       output layer  \n",
    "        h = h_embed + h_inter\n",
    "    \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w= tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1],\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            \n",
    "            h = tf.add( tf.matmul(h, w), dx_wsum )\n",
    "            h = tf.add( h, cooc_wsum )\n",
    "            h = tf.add( h, b )\n",
    "\n",
    "\n",
    "            self.logit = h\n",
    "     \n",
    "    \n",
    "#           L2  \n",
    "            self.regularizer + = tf.nn.l2_loss(w) \n",
    "    \n",
    "#       overall L2\n",
    "        self.regularizer + = (self.regularizer_wide + self.regularizer_cooc)\n",
    "         \n",
    "    \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y) \\\n",
    "                                  + self.L2*self.regularizer )\n",
    "        self.optimizer = \\\n",
    "        tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#       tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                       feed_dict={self.dx:dx_batch, \\\n",
    "                                  self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                  self.lr:lr,\\\n",
    "                                  self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "    \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                  self.cx:cx_test, self.y:y_test,\\\n",
    "                                                  self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        return sess.run( [self.tmpshape], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
