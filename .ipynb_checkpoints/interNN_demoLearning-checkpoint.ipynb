{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wide and embedding, \n",
    "# wide co-occurrence\n",
    "# wide co-occurrence & interaction \n",
    "\n",
    "# TO DO\n",
    "# batch normalization\n",
    "# weight normalization\n",
    "# max norm constraint: dropout paper \n",
    "\n",
    "# start with low regularization, large loss\n",
    "#  more parameters, high learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from six.moves import urllib\n",
    "# from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6938, 186) (771, 186) (6938, 1) (771, 1)\n",
      "(6938, 106) (6938, 80)\n",
      "(771, 106) (771, 80)\n"
     ]
    }
   ],
   "source": [
    "# continuous and categorical feature split\n",
    "\n",
    "# features on columns [0:split_idx] are continuous\n",
    "# the rest are categorical\n",
    "def conti_cate_split(dta_df, split_idx):\n",
    "    \n",
    "    col_cnt= dta_df.shape[1]\n",
    "    conti_cols= range(split_idx)\n",
    "    dis_cols=range(split_idx, col_cnt)\n",
    "    \n",
    "    return dta_df[conti_cols], dta_df[dis_cols]\n",
    "\n",
    "def conti_normalization_train_dta(dta_df):\n",
    "    \n",
    "    return preprocessing.scale(dta_df)\n",
    "\n",
    "def conti_normalization_test_dta(dta_df, train_df):\n",
    "    \n",
    "    mean_dim = np.mean(train_df, axis=0)\n",
    "    std_dim = np.std(train_df, axis=0)\n",
    "        \n",
    "    df=pd.DataFrame()\n",
    "    cols = train_df.columns\n",
    "    idx=0\n",
    "    \n",
    "    for i in cols:\n",
    "        df[i] = (dta_df[i]- mean_dim[idx])*1.0/std_dim[idx]\n",
    "        idx=idx+1\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------data prepro-----------------------\n",
    "\n",
    "files_list=[\"../dataset/dataset_demo/xtrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/xtest.csv\",\\\n",
    "            \"../dataset/dataset_demo/ytrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/ytest.csv\"]\n",
    "\n",
    "xtrain_df=pd.read_csv( files_list[0] ,sep=',', header=None)\n",
    "xtest_df=pd.read_csv( files_list[1] ,sep=',', header=None)\n",
    "ytrain_df=pd.read_csv( files_list[2] ,sep=',', header=None)\n",
    "ytest_df=pd.read_csv( files_list[3] ,sep=',', header=None)\n",
    "\n",
    "print xtrain_df.shape, xtest_df.shape, ytrain_df.shape, ytest_df.shape\n",
    "\n",
    "\n",
    "# get training and testing data prepared\n",
    "\n",
    "cxtrain, dxtrain = conti_cate_split(xtrain_df, 106)\n",
    "cxtest, dxtest = conti_cate_split(xtest_df, 106)\n",
    "\n",
    "cxtest = conti_normalization_test_dta(cxtest, cxtrain)\n",
    "cxtrain = conti_normalization_train_dta(cxtrain)\n",
    "\n",
    "print cxtrain.shape, dxtrain.shape\n",
    "print cxtest.shape, dxtest.shape\n",
    "\n",
    "# cxtrain = cxtrain.as_matrix() \n",
    "dxtrain = dxtrain.as_matrix().astype(int)\n",
    "cxtest = cxtest.as_matrix()\n",
    "dxtest = dxtest.as_matrix().astype(int)\n",
    "\n",
    "ytest = np_utils.to_categorical( ytest_df.as_matrix() )\n",
    "ytrain =np_utils.to_categorical( ytrain_df.as_matrix() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wide, embedding, co-occurrence and feature interaction \n",
    "# version 1: interaction as external hiddens\n",
    "\n",
    "class wide_embed_coocc_NN():\n",
    "    \n",
    "#   build the network graph\n",
    "    def __init__(self, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, session,n_embedding, n_hidden_list, lr, l2):\n",
    "        \n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.N_EMBED = n_embedding\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list)\n",
    "        self.N_TOTAL = self.N_CONTI+ self.N_DISC*self.N_EMBED\n",
    "\n",
    "        self.sess=session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "#       wide part on categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "        \n",
    "#         for i in range(self.N_DISC):\n",
    "            \n",
    "#             with tf.variable_scope(\"wide\"+str(i)):\n",
    "#                 w= tf.Variable(\\\n",
    "#                 tf.random_uniform([self.N_DISC_VOCA[i], self.N_CLASS],-1.0, 1.0) )\n",
    "# #                         tf.random_normal([self.N_DISC_VOCA, self.N_CLASS],\\\n",
    "# #                         stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "\n",
    "#                 b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "\n",
    "#                 if i==0:\n",
    "#                     dx_wsum = tf.nn.embedding_lookup( w, self.dx_trans[i] ) + b\n",
    "                    \n",
    "# #                   L2\n",
    "#                     self.regularizer_wide = tf.nn.l2_loss(w)\n",
    "                    \n",
    "#                 else:\n",
    "#                     dx_wsum = dx_wsum + tf.nn.embedding_lookup( w, self.dx_trans[i] ) + b\n",
    "                    \n",
    "# #                   L2  \n",
    "#                     self.regularizer_wide = self.regularizer_wide + tf.nn.l2_loss(w)\n",
    "        \n",
    "        \n",
    "# #       non-linear\n",
    "#         dx_wsum = tf.nn.relu( dx_wsum )\n",
    "\n",
    "        \n",
    "#       co-occurrence of categorical features\n",
    "        for i in range(20):\n",
    "            \n",
    "            tmpvoca1 = self.N_DISC_VOCA[i] \n",
    "            \n",
    "            for j in range(i+1,20):\n",
    "                \n",
    "                tmpvoca2 = self.N_DISC_VOCA[j]\n",
    "                \n",
    "                with tf.variable_scope(\"cooccur\"+str(i)+str(j)):\n",
    "                    \n",
    "#                   vector approximate of co-occurrence matrix \n",
    "                    w1 = tf.Variable(tf.random_normal([self.N_CLASS, tmpvoca1, 1],\\\n",
    "                                            stddev=math.sqrt(2.0/float( tmpvoca1 ))) )\n",
    "                    \n",
    "                    w2 = tf.Variable(tf.random_normal([self.N_CLASS, 1, tmpvoca2],\\\n",
    "                                            stddev=math.sqrt(2.0/float( tmpvoca2 ))) )\n",
    "            \n",
    "                    \n",
    "                    b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "\n",
    "                    \n",
    "                    cooc_mat = []\n",
    "                    for k in range(self.N_CLASS):\n",
    "                        cooc_mat.append( tf.reshape( tf.matmul(w1[k],w2[k]), [-1,1] ) )\n",
    "                        \n",
    "#                   n_class by voca1*voca2  \n",
    "                    cooc_vec = tf.stack(cooc_mat)\n",
    "                    cooc_vec = tf.reshape(cooc_vec, [self.N_CLASS, -1] )\n",
    "                    cooc_vec = tf.transpose(cooc_vec,[1,0])\n",
    "                    \n",
    "#                   build the idx of co-occurrence features\n",
    "                    cooc_idx = self.dx_trans[i] * tmpvoca2 + self.dx_trans[j]\n",
    "    \n",
    "                    if i==0 and j==1:\n",
    "                        cooc_wsum  = tf.nn.embedding_lookup( cooc_vec, cooc_idx ) + b\n",
    "#                       L2\n",
    "                        self.regularizer_cooc   = tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2)\n",
    "                    else:\n",
    "                        cooc_wsum += tf.nn.embedding_lookup( cooc_vec, cooc_idx ) + b\n",
    "#                       L2\n",
    "                        self.regularizer_cooc += ( tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2) )\n",
    "    \n",
    "        \n",
    "#       non-linear\n",
    "        cooc_wsum = tf.nn.relu( cooc_wsum )\n",
    "\n",
    "    \n",
    "#       feature interaction\n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"interact\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA[i], self.N_CONTI],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "        \n",
    "                if i==0:\n",
    "                    inter_wsum  = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "#                   L2 \n",
    "                    self.regularizer_inter  = tf.nn.l2_loss(w)\n",
    "            \n",
    "                else:\n",
    "                    inter_wsum += tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "#                   L2 \n",
    "                    self.regularizer_inter += tf.nn.l2_loss(w)\n",
    "\n",
    "    \n",
    "#       add noise bias\n",
    "        singleCol_h = tf.reduce_sum( self.cx * inter_wsum, 1 )\n",
    "        singleCol_h = tf.reshape( singleCol_h, [-1,1] )\n",
    "        \n",
    "#         tmp_h=[]\n",
    "#         for i in range( n_hidden_list[0] ):\n",
    "#             with tf.variable_scope(\"noise\"+str( i )):\n",
    "#                 noise_b = tf.Variable(tf.random_normal([ ],\\\n",
    "#                         stddev=math.sqrt(2.0/float(n_hidden_list[0]))))\n",
    "#                 tmp_h.append( singleCol_h + noise_b ) \n",
    "        \n",
    "#         tmp_h = tf.stack( tmp_h )\n",
    "#         tmp_h = tf.transpose( tmp_h, [1,0])\n",
    "            \n",
    "        \n",
    "#       interaction hidden layers\n",
    "        with tf.variable_scope(\"inter_h0\"):\n",
    "            w = tf.Variable(tf.random_normal([1, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(n_hidden_list[0])))) \n",
    "                        \n",
    "            b = tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            \n",
    "            h_inter = tf.matmul( singleCol_h, w )\n",
    "#             h_inter = h_inter + tmp_h\n",
    "            h_inter = tf.nn.relu( h_inter + b)\n",
    "#           L2  \n",
    "            self.regularizer_inter += tf.nn.l2_loss(w)\n",
    "            \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"inter_h\"+str(i)):\n",
    "                w = tf.Variable(\\\n",
    "                        tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b = tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                h_inter = tf.nn.relu( tf.add( tf.matmul(h_inter, w),b) )\n",
    "#               L2\n",
    "                self.regularizer_inter += tf.nn.l2_loss(w)\n",
    "    \n",
    "    \n",
    "#       embedding of categorical features        \n",
    "        tmp_embeded = []                \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"embedding\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA[i], self.N_EMBED],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "                              \n",
    "                tmp_embeded.append( tf.nn.embedding_lookup( w, self.dx_trans[i] ) )\n",
    "                \n",
    "        dx_embeded = tf.concat(tmp_embeded, 1)\n",
    "        x_concate =  tf.concat( [self.cx,dx_embeded], 1 )\n",
    "        \n",
    "#       embedding + continuous hidden layers\n",
    "        with tf.variable_scope(\"h0\"):\n",
    "            w= tf.Variable(tf.random_normal([self.N_TOTAL, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_CONTI)))) \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            h = tf.nn.relu( tf.add( tf.matmul(x_concate, w),b) )\n",
    "#           L2\n",
    "            self.regularizer = tf.nn.l2_loss(w)\n",
    "        \n",
    "                \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"layer\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b= tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                h = tf.nn.relu( tf.add( tf.matmul(h, w),b) )\n",
    "                \n",
    "#               L2\n",
    "                self.regularizer += tf.nn.l2_loss(w)\n",
    "        \n",
    "                \n",
    "#       dropout\n",
    "#       h = tf.nn.dropout(h, self.keep_prob)\n",
    "\n",
    "        \n",
    "#       output layer  \n",
    "        h = tf.concat([h, h_inter], 1)\n",
    "    \n",
    "        with tf.variable_scope(\"temp\"):\n",
    "            w= tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1]*2,\\\n",
    "                                             n_hidden_list[self.N_HIDDEN_LAYERS-1] ],\\\n",
    "                        stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1]))))\n",
    "                           \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[self.N_HIDDEN_LAYERS-1] ] ))\n",
    "            h = tf.nn.relu( tf.add( tf.matmul(h, w),b) )\n",
    "                           \n",
    "#           L2  \n",
    "            self.regularizer += tf.nn.l2_loss(w) \n",
    "\n",
    "    \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w= tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1],\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            \n",
    "            h = tf.matmul(h, w)\n",
    "#             h = tf.add( h, dx_wsum ) #wide\n",
    "            h = tf.add( h, cooc_wsum ) #co-occurrence\n",
    "            h = tf.add( h, b )\n",
    "\n",
    "            self.logit = h\n",
    "     \n",
    "    \n",
    "#           L2  \n",
    "            self.regularizer += tf.nn.l2_loss(w) \n",
    "    \n",
    "#       overall L2\n",
    "        self.regularizer += (self.regularizer_cooc + self.regularizer_inter)\n",
    "#     self.regularizer_wide + \n",
    "         \n",
    "    \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y) \\\n",
    "                                  + self.L2*self.regularizer )\n",
    "        self.optimizer = \\\n",
    "        tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#       tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                       feed_dict={self.dx:dx_batch, \\\n",
    "                                  self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                  self.lr:lr,\\\n",
    "                                  self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "    \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                  self.cx:cx_test, self.y:y_test,\\\n",
    "                                                  self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        return sess.run( [self.tmpshape], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch  0  :  2397.61354348 [0.23216602] [0.20942634]\n",
      "loss on epoch  10  :  1609.22220922 [0.2542153] [0.24517152]\n",
      "loss on epoch  20  :  890.686493202 [0.25032425] [0.24416259]\n",
      "loss on epoch  30  :  428.261118571 [0.25551233] [0.24589218]\n",
      "loss on epoch  40  :  179.341532813 [0.25551233] [0.24589218]\n",
      "loss on epoch  50  :  65.1177196149 [0.25551233] [0.24589218]\n",
      "loss on epoch  60  :  20.8747887965 [0.25551233] [0.24589218]\n",
      "loss on epoch  70  :  6.55449533904 [0.25551233] [0.24589218]\n",
      "loss on epoch  80  :  2.73966475549 [0.25551233] [0.24589218]\n",
      "loss on epoch  90  :  1.92791347316 [0.25551233] [0.24589218]\n",
      "loss on epoch  100  :  1.79373374416 [0.25551233] [0.24589218]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f001f7e52476>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mbatch_y\u001b[0m  \u001b[1;33m=\u001b[0m  \u001b[0mytrain\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0mtmpc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mbatch_dx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_cx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpara_keep_prob\u001b[0m\u001b[1;33m,\u001b[0m                                     \u001b[0mpara_cur_lr\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mpara_eval_byepoch\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-db66b3f63355>\u001b[0m in \u001b[0;36mtrain_batch\u001b[1;34m(self, dx_batch, cx_batch, y_batch, keep_prob, lr)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdx_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m                       \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdx_batch\u001b[0m\u001b[1;33m,\u001b[0m                                   \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mkeep_prob\u001b[0m                                 \u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    938\u001b[0m           \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m             raise ValueError(\n\u001b[0;32m    942\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# wide, embedding, co-occurrence and feature interaction \n",
    "# version 1: interaction as external hiddens\n",
    "\n",
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "\n",
    "#   representation ability\n",
    "para_n_hidden_list = [ 64,32,16 ]\n",
    "#     128,16,8 ]\n",
    "para_lr = 0.045\n",
    "para_n_embedding = 2\n",
    "#   regularization\n",
    "para_batch_size = 32\n",
    "para_keep_prob = 1.0\n",
    "para_l2 = 0.1\n",
    "\n",
    "# fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = [8]*para_n_disc\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 10\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    clf = wide_embed_coocc_NN( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                      para_n_disc_voca, sess, para_n_embedding, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2)\n",
    "    clf.train_ini()\n",
    "    clf.inference_ini()\n",
    "    \n",
    "    para_cur_lr=para_lr\n",
    "    \n",
    "    total_cnt= np.shape(cxtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "    \n",
    "    total_idx=range(total_cnt)\n",
    "    \n",
    "    \n",
    "#   training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc=0.0\n",
    "        \n",
    "#       shuffle traning instances each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "        \n",
    "#       Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                     para_cur_lr )\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "        \n",
    "        tmp_test_acc  = clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "        \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[ 64,32,16 ]\n",
    "\n",
    "0.055 0.25 \n",
    "\n",
    "0.055 0.15\n",
    "\n",
    "0.065 0.1 all regu\n",
    "\n",
    "loss on epoch  0  :  1172.07503877 [0.16601816] [0.17036609]\n",
    "loss on epoch  10  :  800.301286203 [0.21400778] [0.23205535]\n",
    "loss on epoch  20  :  448.827867861 [0.25162128] [0.26722398]\n",
    "loss on epoch  30  :  216.808312875 [0.2697795] [0.26635918]\n",
    "loss on epoch  40  :  91.3842687254 [0.29182878] [0.27428654]\n",
    "loss on epoch  50  :  34.0733124768 [0.30609599] [0.28365523]\n",
    "loss on epoch  60  :  11.7855009282 [0.31776914] [0.28740272]\n",
    "loss on epoch  70  :  4.48057140907 [0.28534371] [0.26289997]\n",
    "loss on epoch  80  :  2.49305601584 [0.28145266] [0.26592678]\n",
    "loss on epoch  90  :  2.02818673408 [0.2542153] [0.24603632]\n",
    "loss on epoch  100  :  1.91188755576 [0.25551233] [0.24589218]\n",
    "loss on epoch  110  :  1.86578034582 [0.25551233] [0.24589218]\n",
    "loss on epoch  120  :  1.83540236564 [0.25551233] [0.24589218]\n",
    "loss on epoch  130  :  1.81479061312 [0.25551233] [0.24589218]\n",
    "loss on epoch  140  :  1.79988374147 [0.25551233] [0.24589218]\n",
    "loss on epoch  150  :  1.78881633227 [0.25551233] [0.24589218]\n",
    "loss on epoch  160  :  1.7794387186 [0.25551233] [0.24589218]\n",
    "loss on epoch  170  :  1.77178843706 [0.25551233] [0.24589218]\n",
    "loss on epoch  180  :  1.76583942274 [0.25551233] [0.24589218]\n",
    "loss on epoch  190  :  1.76063220037 [0.25551233] [0.24589218]\n",
    "loss on epoch  200  :  1.75678650704 [0.25551233] [0.24589218]\n",
    "loss on epoch  210  :  1.75275016493 [0.25551233] [0.24589218]\n",
    "loss on epoch  220  :  1.75108405727 [0.25551233] [0.24589218]\n",
    "loss on epoch  230  :  1.74886897206 [0.25551233] [0.24589218]\n",
    "loss on epoch  240  :  1.74611885459 [0.25551233] [0.24589218]\n",
    "loss on epoch  250  :  1.74387550133 [0.25551233] [0.24589218]\n",
    "loss on epoch \n",
    "\n",
    "0.065 0.1 drop some L2, add interaction regu\n",
    "\n",
    "loss on epoch  0  :  1167.35836001 [0.25810635] [0.23565869]\n",
    "loss on epoch  10  :  799.897740117 [0.23476005] [0.24257711]\n",
    "loss on epoch  20  :  449.965487021 [0.25551233] [0.26434129]\n",
    "loss on epoch  30  :  218.084557498 [0.25551233] [0.26837707]\n",
    "loss on epoch  40  :  92.3575367398 [0.26329443] [0.28567311]\n",
    "loss on epoch  50  :  34.6127185292 [0.28793773] [0.30037475]\n",
    "loss on epoch  60  :  12.138738981 [0.30998704] [0.31522053]\n",
    "loss on epoch  70  :  4.73840802025 [0.32166019] [0.3248775]\n",
    "loss on epoch  80  :  2.6576786847 [0.32814527] [0.33439031]\n",
    "loss on epoch  90  :  2.11652825442 [0.30479896] [0.3441914]\n",
    "loss on epoch  100  :  1.94270474105 [0.30220494] [0.35745171]\n",
    "loss on epoch  110  :  1.85257242344 [0.31128404] [0.37705392]\n",
    "loss on epoch  120  :  1.77273416464 [0.31128404] [0.41582587]\n",
    "loss on epoch  130  :  1.63825527275 [0.28664073] [0.49827039]\n",
    "loss on epoch  140  :  1.46095741568 [0.28145266] [0.58835399]\n",
    "loss on epoch  150  :  1.29705251036 [0.27237353] [0.64917845]\n",
    "loss on epoch  160  :  1.16783825032 [0.26718548] [0.68881524]\n",
    "loss on epoch  170  :  1.06029736609 [0.25940338] [0.72023636]\n",
    "loss on epoch  180  :  0.966977777029 [0.25291827] [0.79633898]\n",
    "\n",
    "\n",
    "0.065 0.1 drop some L2\n",
    "\n",
    "loss on epoch  0  :  325.844520993 [0.14396887] [0.13116172]\n",
    "loss on epoch  10  :  217.151938403 [0.21530479] [0.2267224]\n",
    "loss on epoch  20  :  122.24666433 [0.28015563] [0.27990776]\n",
    "loss on epoch  30  :  59.594521081 [0.28534371] [0.29187086]\n",
    "loss on epoch  40  :  25.9309680992 [0.30609599] [0.294177]\n",
    "loss on epoch  50  :  10.6316536621 [0.29831389] [0.31320265]\n",
    "loss on epoch  60  :  4.76818786506 [0.32295719] [0.31781495]\n",
    "loss on epoch  70  :  2.88817745889 [0.32295719] [0.32819256]\n",
    "loss on epoch  80  :  2.37351186849 [0.30090791] [0.3377054]\n",
    "loss on epoch  90  :  2.23753988357 [0.30479896] [0.34087634]\n",
    "loss on epoch  100  :  2.18417358785 [0.31647211] [0.34202939]\n",
    "loss on epoch  110  :  2.14660033253 [0.32555124] [0.34202939]\n",
    "loss on epoch  120  :  2.11423155997 [0.31776914] [0.34851542]\n",
    "loss on epoch  130  :  2.08518664539 [0.31776914] [0.35038915]\n",
    "loss on epoch  140  :  2.0599590264 [0.30609599] [0.3501009]\n",
    "loss on epoch  150  :  2.03664129255 [0.30869001] [0.35168636]\n",
    "loss on epoch  160  :  2.01500192505 [0.31647211] [0.35673106]\n",
    "loss on epoch  170  :  1.99557032629 [0.31258106] [0.35946959]\n",
    "loss on epoch  180  :  1.97540657222 [0.32036316] [0.36321706]\n",
    "loss on epoch  190  :  1.95726226049 [0.31128404] [0.36422601]\n",
    "loss on epoch  200  :  1.94178240277 [0.30609599] [0.36364946]\n",
    "loss on epoch  210  :  1.92472366713 [0.30998704] [0.3735947]\n",
    "loss on epoch \n",
    "\n",
    "0.065 0.1\n",
    "\n",
    "loss on epoch  0  :  332.805889836 [0.17509727] [0.1836264]\n",
    "loss on epoch  10  :  224.487996984 [0.23605707] [0.23882963]\n",
    "loss on epoch  20  :  126.475320039 [0.27885863] [0.2711156]\n",
    "loss on epoch  30  :  61.8446098963 [0.29961088] [0.28596136]\n",
    "loss on epoch  40  :  27.0856315825 [0.29831389] [0.2822139]\n",
    "loss on epoch  50  :  11.1628189175 [0.28793773] [0.27241281]\n",
    "loss on epoch  60  :  4.88656196329 [0.28534371] [0.27558374]\n",
    "loss on epoch  70  :  2.78419123314 [0.28664073] [0.27399826]\n",
    "loss on epoch  80  :  2.17331035546 [0.26070037] [0.27053908]\n",
    "loss on epoch  90  :  1.99862699707 [0.24513619] [0.26347652]\n",
    "loss on epoch  100  :  1.93286249318 [0.23735408] [0.26347652]\n",
    "loss on epoch  110  :  1.89984143884 [0.2386511] [0.26390889]\n",
    "loss on epoch  120  :  1.87880094129 [0.23994812] [0.26578265]\n",
    "loss on epoch  130  :  1.86088351243 [0.24254215] [0.27212453]\n",
    "loss on epoch  140  :  1.84647996613 [0.2464332] [0.27471894]\n",
    "loss on epoch  150  :  1.83207054105 [0.24773023] [0.27644855]\n",
    "loss on epoch  160  :  1.81919553048 [0.25291827] [0.27745748]\n",
    "loss on epoch  170  :  1.80824454515 [0.25162128] [0.28062841]\n",
    "loss on epoch  180  :  1.79873686643 [0.24902724] [0.28077257]\n",
    "loss on epoch  190  :  1.78950325427 [0.25032425] [0.2835111]\n",
    "loss on epoch  200  :  1.78142097427 [0.24773023] [0.28495243]\n",
    "loss on epoch  210  :  1.77397503069 [0.25032425] [0.28552896]\n",
    "loss on epoch  220  :  1.76741607322 [0.24383917] [0.28466415]\n",
    "loss on epoch  230  :  1.76024768419 [0.24124514] [0.28740272]\n",
    "loss on epoch  240  :  1.75431586029 [0.23994812] [0.28942057]\n",
    "loss on epoch  250  :  1.74874272115 [0.22827497] [0.28869992]\n",
    "loss on epoch  260  :  1.74332360978 [0.2386511] [0.29172671]\n",
    "loss on epoch  270  :  1.73768643373 [0.24254215] [0.29662728]\n",
    "loss on epoch  280  :  1.73303826153 [0.23994812] [0.29749209]\n",
    "loss on epoch  290  :  1.72819698795 [0.23605707] [0.29388872]\n",
    "loss on epoch  300  :  1.72463215667 [0.23994812] [0.29864514]\n",
    "loss on epoch  310  :  1.72031790846 [0.23735408] [0.30354568]\n",
    "loss on epoch  320  :  1.71613417952 [0.2386511] [0.30455464]\n",
    "loss on epoch  330  :  1.71238634542 [0.23735408] [0.30426636]\n",
    "loss on epoch  340  :  1.7087495774 [0.23994812] [0.30368984]\n",
    "loss on epoch  350  :  1.70528704645 [0.24254215] [0.30815798]\n",
    "loss on epoch  360  :  1.70067572925 [0.23994812] [0.30599597]\n",
    "loss on epoch  370  :  1.69772587882 [0.24124514] [0.31147304]\n",
    "loss on epoch  380  :  1.69505295157 [0.24383917] [0.31132892]\n",
    "loss on epoch  390  :  1.69087570409 [0.2464332] [0.30974343]\n",
    "loss on epoch  400  :  1.68872178594 [0.24513619] [0.31132892\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wide, embedding, co-occurrence and feature interaction \n",
    "# version 2: individual hidden layers on embedding and interaction\n",
    "\n",
    "class InterNN_IndiH():\n",
    "    \n",
    "    def __init__(self, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, session,n_embedding, n_hidden_list, lr, l2, batch_size):\n",
    "        \n",
    "#       build the network graph \n",
    "        self.LEARNING_RATE = lr\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.N_EMBED = n_embedding\n",
    "        self.N_BATCH = batch_size\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list)\n",
    "        self.N_TOTAL = self.N_CONTI+ self.N_DISC*self.N_EMBED\n",
    "        self.N_TOTAL_EMBED = self.N_DISC*self.N_EMBED\n",
    "\n",
    "        self.sess=session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "#       wide part on categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "    \n",
    "    \n",
    "    \n",
    "#         for i in range(self.N_DISC):\n",
    "            \n",
    "#             with tf.variable_scope(\"wide\"+str(i)):\n",
    "#                 w= tf.Variable(\\\n",
    "#                 tf.random_uniform([self.N_DISC_VOCA[i], self.N_CLASS],-1.0, 1.0) )\n",
    "# #                         tf.random_normal([self.N_DISC_VOCA, self.N_CLASS],\\\n",
    "# #                         stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "#                 if i==0:\n",
    "#                     dx_wsum = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                    \n",
    "# #                   L2\n",
    "#                     self.regularizer_wide = tf.nn.l2_loss(w)\n",
    "                    \n",
    "#                 else:\n",
    "#                     dx_wsum = dx_wsum + tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                    \n",
    "# #                   L2  \n",
    "#                     self.regularizer_wide = self.regularizer_wide + tf.nn.l2_loss(w)\n",
    "               \n",
    "    \n",
    "    \n",
    "    \n",
    "#       co-occurrence of categorical features\n",
    "        for i in range(20):\n",
    "            \n",
    "            tmpvoca1 = self.N_DISC_VOCA[i] \n",
    "            \n",
    "            for j in range(i+1,20):\n",
    "                \n",
    "                tmpvoca2 = self.N_DISC_VOCA[j]\n",
    "                \n",
    "                with tf.variable_scope(\"cooccur\"+str(i)+str(j)):\n",
    "                    \n",
    "                    \n",
    "#                   vector approximate of co-occurrence matrix \n",
    "                    w1 = tf.Variable(tf.random_normal([self.N_CLASS, tmpvoca1, 1],\\\n",
    "                                            stddev=math.sqrt(2.0/float( tmpvoca1 ))) )\n",
    "                    \n",
    "                    w2 = tf.Variable(tf.random_normal([self.N_CLASS, 1, tmpvoca2],\\\n",
    "                                            stddev=math.sqrt(2.0/float( tmpvoca2 ))) )\n",
    "            \n",
    "                    \n",
    "                    b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "\n",
    "                    \n",
    "                    cooc_mat = []\n",
    "                    for k in range(self.N_CLASS):\n",
    "                        cooc_mat.append( tf.reshape( tf.matmul(w1[k],w2[k]), [-1,1] ) )\n",
    "                        \n",
    "#                   n_class by voca1*voca2  \n",
    "        \n",
    "                    cooc_vec = tf.stack(cooc_mat)\n",
    "                    cooc_vec = tf.reshape(cooc_vec, [self.N_CLASS, -1] )\n",
    "                    cooc_vec = tf.transpose(cooc_vec,[1,0])\n",
    "                    \n",
    "#                   build the idx of co-occurrence features\n",
    "                    cooc_idx = self.dx_trans[i] * tmpvoca2 + self.dx_trans[j]\n",
    "    \n",
    "                    if i==0 and j==1:\n",
    "                        cooc_wsum  = tf.nn.embedding_lookup( cooc_vec, cooc_idx ) + b\n",
    "#                       L2 \n",
    "                        self.regularizer_cooc   = tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2) \n",
    "                    else:\n",
    "                        cooc_wsum += tf.nn.embedding_lookup( cooc_vec, cooc_idx ) + b\n",
    "#                       L2\n",
    "                        self.regularizer_cooc += (tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2))\n",
    "    \n",
    "#       non-linear \n",
    "        cooc_wsum = tf.nn.relu( cooc_wsum )\n",
    "        \n",
    "        \n",
    "        \n",
    "#       embedding of categorical features        \n",
    "        tmp_embeded = []                \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"embedding\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA[i], self.N_EMBED],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "                              \n",
    "                tmp_embeded.append( tf.nn.embedding_lookup( w, self.dx_trans[i] ) )\n",
    "                \n",
    "        dx_embeded = tf.concat(tmp_embeded, 1)\n",
    "        \n",
    "        \n",
    "#       embedding hidden layers\n",
    "        with tf.variable_scope(\"embed_h0\"):\n",
    "            w= tf.Variable(tf.random_normal([self.N_TOTAL_EMBED, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_CONTI)))) \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            h_embed = tf.nn.relu( tf.add( tf.matmul(dx_embeded, w),b) )\n",
    "            \n",
    "#           L2   \n",
    "            self.regularizer = tf.nn.l2_loss(w)\n",
    "                \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"embed_h\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b= tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                h_embed = tf.nn.relu( tf.add( tf.matmul(h_embed, w),b) )       \n",
    "            \n",
    "#               L2   \n",
    "                self.regularizer += tf.nn.l2_loss(w)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#       feature interaction\n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"interact\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA[i], self.N_CONTI],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "        \n",
    "                if i==0:\n",
    "                    inter_wsum  = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "#                   L2 \n",
    "                    self.regularizer_inter  = tf.nn.l2_loss(w)\n",
    "            \n",
    "                else:\n",
    "                    inter_wsum += tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "#                   L2 \n",
    "                    self.regularizer_inter += tf.nn.l2_loss(w)\n",
    "\n",
    "#       add noise bias\n",
    "        singleCol_h = tf.reduce_sum( self.cx * inter_wsum, 1   )\n",
    "        tmp_h=[]\n",
    "        for i in range( n_hidden_list[0] ):\n",
    "            with tf.variable_scope(\"noise\"+str( i )):\n",
    "                noise_b = tf.Variable(tf.random_normal([ ],\\\n",
    "                        stddev=math.sqrt(2.0/float(n_hidden_list[0]))))\n",
    "                tmp_h.append( singleCol_h + noise_b ) \n",
    "                \n",
    "#               L2  \n",
    "                self.regularizer_inter += tf.nn.l2_loss(noise_b)\n",
    "        \n",
    "#         tmp_h=[]\n",
    "#         for i in range( n_hidden_list[0] ):\n",
    "#             with tf.variable_scope(\"noise\"+str( i )):\n",
    "#                 noise_b = tf.Variable(tf.random_normal([ self.N_CONTI ],\\\n",
    "#                         stddev=math.sqrt(2.0/float(n_hidden_list[0]))))\n",
    "#                 tmp_h.append( tf.reduce_sum( self.cx * (inter_wsum + noise_b), 1 ) ) \n",
    "        \n",
    "        tmp_h = tf.stack( tmp_h )\n",
    "        tmp_h = tf.transpose( tmp_h, [1,0])\n",
    "            \n",
    "        \n",
    "#       interaction hidden layers\n",
    "\n",
    "\n",
    "#         singleCol_h = tf.reduce_sum( self.cx * inter_wsum, 1   )\n",
    "        \n",
    "#         tmp_h = [ singleCol_h ]*n_hidden_list[0]\n",
    "        \n",
    "#         tmp_h = tf.stack( tmp_h )\n",
    "#         tmp_h = tf.transpose( tmp_h, [1,0])\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope(\"inter_h0\"):\n",
    "            w = tf.Variable(tf.random_normal([self.N_CONTI, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_CONTI)))) \n",
    "                        \n",
    "            b = tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            \n",
    "            h_inter = tf.matmul(self.cx, w)\n",
    "            h_inter = h_inter + tmp_h\n",
    "            h_inter = tf.nn.relu( h_inter + b)\n",
    "                             \n",
    "#           L2  \n",
    "            self.regularizer += tf.nn.l2_loss(w)\n",
    "            \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"inter_h\"+str(i)):\n",
    "                w = tf.Variable(\\\n",
    "                        tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b = tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                h_inter = tf.nn.relu( tf.add( tf.matmul(h_inter, w),b) )\n",
    "        \n",
    "#               L2\n",
    "                self.regularizer += tf.nn.l2_loss(w)\n",
    "    \n",
    "    \n",
    "#       dropout\n",
    "#       h = tf.nn.dropout(h, self.keep_prob)\n",
    "        \n",
    "        \n",
    "#       output layer  \n",
    "        h = tf.concat([h_embed, h_inter], 1)\n",
    "        \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w = tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1]*2,\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "            b = tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            \n",
    "            h = tf.matmul(h, w)\n",
    "#             h = tf.add( h, dx_wsum )\n",
    "            h = tf.add( h, cooc_wsum )\n",
    "            h = tf.add( h, b )\n",
    "        \n",
    "            self.logit = h\n",
    "            \n",
    "#           L2  \n",
    "            self.regularizer += tf.nn.l2_loss(w) \n",
    "    \n",
    "    \n",
    "#       overall L2\n",
    "        self.regularizer += (self.regularizer_cooc + self.regularizer_inter)\n",
    "#       self.regularizer_wide + \n",
    "         \n",
    "        \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y) \\\n",
    "                                  + self.L2*self.regularizer )\n",
    "        self.optimizer = \\\n",
    "        tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#       tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                       feed_dict={self.dx:dx_batch, \\\n",
    "                                  self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                  self.lr:lr,\\\n",
    "                                  self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "        \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                  self.cx:cx_test, self.y:y_test,\\\n",
    "                                                  self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        return sess.run( [self.tmp1], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wide, embedding, co-occurrence and feature interaction \n",
    "# version 2: individual hidden layers on embedding and interaction\n",
    "\n",
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "\n",
    "#   representation ability\n",
    "para_n_hidden_list = [ 128,64,32,16 ]\n",
    "#     128,16,8 ]\n",
    "para_lr = 0.075\n",
    "para_n_embedding = 2\n",
    "#   regularization\n",
    "para_batch_size = 32\n",
    "para_keep_prob = 1.0\n",
    "para_l2 = 0.25\n",
    "\n",
    "# fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = [8]*para_n_disc\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 10\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    clf = InterNN_IndiH( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                      para_n_disc_voca, sess, para_n_embedding, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2, para_batch_size)\n",
    "    clf.train_ini()\n",
    "    clf.inference_ini()\n",
    "    \n",
    "    para_cur_lr=para_lr\n",
    "    \n",
    "    total_cnt= np.shape(cxtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "    \n",
    "    total_idx=range(total_cnt)\n",
    "    \n",
    "    \n",
    "#   training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc=0.0\n",
    "        \n",
    "#       shuffle traning instances each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "        \n",
    "#       Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                     para_cur_lr )\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "        \n",
    "        tmp_test_acc  = clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "        \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[128, 64, 32, 16]\n",
    "\n",
    "0.075 0.25\n",
    "\n",
    "loss on epoch  0  :  2980.92136863 [0.1919585] [0.1814644]\n",
    "loss on epoch  10  :  1920.12954881 [0.230869] [0.22614586]\n",
    "loss on epoch  20  :  999.76868467 [0.21271077] [0.22946094]\n",
    "loss on epoch  30  :  444.889363607 [0.26848248] [0.25036034]\n",
    "loss on epoch  40  :  171.663069054 [0.2542153] [0.27601615]\n",
    "loss on epoch  50  :  60.137812067 [0.26718548] [0.27169213]\n",
    "loss on epoch  60  :  22.0798286862 [0.2775616] [0.26780051]\n",
    "loss on epoch  70  :  10.2165232632 [0.29053178] [0.26996252]\n",
    "loss on epoch  80  :  5.84338207598 [0.30090791] [0.2711156]\n",
    "loss on epoch  90  :  3.5995438695 [0.31128404] [0.27644855]\n",
    "loss on epoch  100  :  2.41466889116 [0.31128404] [0.27875468]\n",
    "loss on epoch  110  :  2.02685040567 [0.31258106] [0.28279042]\n",
    "loss on epoch  120  :  1.87719280852 [0.30220494] [0.27961949]\n",
    "loss on epoch  130  :  1.81999501917 [0.30350193] [0.27875468]\n",
    "loss on epoch  140  :  1.79834823973 [0.26459143] [0.26160276]\n",
    "loss on epoch  150  :  1.78521902142 [0.26329443] [0.26549438]\n",
    "loss on epoch  160  :  1.77017820268 [0.27107653] [0.26145864]\n",
    "loss on epoch  170  :  1.75266613894 [0.25551233] [0.24589218]\n",
    "loss on epoch  180  :  1.74597876878 [0.25551233] [0.24589218]\n",
    "loss on epoch  190  :  1.74167404517 [0.25551233] [0.24589218]\n",
    "loss on epoch  200  :  1.73957077607 [0.25551233] [0.24589218]\n",
    "loss on epoch  210  :  1.7380380349 [0.25551233] [0.24589218]\n",
    "loss on epoch  220  :  1.73560580611 [0.25551233] [0.24589218]\n",
    "loss on epoch  230  :  1.73450110853 [0.25551233] [0.24589218]\n",
    "loss on epoch  240  :  1.73394604634 [0.25551233] [0.24589218]\n",
    "\n",
    "\n",
    "\n",
    "0.04 0.15\n",
    "loss on epoch  0  :  1813.85026268 [0.14007781] [0.17223984]\n",
    "loss on epoch  10  :  1444.94381431 [0.21919584] [0.2164889]\n",
    "loss on epoch  20  :  1027.52567743 [0.22438392] [0.22960508]\n",
    "loss on epoch  30  :  674.013945685 [0.23346303] [0.24055924]\n",
    "loss on epoch  40  :  413.576934814 [0.28145266] [0.25569329]\n",
    "loss on epoch  50  :  238.739363847 [0.28793773] [0.25381956]\n",
    "loss on epoch  60  :  131.519887253 [0.29312581] [0.26707986]\n",
    "loss on epoch  70  :  70.8839188328 [0.28404668] [0.27601615]\n",
    "loss on epoch  80  :  38.5956713888 [0.28664073] [0.27500722]\n",
    "loss on epoch  90  :  22.4106277007 [0.28015563] [0.27659267]\n",
    "loss on epoch  100  :  14.5088500005 [0.28274968] [0.28581724]\n",
    "loss on epoch  110  :  10.4810321773 [0.28145266] [0.28322282]\n",
    "loss on epoch  120  :  8.26340355697 [0.30479896] [0.28898817]\n",
    "loss on epoch  130  :  6.84820767464 [0.30479896] [0.29677141]\n",
    "loss on epoch  140  :  5.76905129132 [0.31776914] [0.29172671]\n",
    "loss on epoch  150  :  4.88457653699 [0.30739298] [0.30008647]\n",
    "loss on epoch  160  :  4.13123914158 [0.30998704] [0.29994234]\n",
    "loss on epoch  170  :  3.48497954452 [0.31776914] [0.30628422]\n",
    "loss on epoch  180  :  2.94781986431 [0.31128404] [0.31507638]\n",
    "loss on epoch  190  :  2.55362282528 [0.30479896] [0.3202652]\n",
    "loss on epoch  200  :  2.3066153736 [0.30350193] [0.32559815]\n",
    "loss on epoch  210  :  2.14339051313 [0.29701686] [0.33294898]\n",
    "loss on epoch  220  :  2.03205191151 [0.30479896] [0.34202939]\n",
    "loss on epoch  230  :  1.95423617628 [0.29571983] [0.35773998]\n",
    "loss on epoch  240  :  1.88982394338 [0.29312581] [0.37835112]\n",
    "loss on epoch  250  :  1.83355036268 [0.28793773] [0.39939463]\n",
    "loss on epoch  260  :  1.78425233232 [0.27367055] [0.42029405]\n",
    "loss on epoch  270  :  1.74030464281 [0.27237353] [0.43787834]\n",
    "loss on epoch  280  :  1.70205291443 [0.27107653] [0.47492072]\n",
    "loss on epoch  290  :  1.66948302587 [0.27237353] [0.4876045]\n",
    "loss on epoch  300  :  1.64072179629 [0.26848248] [0.53617758]\n",
    "\n",
    "0.04 0.25\n",
    "\n",
    "loss on epoch  0  :  3022.73503847 [0.13748379] [0.11617181]\n",
    "loss on epoch  10  :  2354.38716295 [0.22438392] [0.21951571]\n",
    "loss on epoch  20  :  1630.79777922 [0.25291827] [0.24113578]\n",
    "loss on epoch  30  :  1041.21271317 [0.2775616] [0.2706832]\n",
    "loss on epoch  40  :  619.60531588 [0.28534371] [0.26909772]\n",
    "loss on epoch  50  :  344.483751791 [0.27237353] [0.26823291]\n",
    "loss on epoch  60  :  179.307656889 [0.27626458] [0.26044971]\n",
    "loss on epoch  70  :  87.79225028 [0.27885863] [0.26102623]\n",
    "loss on epoch  80  :  40.9197322174 [0.2775616] [0.25771114]\n",
    "loss on epoch  90  :  18.6923910865 [0.25810635] [0.248919]\n",
    "loss on epoch  100  :  8.86420894111 [0.25551233] [0.24560392]\n",
    "loss on epoch  110  :  4.75540789851 [0.25940338] [0.24949554]\n",
    "loss on epoch  120  :  3.10697445273 [0.30609599] [0.2822139]\n",
    "loss on epoch  130  :  2.41909366625 [0.31517509] [0.28596136]\n",
    "loss on epoch  140  :  2.10791990382 [0.31258106] [0.28985298]\n",
    "loss on epoch  150  :  1.97171778977 [0.32555124] [0.29230326]\n",
    "loss on epoch  160  :  1.913080184 [0.32295719] [0.29518592]\n",
    "loss on epoch  170  :  1.87703902743 [0.32036316] [0.29720381]\n",
    "loss on epoch  180  :  1.85110016498 [0.32166019] [0.29605073]\n",
    "loss on epoch  190  :  1.83072735976 [0.32295719] [0.29806861]\n",
    "loss on epoch  200  :  1.81598060661 [0.32425421] [0.29806861]\n",
    "loss on epoch  210  :  1.80346884716 [0.32684824] [0.29691553]\n",
    "loss on epoch  220  :  1.79394866085 [0.32684824] [0.29648313]\n",
    "loss on epoch  230  :  1.78761300058 [0.32295719] [0.29734793]\n",
    "loss on epoch  240  :  1.78324816238 [0.32684824] [0.29720381]\n",
    "loss on epoch  250  :  1.77923020058 [0.32555124] [0.2959066]\n",
    "loss on epoch  260  :  1.77586780433 [0.32425421] [0.2950418]\n",
    "loss on epoch  270  :  1.77452967288 [0.32166019] [0.2950418]\n",
    "loss on epoch  280  :  1.77241332387 [0.32555124] [0.294177]\n",
    "loss on epoch  290  :  1.77146135657 [0.32036316] [0.29345632]\n",
    "loss on epoch  300  :  1.77025835216 [0.32684824] [0.29287979]\n",
    "loss on epoch  310  :  1.7693916653 [0.32166019] [0.29143846]\n",
    "loss on epoch  320  :  1.76785501617 [0.32166019] [0.29287979]\n",
    "loss on epoch  330  :  1.76768690734 [0.32425421] [0.29187086]\n",
    "loss on epoch  340  :  1.76657673054 [0.32166019] [0.29287979]\n",
    "loss on epoch  350  :  1.76623662037 [0.31517509] [0.29360047]\n",
    "loss on epoch  360  :  1.76513794175 [0.31517509] [0.29273567]\n",
    "loss on epoch  370  :  1.76418453345 [0.31776914] [0.29316807]\n",
    "loss on epoch  380  :  1.76357118675 [0.31647211] [0.29215911]\n",
    "loss on epoch  390  :  1.76390206262 [0.31906614] [0.29244739]\n",
    "loss on epoch  400  :  1.76301965007 [0.31776914] [0.29273567]\n",
    "\n",
    "0.065 0.25\n",
    "loss on epoch  0  :  3000.83908759 [0.17120622] [0.17598732]\n",
    "loss on epoch  10  :  2021.8409729 [0.26070037] [0.24084751]\n",
    "loss on epoch  20  :  1133.03225482 [0.28274968] [0.27241281]\n",
    "loss on epoch  30  :  553.712874236 [0.28015563] [0.28077257]\n",
    "loss on epoch  40  :  238.452506313 [0.28664073] [0.28250217]\n",
    "loss on epoch  50  :  92.1786404009 [0.29701686] [0.28797925]\n",
    "loss on epoch  60  :  34.2706054582 [0.29961088] [0.29244739]\n",
    "loss on epoch  70  :  14.0935545409 [0.30350193] [0.29129431]\n",
    "loss on epoch  80  :  7.20146068158 [0.30479896] [0.29720381]\n",
    "loss on epoch  90  :  4.41210737714 [0.30479896] [0.31651774]\n",
    "loss on epoch  100  :  3.03185175194 [0.31906614] [0.32862496]\n",
    "loss on epoch  110  :  2.36303060474 [0.31906614] [0.32271549]\n",
    "loss on epoch  120  :  2.07406861087 [0.32295719] [0.32876909]\n",
    "loss on epoch  130  :  1.94719672369 [0.31517509] [0.32848084]\n",
    "loss on epoch  140  :  1.88129817281 [0.32295719] [0.32675123]\n",
    "loss on epoch  150  :  1.84270298481 [0.31647211] [0.32689536]\n",
    "loss on epoch  160  :  1.81902885437 [0.31258106] [0.32545403]\n",
    "loss on epoch  170  :  1.80198419536 [0.31128404] [0.32430094]\n",
    "loss on epoch  180  :  1.79108801539 [0.30998704] [0.3240127]\n",
    "loss on epoch  190  :  1.78217020355 [0.31258106] [0.32271549]\n",
    "loss on epoch  200  :  1.7755769138 [0.31258106] [0.32098588]\n",
    "loss on epoch  210  :  1.77042962887 [0.30739298] [0.32170653]\n",
    "loss on epoch  220  :  1.76636268806 [0.31647211] [0.3198328]\n",
    "loss on epoch  230  :  1.7633897894 [0.31647211] [0.32185069]\n",
    "loss on epoch  240  :  1.76053985401 [0.31647211] [0.32242721]\n",
    "loss on epoch  250  :  1.75875376993 [0.31776914] [0.32228309]\n",
    "loss on epoch  260  :  1.75633004749 [0.32036316] [0.32170653]\n",
    "loss on epoch  270  :  1.75460924263 [0.31647211] [0.32285962]\n",
    "loss on epoch  280  :  1.75377040605 [0.31387809] [0.32213894]\n",
    "loss on epoch  290  :  1.75220181986 [0.32036316] [0.3240127]\n",
    "loss on epoch  300  :  1.75125846708 [0.31647211] [0.32257134]\n",
    "loss on epoch  310  :  1.75043590312 [0.31776914] [0.32300374]\n",
    "loss on epoch  320  :  1.74990666244 [0.31258106] [0.32314789]\n",
    "loss on epoch  330  :  1.74844655615 [0.31517509] [0.3253099]\n",
    "loss on epoch  340  :  1.74757094129 [0.31387809] [0.32430094]\n",
    "loss on epoch  350  :  1.74773016682 [0.30739298] [0.32300374]\n",
    "loss on epoch  360  :  1.74644510117 [0.31128404] [0.32675123]\n",
    "loss on epoch  370  :  1.74650620586 [0.31258106] [0.32257134]\n",
    "loss on epoch  380  :  1.74503148982 [0.30869001] [0.32473335]\n",
    "loss on epoch  390  :  1.74497717398 [0.31258106] [0.32458922]\n",
    "loss on epoch  400  :  1.74421742779 [0.30090791] [0.32776016]\n",
    "loss on epoch  410  :  1.74366191581 [0.30479896] [0.32646295]\n",
    "loss on epoch  420  :  1.74315313774 [0.31128404] [0.32617468]\n",
    "loss on epoch \n",
    "\n",
    "0.065 0.2\n",
    "\n",
    "loss on epoch  0  :  2415.1183709 [0.1841764] [0.19054483]\n",
    "loss on epoch  10  :  1640.92718393 [0.25032425] [0.23508215]\n",
    "loss on epoch  20  :  923.604782952 [0.22697794] [0.23277602]\n",
    "loss on epoch  30  :  452.972128691 [0.22308689] [0.25439608]\n",
    "loss on epoch  40  :  196.297149658 [0.24902724] [0.253243]\n",
    "loss on epoch  50  :  76.8364204124 [0.2697795] [0.26621506]\n",
    "loss on epoch  60  :  29.1407067687 [0.26588845] [0.28682616]\n",
    "loss on epoch  70  :  12.2159188324 [0.30350193] [0.30974343]\n",
    "loss on epoch  80  :  6.27734933297 [0.31647211] [0.32257134]\n",
    "loss on epoch  90  :  3.86496341008 [0.32295719] [0.31767079]\n",
    "loss on epoch  100  :  2.80003212668 [0.31128404] [0.32516575]\n",
    "loss on epoch  110  :  2.27337547695 [0.31647211] [0.3304987]\n",
    "loss on epoch  120  :  2.02081030331 [0.31517509] [0.33323723]\n",
    "loss on epoch  130  :  1.91915968833 [0.31776914] [0.33712885]\n",
    "loss on epoch  140  :  1.86473320866 [0.31647211] [0.33943498]\n",
    "loss on epoch  150  :  1.83126418017 [0.31906614] [0.34952435]\n",
    "loss on epoch  160  :  1.80747572139 [0.32295719] [0.34938022]\n",
    "loss on epoch  170  :  1.78739842331 [0.31776914] [0.35356009]\n",
    "loss on epoch  180  :  1.7721317703 [0.30739298] [0.36552322]\n",
    "loss on epoch  190  :  1.75847657356 [0.31647211] [0.37820697]\n",
    "loss on epoch  200  :  1.7453189426 [0.30869001] [0.39607957]\n",
    "loss on epoch  210  :  1.73079885542 [0.31128404] [0.42490631]\n",
    "loss on epoch  220  :  1.71684144493 [0.31387809] [0.4517152]\n",
    "loss on epoch  230  :  1.6998641215 [0.30090791] [0.47982126]\n",
    "loss on epoch  240  :  1.68529631142 [0.29701686] [0.51052177]\n",
    "loss on epoch  250  :  1.66939626193 [0.28664073] [0.54151052]\n",
    "loss on epoch  260  :  1.65720480791 [0.28015563] [0.56658977]\n",
    "loss on epoch \n",
    "\n",
    "0.055 0.2\n",
    "loss on epoch  0  :  2405.1642829 [0.15434501] [0.16402422]\n",
    "loss on epoch  10  :  1822.05523569 [0.25680932] [0.23551455]\n",
    "loss on epoch  20  :  1206.30413084 [0.25032425] [0.26044971]\n",
    "loss on epoch  30  :  727.089557224 [0.2619974] [0.24704526]\n",
    "loss on epoch  40  :  403.501825827 [0.24124514] [0.23205535]\n",
    "loss on epoch  50  :  206.482957063 [0.27107653] [0.23998271]\n",
    "loss on epoch  60  :  97.5921313674 [0.27885863] [0.24156818]\n",
    "loss on epoch  70  :  42.7869045116 [0.26848248] [0.25108099]\n",
    "loss on epoch  80  :  17.7319767475 [0.26848248] [0.27284521]\n",
    "loss on epoch  90  :  7.41176229936 [0.28015563] [0.28682616]\n",
    "loss on epoch  100  :  3.63745385519 [0.30479896] [0.30023062]\n",
    "loss on epoch  110  :  2.40975147375 [0.31128404] [0.31046411]\n",
    "loss on epoch  120  :  2.02496744968 [0.31128404] [0.31925628]\n",
    "loss on epoch  130  :  1.89303689698 [0.31906614] [0.32300374]\n",
    "loss on epoch  140  :  1.83951033983 [0.31128404] [0.3253099]\n",
    "loss on epoch  150  :  1.8114690295 [0.31906614] [0.33107525]\n",
    "loss on epoch  160  :  1.79382089277 [0.30998704] [0.3326607]\n",
    "loss on epoch  170  :  1.78107121476 [0.30739298] [0.34145287]\n",
    "loss on epoch  180  :  1.7699627512 [0.31387809] [0.34275007]\n",
    "loss on epoch  190  :  1.76237651982 [0.31647211] [0.34260595]\n",
    "loss on epoch  200  :  1.75571231323 [0.32036316] [0.343759]\n",
    "loss on epoch  210  :  1.74952059598 [0.31647211] [0.35082155]\n",
    "loss on epoch  220  :  1.7445356316 [0.32295719] [0.35240704]\n",
    "\n",
    "0.055 0.1\n",
    "loss on epoch  0  :  1214.3227816 [0.18158236] [0.19731911]\n",
    "loss on epoch  10  :  883.076656483 [0.24124514] [0.24819833]\n",
    "loss on epoch  20  :  547.064853527 [0.2464332] [0.25540501]\n",
    "loss on epoch  30  :  300.342731476 [0.2464332] [0.26722398]\n",
    "loss on epoch  40  :  147.775008308 [0.25551233] [0.27385414]\n",
    "loss on epoch  50  :  66.0783908632 [0.26329443] [0.27745748]\n",
    "loss on epoch  60  :  27.6277261134 [0.26459143] [0.27486306]\n",
    "loss on epoch  70  :  11.4821305098 [0.29053178] [0.27875468]\n",
    "loss on epoch  80  :  5.25120859897 [0.30090791] [0.28913233]\n",
    "loss on epoch  90  :  2.95559102407 [0.30869001] [0.31118479]\n",
    "loss on epoch  100  :  2.20472129738 [0.31776914] [0.32920149]\n",
    "loss on epoch  110  :  1.96927175202 [0.30479896] [0.33900261]\n",
    "loss on epoch  120  :  1.88147748841 [0.31128404] [0.3483713]\n",
    "loss on epoch  130  :  1.83761433043 [0.30998704] [0.35701931]\n",
    "loss on epoch  140  :  1.80635545817 [0.30739298] [0.37042376]\n",
    "loss on epoch  150  :  1.77693002367 [0.30869001] [0.39391756]\n",
    "loss on epoch  160  :  1.74524878186 [0.29701686] [0.41784376]\n",
    "loss on epoch  170  :  1.70953490557 [0.28274968] [0.45358893]\n",
    "loss on epoch  180  :  1.67064969518 [0.28404668] [0.49899107]\n",
    "loss on epoch  190  :  1.63381898017 [0.29961088] [0.54597867]\n",
    "loss on epoch  200  :  1.601610325 [0.27107653] [0.58633614]\n",
    "loss on epoch \n",
    "\n",
    "\n",
    "0.055 0.3\n",
    "loss on epoch  0  :  3615.92828143 [0.20622568] [0.1925627]\n",
    "loss on epoch  10  :  2570.61795496 [0.25032425] [0.24517152]\n",
    "loss on epoch  20  :  1554.36551073 [0.23216602] [0.23263188]\n",
    "loss on epoch  30  :  836.849906356 [0.22568093] [0.23767656]\n",
    "loss on epoch  40  :  403.61595832 [0.29571983] [0.2528106]\n",
    "loss on epoch  50  :  173.99371896 [0.27626458] [0.24920726]\n",
    "loss on epoch  60  :  66.8145278295 [0.29961088] [0.25583741]\n",
    "loss on epoch  70  :  23.1336858714 [0.29701686] [0.26160276]\n",
    "loss on epoch  80  :  7.77529838792 [0.29571983] [0.26751226]\n",
    "loss on epoch  90  :  3.21875131572 [0.25551233] [0.24589218]\n",
    "loss on epoch  100  :  2.10442849663 [0.25551233] [0.24589218]\n",
    "loss on epoch  110  :  1.87542454788 [0.25551233] [0.24589218]\n",
    "loss on epoch  120  :  1.82239278047 [0.25551233] [0.24589218]\n",
    "loss on epoch  130  :  1.78458884524 [0.25551233] [0.24589218]\n",
    "loss on epoch  140  :  1.77022338686 [0.25551233] [0.24589218]\n",
    "loss on epoch  150  :  1.7620690901 [0.25551233] [0.24589218]\n",
    "loss on epoch  160  :  1.75576829027 [0.25551233] [0.24589218]\n",
    "loss on epoch  170  :  1.75212164278 [0.25551233] [0.24589218]\n",
    "loss on epoch  180  :  1.74801673105 [0.25551233] [0.24589218]\n",
    "loss on epoch  190  :  1.74692355317 [0.25551233] [0.24589218]\n",
    "loss on epoch  200  :  1.74399915834 [0.25551233] [0.24589218]\n",
    "loss on epoch  210  :  1.74284921587 [0.25551233] [0.24589218]\n",
    "loss on epoch  220  :  1.7411341038 [0.25551233] [0.24589218]\n",
    "loss on epoch  230  :  1.73962150956 [0.25551233] [0.24589218]\n",
    "loss on epoch  240  :  1.73871049395 [0.25551233] [0.24589218]\n",
    "loss on epoch  250  :  1.73769390528 [0.25551233] [0.24589218]\n",
    "loss on epoch  260  :  1.73628741006 [0.25551233] [0.24589218]\n",
    "loss on epoch  270  :  1.7356295685 [0.25551233] [0.24589218]\n",
    "\n",
    "\n",
    "[ 64,32,16 ]\n",
    "\n",
    "0.055 0.3\n",
    "\n",
    "loss on epoch  0  :  3538.809611 [0.22438392] [0.21504757]\n",
    "loss on epoch  10  :  2595.15303209 [0.19844358] [0.19674258]\n",
    "loss on epoch  20  :  1622.10589882 [0.26070037] [0.25454021]\n",
    "loss on epoch  30  :  898.301842018 [0.26588845] [0.26549438]\n",
    "loss on epoch  40  :  443.957723688 [0.2697795] [0.26996252]\n",
    "loss on epoch  50  :  196.289967007 [0.2697795] [0.27443066]\n",
    "loss on epoch  60  :  78.9105699327 [0.27496758] [0.28062841]\n",
    "loss on epoch  70  :  30.2949818505 [0.28664073] [0.29057366]\n",
    "loss on epoch  80  :  12.5827726744 [0.30998704] [0.29835686]\n",
    "loss on epoch  90  :  6.71447044611 [0.30998704] [0.30080715]\n",
    "loss on epoch  100  :  4.66515111482 [0.32166019] [0.29806861]\n",
    "loss on epoch  110  :  3.67433682526 [0.32166019] [0.29605073]\n",
    "loss on epoch  120  :  2.96085031938 [0.30998704] [0.30268088]\n",
    "loss on epoch  130  :  2.41947878842 [0.32036316] [0.31392333]\n",
    "loss on epoch  140  :  2.07686446276 [0.32295719] [0.32040933]\n",
    "loss on epoch  150  :  1.90246647044 [0.32555124] [0.32545403]\n",
    "loss on epoch  160  :  1.82202365553 [0.32036316] [0.3240127]\n",
    "loss on epoch  170  :  1.78665670753 [0.32036316] [0.32891324]\n",
    "loss on epoch  180  :  1.76989234432 [0.32555124] [0.32761604]\n",
    "loss on epoch  190  :  1.76004665704 [0.31647211] [0.32948977]\n",
    "loss on epoch  200  :  1.75389075997 [0.32425421] [0.32963389]\n",
    "loss on epoch  210  :  1.74937579588 [0.32555124] [0.32876909]\n",
    "loss on epoch  220  :  1.74488390209 [0.31906614] [0.33121938]\n",
    "loss on epoch  230  :  1.74217996388 [0.32166019] [0.3304987]\n",
    "loss on epoch  240  :  1.73865111503 [0.31517509] [0.3304987]\n",
    "loss on epoch  250  :  1.73666188066 [0.31776914] [0.33107525]\n",
    "loss on epoch  260  :  1.7345862074 [0.32555124] [0.33208418]\n",
    "loss on epoch  270  :  1.73225748318 [0.32425421] [0.33294898]\n",
    "loss on epoch  280  :  1.72988482482 [0.32555124] [0.33323723]\n",
    "loss on epoch  290  :  1.72851397428 [0.31906614] [0.3368406]\n",
    "loss on epoch  300  :  1.72767322538 [0.31517509] [0.33885846]\n",
    "loss on epoch  310  :  1.72534208883 [0.31776914] [0.34029979]\n",
    "\n",
    "0.045 0.3\n",
    "\n",
    "loss on epoch  0  :  3527.2062457 [0.16342412] [0.17425771]\n",
    "loss on epoch  10  :  2692.3897106 [0.21530479] [0.23983857]\n",
    "loss on epoch  20  :  1796.63398347 [0.25940338] [0.24070337]\n",
    "loss on epoch  30  :  1089.10582818 [0.25940338] [0.25929663]\n",
    "loss on epoch  40  :  606.953802321 [0.26329443] [0.26722398]\n",
    "loss on epoch  50  :  311.657185166 [0.27496758] [0.28365523]\n",
    "loss on epoch  60  :  147.537786978 [0.25810635] [0.26376477]\n",
    "loss on epoch  70  :  64.825095318 [0.27107653] [0.26088211]\n",
    "loss on epoch  80  :  26.9724987613 [0.26848248] [0.25987315]\n",
    "loss on epoch  90  :  11.2345146471 [0.25291827] [0.24791007]\n",
    "loss on epoch  100  :  5.22163809891 [0.25162128] [0.24776593]\n",
    "loss on epoch  110  :  3.08083092504 [0.25810635] [0.25295475]\n",
    "loss on epoch  120  :  2.34396255127 [0.26329443] [0.2541078]\n",
    "loss on epoch  130  :  2.06469206622 [0.30998704] [0.28019601]\n",
    "loss on epoch  140  :  1.93668902803 [0.31517509] [0.28423178]\n",
    "loss on epoch  150  :  1.8661926019 [0.30998704] [0.28552896]\n",
    "loss on epoch  160  :  1.82431523943 [0.31387809] [0.28624964]\n",
    "loss on epoch  170  :  1.79818637559 [0.31647211] [0.28452003]\n",
    "loss on epoch  180  :  1.78158993081 [0.30998704] [0.28639376]\n",
    "loss on epoch  190  :  1.77237643522 [0.31128404] [0.28783512]\n",
    "loss on epoch  200  :  1.76762638821 [0.31517509] [0.28697032]\n",
    "loss on epoch  210  :  1.76267459106 [0.31647211] [0.28956473]\n",
    "loss on epoch  220  :  1.76056111521 [0.31776914] [0.29129431]\n",
    "loss on epoch  230  :  1.75803216022 [0.31906614] [0.29518592]\n",
    "loss on epoch  240  :  1.75444278287 [0.31776914] [0.29605073]\n",
    "loss on epoch  250  :  1.751945553 [0.31906614] [0.29619488]\n",
    "loss on epoch  260  :  1.75040145715 [0.32036316] [0.29749209]\n",
    "loss on epoch  270  :  1.74882869698 [0.31906614] [0.30123955]\n",
    "loss on epoch  280  :  1.74721703154 [0.31906614] [0.30282503]\n",
    "loss on epoch  290  :  1.74496475635 [0.32036316] [0.30599597]\n",
    "loss on epoch  300  :  1.74315591046 [0.32295719] [0.3078697]\n",
    "loss on epoch  310  :  1.74128439967 [0.32814527] [0.30988759]\n",
    "loss on epoch  320  :  1.73928736316 [0.32295719] [0.31046411]\n",
    "loss on epoch  330  :  1.73696815085 [0.32555124] [0.31291437]\n",
    "loss on epoch  340  :  1.73604475679 [0.32425421] [0.31622946]\n",
    "loss on epoch  350  :  1.73450473117 [0.32425421] [0.31752667]\n",
    "loss on epoch  360  :  1.73281933864 [0.32684824] [0.31824735]\n",
    "loss on epoch  370  :  1.73161647386 [0.32814527] [0.3202652]\n",
    "loss on epoch  380  :  1.73021129215 [0.32684824] [0.32199481]\n",
    "loss on epoch  390  :  1.72862869887 [0.32555124] [0.32329202]\n",
    "loss on epoch  400  :  1.72775928069 [0.32425421] [0.32502162]\n",
    "\n",
    "0.065 0.25\n",
    "\n",
    "loss on epoch  0  :  2921.43431487 [0.15434501] [0.1729605]\n",
    "loss on epoch  10  :  1991.26863494 [0.21660182] [0.23277602]\n",
    "loss on epoch  20  :  1114.40263423 [0.25940338] [0.25165755]\n",
    "loss on epoch  30  :  537.532615096 [0.23216602] [0.24963966]\n",
    "loss on epoch  40  :  225.796810857 [0.26588845] [0.25454021]\n",
    "loss on epoch  50  :  82.8712526604 [0.25680932] [0.24675699]\n",
    "loss on epoch  60  :  27.1193968367 [0.24513619] [0.24401845]\n",
    "loss on epoch  70  :  8.71390389955 [0.23605707] [0.2421447]\n",
    "loss on epoch  80  :  3.54491505689 [0.23994812] [0.24099164]\n",
    "loss on epoch  90  :  2.26177679168 [0.23605707] [0.23897377]\n",
    "loss on epoch  100  :  1.94379551654 [0.23605707] [0.2373883]\n",
    "loss on epoch  110  :  1.84164449352 [0.24254215] [0.24084751]\n",
    "loss on epoch  120  :  1.80054399206 [0.30220494] [0.27039492]\n",
    "loss on epoch  130  :  1.78005009448 [0.31258106] [0.28019601]\n",
    "loss on epoch  140  :  1.76697096725 [0.31517509] [0.28322282]\n",
    "loss on epoch  150  :  1.75738607402 [0.31128404] [0.29360047]\n",
    "loss on epoch  160  :  1.7507715297 [0.30998704] [0.30152783]\n",
    "loss on epoch  170  :  1.74384902694 [0.31387809] [0.30887863]\n",
    "loss on epoch  180  :  1.73778158537 [0.30739298] [0.31406745]\n",
    "loss on epoch  190  :  1.73310478749 [0.31647211] [0.32372442]\n",
    "loss on epoch  200  :  1.72765880326 [0.32425421] [0.32992217]\n",
    "loss on epoch  210  :  1.72318710663 [0.32425421] [0.33338138]\n",
    "loss on epoch  220  :  1.71833285137 [0.32555124] [0.337273]\n",
    "loss on epoch  230  :  1.71249576189 [0.32295719] [0.34347075]\n",
    "loss on epoch  240  :  1.70597554854 [0.31906614] [0.35370424]\n",
    "loss on epoch  250  :  1.69754237157 [0.31776914] [0.36292881]\n",
    "loss on epoch  260  :  1.68777828581 [0.32166019] [0.37748632]\n",
    "loss on epoch  270  :  1.67723594882 [0.31258106] [0.38988182]\n",
    "loss on epoch  280  :  1.66504802969 [0.29312581] [0.41395214]\n",
    "loss on epoch  290  :  1.65263791548 [0.29442284] [0.42706832]\n",
    "loss on epoch  300  :  1.64188275017 [0.29053178] [0.44854426]\n",
    "loss on epoch  310  :  1.63121555635 [0.28404668] [0.47492072]\n",
    "loss on epoch  320  :  1.61832285479 [0.28664073] [0.48501009]\n",
    "loss on epoch  330  :  1.61033396864 [0.29961088] [0.5197463]\n",
    "loss on epoch  340  :  1.60363788296 [0.27885863] [0.52536756]\n",
    "loss on epoch  350  :  1.59526930584 [0.27885863] [0.54050159]\n",
    "loss on epoch  360  :  1.59068557951 [0.28145266] [0.58013839]\n",
    "loss on epoch  370  :  1.5861019448 [0.28015563] [0.56586915]\n",
    "loss on epoch  380  :  1.58222953644 [0.27626458] [0.5841741]\n",
    "loss on epoch  390  :  1.57821790819 [0.28534371] [0.58820987]\n",
    "loss on epoch  400  :  1.57750490087 [0.2775616] [0.60910928]\n",
    "loss on epoch  410  :  1.57443232724 [0.28793773] [0.61184782]\n",
    "loss on epoch  420  :  1.57079277381 [0.28923476] [0.61083883]\n",
    "loss on epoch  430  :  1.56823886048 [0.2775616] [0.62337852]\n",
    "loss on epoch  440  :  1.56650043858 [0.29312581] [0.6264053]\n",
    "loss on epoch  450  :  1.56567399369 [0.29053178] [0.62784666]\n",
    "loss on epoch  460  :  1.56381078285 [0.28923476] [0.62438744]\n",
    "loss on epoch  470  :  1.56204192672 [0.28274968] [0.62899971]\n",
    "loss on epoch  480  :  1.56032571473 [0.2775616] [0.63274717]\n",
    "loss on epoch  490  :  1.56006186593 [0.29053178] [0.63087344]\n",
    "loss on epoch  500  :  1.55774901145 [0.29182878] [0.62799078]\n",
    "\n",
    "    \n",
    "0.045 0.15    \n",
    "\n",
    "loss on epoch  0  :  1767.57469573 [0.16212711] [0.16488902]\n",
    "loss on epoch  10  :  1372.31559471 [0.23735408] [0.23637936]\n",
    "loss on epoch  20  :  931.9710764 [0.25291827] [0.24084751]\n",
    "loss on epoch  30  :  572.029821325 [0.24773023] [0.24315365]\n",
    "loss on epoch  40  :  321.629333072 [0.26718548] [0.24618046]\n",
    "loss on epoch  50  :  166.821157526 [0.27367055] [0.26030555]\n",
    "loss on epoch  60  :  80.7114824366 [0.26329443] [0.26693571]\n",
    "loss on epoch  70  :  37.1020909062 [0.26329443] [0.26145864]\n",
    "loss on epoch  80  :  16.9038463434 [0.27626458] [0.2647737]\n",
    "loss on epoch  90  :  8.34884393657 [0.30220494] [0.28956473]\n",
    "loss on epoch  100  :  4.8983892953 [0.30220494] [0.29475352]\n",
    "loss on epoch  110  :  3.41299184936 [0.29961088] [0.30484289]\n",
    "loss on epoch  120  :  2.72366370095 [0.30869001] [0.31003171]\n",
    "loss on epoch  130  :  2.35682953287 [0.30998704] [0.31449986]\n",
    "loss on epoch  140  :  2.14423345195 [0.32166019] [0.32415682]\n",
    "loss on epoch  150  :  2.00732905832 [0.31517509] [0.3326607]\n",
    "loss on epoch  160  :  1.91791861973 [0.30609599] [0.337273]\n",
    "loss on epoch  170  :  1.856142591 [0.30350193] [0.3488037]\n",
    "loss on epoch  180  :  1.81220842677 [0.30609599] [0.35932547]\n",
    "loss on epoch  190  :  1.77689304341 [0.30998704] [0.37287402]\n",
    "loss on epoch  200  :  1.74662932367 [0.31258106] [0.38930526]\n",
    "loss on epoch  210  :  1.71966821673 [0.30220494] [0.41106948]\n",
    "loss on epoch  220  :  1.69432809949 [0.30739298] [0.43528393]\n",
    "loss on epoch  230  :  1.66779694679 [0.28793773] [0.4586336]\n",
    "loss on epoch  240  :  1.64370422893 [0.28923476] [0.49149612]\n",
    "loss on epoch  250  :  1.61674487922 [0.28145266] [0.52176422]\n",
    "loss on epoch  260  :  1.5938981192 [0.29053178] [0.55232054]\n",
    "loss on epoch  270  :  1.57478572152 [0.28923476] [0.57336408]\n",
    "loss on epoch  280  :  1.55752837492 [0.28274968] [0.58316517]\n",
    "    \n",
    "[ 128,64,16 ]\n",
    "0.065 0.25\n",
    "loss on epoch  0  :  2977.87608959 [0.1919585] [0.15854713]\n",
    "loss on epoch  10  :  2019.57307943 [0.23605707] [0.23854136]\n",
    "loss on epoch  20  :  1134.16836096 [0.23735408] [0.25612569]\n",
    "loss on epoch  30  :  555.670838815 [0.23994812] [0.26059383]\n",
    "loss on epoch  40  :  240.495475345 [0.25680932] [0.26621506]\n",
    "loss on epoch  50  :  94.1048941082 [0.26459143] [0.27226865]\n",
    "loss on epoch  60  :  35.755759822 [0.2697795] [0.28826752]\n",
    "loss on epoch  70  :  14.8548342254 [0.26848248] [0.29734793]\n",
    "loss on epoch  80  :  7.34670079196 [0.29571983] [0.30210435]\n",
    "loss on epoch  90  :  4.27060962386 [0.26718548] [0.29994234]\n",
    "loss on epoch  100  :  2.92792832631 [0.25680932] [0.28106081]\n",
    "loss on epoch  110  :  2.28485731284 [0.24254215] [0.25886422]\n",
    "loss on epoch  120  :  2.00470601519 [0.24124514] [0.2493514]\n",
    "loss on epoch  130  :  1.89255996931 [0.23346303] [0.24517152]\n",
    "loss on epoch  140  :  1.83380711134 [0.27885863] [0.26506197]\n",
    "loss on epoch  150  :  1.78784273896 [0.28923476] [0.29432112]\n",
    "loss on epoch  160  :  1.74635697349 [0.29442284] [0.32790428]\n",
    "loss on epoch  170  :  1.70921373754 [0.29053178] [0.37100029]\n",
    "loss on epoch  180  :  1.67862620012 [0.28664073] [0.39881811]\n",
    "loss on epoch  190  :  1.6560735686 [0.29053178] [0.41856444]\n",
    "loss on epoch  200  :  1.63647926019 [0.28923476] [0.43052754]\n",
    "loss on epoch  210  :  1.62218755097 [0.29701686] [0.44292304]\n",
    "loss on epoch  220  :  1.60957927947 [0.30998704] [0.44984144]\n",
    "loss on epoch  230  :  1.59824935319 [0.30090791] [0.45690399]\n",
    "loss on epoch  240  :  1.58998712345 [0.29442284] [0.46612856]\n",
    "loss on epoch  250  :  1.58220509743 [0.29182878] [0.47290286]\n",
    "loss on epoch  260  :  1.57375625273 [0.28274968] [0.47636205]\n",
    "\n",
    "0.045 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wide, embedding, co-occurrence and feature interaction\n",
    "# version 3: embedding + interaction\n",
    "\n",
    "class InterNN_fuse():\n",
    "    \n",
    "    def __init__(self, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, session,n_embedding, n_hidden_list, lr, l2, batch_size):\n",
    "        \n",
    "#       build the network graph \n",
    "        self.LEARNING_RATE = lr\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC  = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.N_EMBED = n_embedding\n",
    "        self.N_BATCH = batch_size\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list)\n",
    "        self.N_TOTAL = self.N_CONTI+ self.N_DISC*self.N_EMBED\n",
    "        self.N_TOTAL_EMBED = self.N_DISC*self.N_EMBED\n",
    "\n",
    "        self.sess = session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y  = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "#       wide part on categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "    \n",
    "    \n",
    "#         for i in range(self.N_DISC):\n",
    "            \n",
    "#             with tf.variable_scope(\"wide\"+str(i)):\n",
    "#                 w= tf.Variable(\\\n",
    "#                 tf.random_uniform([self.N_DISC_VOCA[i], self.N_CLASS],-1.0, 1.0) )\n",
    "# #                         tf.random_normal([self.N_DISC_VOCA, self.N_CLASS],\\\n",
    "# #                         stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "#                 if i==0:\n",
    "#                     dx_wsum = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                    \n",
    "# #                   L2\n",
    "#                     self.regularizer_wide = tf.nn.l2_loss(w)\n",
    "                    \n",
    "#                 else:\n",
    "#                     dx_wsum = dx_wsum + tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                    \n",
    "# #                   L2  \n",
    "#                     self.regularizer_wide = self.regularizer_wide + tf.nn.l2_loss(w)\n",
    "               \n",
    "    \n",
    "#       co-occurrence of categorical features\n",
    "        for i in range(20):\n",
    "            \n",
    "            tmpvoca1 = self.N_DISC_VOCA[i] \n",
    "            \n",
    "            for j in range(i+1,20):\n",
    "                \n",
    "                tmpvoca2 = self.N_DISC_VOCA[j]\n",
    "                \n",
    "                with tf.variable_scope(\"cooccur\"+str(i)+str(j)):\n",
    "                    \n",
    "                    \n",
    "#                   vector approximate of co-occurrence matrix \n",
    "                    w1 = tf.Variable(tf.random_normal([self.N_CLASS, tmpvoca1, 1],\\\n",
    "                                            stddev=math.sqrt(2.0/float( tmpvoca1 ))) )\n",
    "                    \n",
    "                    w2 = tf.Variable(tf.random_normal([self.N_CLASS, 1, tmpvoca2],\\\n",
    "                                            stddev=math.sqrt(2.0/float( tmpvoca2 ))) )\n",
    "            \n",
    "                    b  = tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "                    \n",
    "                    cooc_mat = []\n",
    "                    for k in range(self.N_CLASS):\n",
    "                        cooc_mat.append( tf.reshape( tf.matmul(w1[k],w2[k]), [-1,1] ) )\n",
    "                        \n",
    "#                   n_class by voca1*voca2  \n",
    "                    cooc_vec = tf.stack(cooc_mat)\n",
    "                    cooc_vec = tf.reshape(cooc_vec, [self.N_CLASS, -1] )\n",
    "                    cooc_vec = tf.transpose(cooc_vec,[1,0])\n",
    "                    \n",
    "#                   build the idx of co-occurrence features\n",
    "                    cooc_idx = self.dx_trans[i] * tmpvoca2 + self.dx_trans[j]\n",
    "    \n",
    "                    if i==0 and j==1:\n",
    "#                       co-occurrence weighted sum\n",
    "                        cooc_wsum  = tf.nn.embedding_lookup( cooc_vec, cooc_idx ) + b\n",
    "#                       L2 \n",
    "                        self.regularizer_cooc   = tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2) \n",
    "                    else:\n",
    "                        cooc_wsum += tf.nn.embedding_lookup( cooc_vec, cooc_idx ) + b\n",
    "#                       L2\n",
    "                        self.regularizer_cooc += (tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2))\n",
    "    \n",
    "#       non-linear on co-occurrence weighted sum\n",
    "        cooc_wsum = tf.nn.relu( cooc_wsum )\n",
    "        \n",
    "        \n",
    "#       embedding of categorical features        \n",
    "        tmp_embeded = []                \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"embedding\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA[i], self.N_EMBED],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "                              \n",
    "                tmp_embeded.append( tf.nn.embedding_lookup( w, self.dx_trans[i] ) )\n",
    "                \n",
    "        dx_embeded = tf.concat(tmp_embeded, 1)\n",
    "        x_concate =  tf.concat( [self.cx,dx_embeded], 1 )\n",
    "        \n",
    "#       feature interaction\n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"interact\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA[i], self.N_CONTI],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "        \n",
    "                if i==0:\n",
    "                    inter_wsum  = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "#                   L2 \n",
    "                    self.regularizer_inter   = tf.nn.l2_loss(w)\n",
    "            \n",
    "                else:\n",
    "                    inter_wsum += tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "#                   L2 \n",
    "                    self.regularizer_inter += tf.nn.l2_loss(w)\n",
    "\n",
    "    \n",
    "#       add noise bias\n",
    "        singleCol_h = tf.reduce_sum( self.cx * inter_wsum, 1   )\n",
    "        tmp_h=[]\n",
    "        for i in range( n_hidden_list[0] ):\n",
    "            with tf.variable_scope(\"noise\"+str( i )):\n",
    "                noise_b = tf.Variable(tf.random_normal([ ],\\\n",
    "                        stddev=math.sqrt(2.0/float(n_hidden_list[0]))))\n",
    "                tmp_h.append( singleCol_h + noise_b ) \n",
    "        \n",
    "        \n",
    "#         tmp_h=[]\n",
    "#         for i in range( n_hidden_list[0] ):\n",
    "#             with tf.variable_scope(\"noise\"+str( i )):\n",
    "#                 noise_b = tf.Variable(tf.random_normal([ self.N_CONTI ],\\\n",
    "#                         stddev=math.sqrt(2.0/float(n_hidden_list[0]))))\n",
    "#                 tmp_h.append( tf.reduce_sum( self.cx * (inter_wsum + noise_b), 1 ) ) \n",
    "        \n",
    "        \n",
    "        tmp_h = tf.stack( tmp_h )\n",
    "        tmp_h = tf.transpose( tmp_h, [1,0])\n",
    "        \n",
    "                 \n",
    "#       interaction hidden layers \n",
    "        with tf.variable_scope(\"inter_h0\"):\n",
    "            \n",
    "            w= tf.Variable(tf.random_normal([self.N_TOTAL, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_TOTAL)))) \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            \n",
    "            h_inter = tf.matmul(x_concate, w)\n",
    "            h_inter = h_inter + tmp_h\n",
    "            h_inter = tf.nn.relu( tf.add( h_inter, b ) )\n",
    "            \n",
    "#           L2   \n",
    "            self.regularizer = tf.nn.l2_loss(w)\n",
    "        \n",
    "#       dropout\n",
    "#         h_inter = tf.nn.dropout(h_inter, self.keep_prob)\n",
    "        \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"inter_h\"+str(i)):\n",
    "                w = tf.Variable(\\\n",
    "                        tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b = tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                h_inter = tf.nn.relu( tf.add( tf.matmul(h_inter, w),b) )\n",
    "        \n",
    "#               L2\n",
    "                self.regularizer += tf.nn.l2_loss(w)\n",
    "        \n",
    "        \n",
    "#       output layer  \n",
    "        h = h_inter\n",
    "        \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w = tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1],\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "            b = tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            \n",
    "            h = tf.matmul(h, w)\n",
    "#             h = tf.add( h, dx_wsum )\n",
    "            h = tf.add( h, cooc_wsum )\n",
    "            h = tf.add( h, b )\n",
    "        \n",
    "            self.logit = h\n",
    "            \n",
    "#           L2  \n",
    "            self.regularizer += tf.nn.l2_loss(w) \n",
    "    \n",
    "#       overall L2\n",
    "        self.regularizer += (self.regularizer_cooc + self.regularizer_inter)\n",
    "#       self.regularizer_wide + \n",
    "         \n",
    "        \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y) \\\n",
    "                                  + self.L2*self.regularizer )\n",
    "        self.optimizer = \\\n",
    "        tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#       tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                       feed_dict={self.dx:dx_batch, \\\n",
    "                                  self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                  self.lr:lr,\\\n",
    "                                  self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "        \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                  self.cx:cx_test, self.y:y_test,\\\n",
    "                                                  self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        return sess.run( [self.tmp1], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch  0  :  2913.48150522 [0.10246433] [0.1140098]\n",
      "loss on epoch  10  :  2008.20185174 [0.2386511] [0.24243298]\n",
      "loss on epoch  20  :  1134.39717102 [0.22827497] [0.25194579]\n",
      "loss on epoch  30  :  552.362633034 [0.23346303] [0.26722398]\n",
      "loss on epoch  40  :  234.95337126 [0.27107653] [0.27947536]\n",
      "loss on epoch  50  :  88.1163170779 [0.27367055] [0.27817816]\n",
      "loss on epoch  60  :  30.3773007658 [0.2697795] [0.28552896]\n",
      "loss on epoch  70  :  10.9335334698 [0.27885863] [0.28653792]\n",
      "loss on epoch  80  :  5.02554154396 [0.29961088] [0.29071778]\n",
      "loss on epoch  90  :  3.08871254215 [0.30609599] [0.29864514]\n",
      "loss on epoch  100  :  2.27907398122 [0.31387809] [0.30570769]\n",
      "loss on epoch  110  :  1.94261367012 [0.31647211] [0.30729318]\n",
      "loss on epoch  120  :  1.83664976502 [0.31517509] [0.3116172]\n",
      "loss on epoch  130  :  1.80286608416 [0.32036316] [0.31291437]\n",
      "loss on epoch  140  :  1.78456078304 [0.32166019] [0.31651774]\n",
      "loss on epoch  150  :  1.77202774253 [0.32555124] [0.31723839]\n",
      "loss on epoch  160  :  1.76455165391 [0.32425421] [0.31911212]\n",
      "loss on epoch  170  :  1.75736147772 [0.32555124] [0.3185356]\n",
      "loss on epoch  180  :  1.75200630117 [0.32944229] [0.318968]\n",
      "loss on epoch  190  :  1.74698198614 [0.33073929] [0.3202652]\n",
      "loss on epoch  200  :  1.74308752296 [0.33463034] [0.32271549]\n",
      "loss on epoch  210  :  1.73852605069 [0.33073929] [0.32358029]\n",
      "loss on epoch  220  :  1.7346023277 [0.33203632] [0.32502162]\n",
      "loss on epoch  230  :  1.73263274464 [0.32684824] [0.32790428]\n",
      "loss on epoch  240  :  1.72958576072 [0.32425421] [0.32992217]\n",
      "loss on epoch  250  :  1.72793793016 [0.32944229] [0.3313635]\n",
      "loss on epoch  260  :  1.72482440207 [0.32425421] [0.33366963]\n",
      "loss on epoch  270  :  1.72227659656 [0.33073929] [0.33583164]\n",
      "loss on epoch  280  :  1.71921039124 [0.32944229] [0.33799365]\n",
      "loss on epoch  290  :  1.71528785593 [0.32425421] [0.34404728]\n",
      "loss on epoch  300  :  1.714146039 [0.32295719] [0.34404728]\n",
      "loss on epoch  310  :  1.71043196155 [0.31776914] [0.3501009]\n",
      "loss on epoch  320  :  1.70810530693 [0.32036316] [0.35312769]\n",
      "loss on epoch  330  :  1.70438952358 [0.31647211] [0.35903719]\n",
      "loss on epoch  340  :  1.70194292565 [0.31387809] [0.36379361]\n",
      "loss on epoch  350  :  1.69843165135 [0.31387809] [0.3727299]\n",
      "loss on epoch  360  :  1.69497856829 [0.30869001] [0.37964833]\n",
      "loss on epoch  370  :  1.69090846181 [0.31258106] [0.39218795]\n",
      "loss on epoch  380  :  1.68682802496 [0.30350193] [0.40040359]\n",
      "loss on epoch  390  :  1.68294353783 [0.30609599] [0.40789852]\n",
      "loss on epoch  400  :  1.67946407309 [0.31517509] [0.42043817]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 1118, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 300, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 1044, in getinnerframes\n",
      "    framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python2.7/inspect.py\", line 1004, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 454, in getsourcefile\n",
      "    if hasattr(getmodule(object, filename), '__loader__'):\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 490, in getmodule\n",
      "    for modname, module in sys.modules.items():\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unfortunately, your original traceback can not be constructed.\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[1;34m(self, code_obj, result)\u001b[0m\n\u001b[0;32m   2900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2902\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2903\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2904\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[0;32m   1828\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1829\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[1;32m-> 1830\u001b[1;33m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[0;32m   1831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1832\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1390\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[1;32m-> 1392\u001b[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1298\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[1;32m-> 1300\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1301\u001b[0m             )\n\u001b[0;32m   1302\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 \u001b[0mstructured_traceback_parts\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m             \u001b[0mstructured_traceback_parts\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstructured_traceback_parts\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "\n",
    "#   representation ability\n",
    "para_n_hidden_list = [ 32,32,16 ]\n",
    "#     128,16,8\n",
    "para_lr = 0.065\n",
    "para_n_embedding = 2\n",
    "#   regularization\n",
    "para_batch_size = 32\n",
    "para_keep_prob = 0.8\n",
    "para_l2 = 0.25\n",
    "\n",
    "# fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = [8]*para_n_disc\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 10\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    clf = InterNN_fuse( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                      para_n_disc_voca, sess, para_n_embedding, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2, para_batch_size)\n",
    "    clf.train_ini()\n",
    "    clf.inference_ini()\n",
    "    \n",
    "    para_cur_lr=para_lr\n",
    "    \n",
    "    total_cnt= np.shape(cxtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "    \n",
    "    total_idx=range(total_cnt)\n",
    "    \n",
    "#   training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc=0.0\n",
    "        \n",
    "#       shuffle traning instances each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "    \n",
    "#       Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                     para_cur_lr )\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "    \n",
    "        tmp_test_acc  = clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[ 128,64,16 ]\n",
    "0.06 0.35\n",
    "\n",
    "0.05, 0.35\n",
    "loss on epoch  0  :  4112.37932897 [0.1608301] [0.13995388]\n",
    "loss on epoch  10  :  3059.09017605 [0.25680932] [0.23681176]\n",
    "loss on epoch  20  :  1960.28582029 [0.25810635] [0.25295475]\n",
    "loss on epoch  30  :  1124.10197449 [0.28145266] [0.2711156]\n",
    "loss on epoch  40  :  583.568243804 [0.30998704] [0.27313346]\n",
    "loss on epoch  50  :  274.963851929 [0.27367055] [0.27990776]\n",
    "loss on epoch  60  :  117.810368503 [0.27496758] [0.27385414]\n",
    "loss on epoch  70  :  46.1854696627 [0.28534371] [0.28077257]\n",
    "loss on epoch  80  :  16.9772326946 [0.29053178] [0.28942057]\n",
    "loss on epoch  90  :  6.35192130451 [0.29831389] [0.30138367]\n",
    "loss on epoch  100  :  3.05701557464 [0.30869001] [0.31392333]\n",
    "loss on epoch  110  :  2.1787649371 [0.31776914] [0.31954452]\n",
    "loss on epoch  120  :  1.93165977023 [0.31647211] [0.32718363]\n",
    "loss on epoch  130  :  1.84372538659 [0.31906614] [0.32992217]\n",
    "loss on epoch  140  :  1.80727599948 [0.31258106] [0.33395791]\n",
    "loss on epoch  150  :  1.78846734707 [0.31128404] [0.33611992]\n",
    "loss on epoch  160  :  1.77639710903 [0.31647211] [0.33568752]\n",
    "loss on epoch  170  :  1.76785224787 [0.31517509] [0.33900261]\n",
    "loss on epoch  180  :  1.76091153037 [0.31906614] [0.33972326]\n",
    "loss on epoch  190  :  1.7558026805 [0.31387809] [0.34361488]\n",
    "loss on epoch  200  :  1.75204727881 [0.31258106] [0.34592101]\n",
    "loss on epoch  210  :  1.74816672449 [0.31387809] [0.34779474]\n",
    "loss on epoch  220  :  1.74480803035 [0.31387809] [0.35240704]\n",
    "loss on epoch  230  :  1.74340877875 [0.31517509] [0.35298356]\n",
    "loss on epoch  240  :  1.74105864929 [0.31906614] [0.3561545]\n",
    "loss on epoch  250  :  1.73833712273 [0.31906614] [0.3557221]\n",
    "loss on epoch  260  :  1.73768513843 [0.31517509] [0.35428077]\n",
    "loss on epoch  270  :  1.736201368 [0.31647211] [0.35557798]\n",
    "loss on epoch  280  :  1.73401567285 [0.31258106] [0.3561545]\n",
    "loss on epoch  290  :  1.731798406 [0.31387809] [0.36292881]\n",
    "loss on epoch  300  :  1.72962650546 [0.32036316] [0.36091092]\n",
    "loss on epoch  310  :  1.72767726249 [0.32036316] [0.3672528]\n",
    "loss on epoch  320  :  1.72687107545 [0.31517509] [0.3676852]\n",
    "loss on epoch  330  :  1.72526504706 [0.31258106] [0.36869416]\n",
    "loss on epoch  340  :  1.72467513493 [0.31128404] [0.36999136]\n",
    "loss on epoch  350  :  1.72244894118 [0.31128404] [0.37546843]\n",
    "loss on epoch  360  :  1.7203446726 [0.30998704] [0.37561256]\n",
    "loss on epoch  370  :  1.7189220477 [0.32295719] [0.37849525]\n",
    "loss on epoch  380  :  1.71831009951 [0.31387809] [0.37791872]\n",
    "loss on epoch  390  :  1.71590292178 [0.31647211] [0.38209859]\n",
    "loss on epoch  400  :  1.71552342728 [0.31387809] [0.39017007]\n",
    "\n",
    "\n",
    "[ 128,32,16 ]\n",
    "\n",
    "0.05 0.25\n",
    "loss on epoch  0  :  2984.16321818 [0.14526589] [0.14600749]\n",
    "loss on epoch  10  :  2228.49032367 [0.21011673] [0.23493803]\n",
    "loss on epoch  20  :  1435.26218725 [0.22568093] [0.23839723]\n",
    "loss on epoch  30  :  827.165083709 [0.24383917] [0.25756702]\n",
    "loss on epoch  40  :  432.92500616 [0.27626458] [0.2826463]\n",
    "loss on epoch  50  :  206.896535732 [0.28145266] [0.28999713]\n",
    "loss on epoch  60  :  91.1961491903 [0.29182878] [0.294177]\n",
    "loss on epoch  70  :  38.1252215527 [0.29312581] [0.29734793]\n",
    "loss on epoch  80  :  16.1940223685 [0.29571983] [0.29922166]\n",
    "loss on epoch  90  :  7.80226797969 [0.31647211] [0.30945519]\n",
    "loss on epoch  100  :  4.63881570101 [0.32425421] [0.3185356]\n",
    "loss on epoch  110  :  3.2436019403 [0.32036316] [0.32905737]\n",
    "loss on epoch  120  :  2.52279345194 [0.31258106] [0.33972326]\n",
    "loss on epoch  130  :  2.141223509 [0.31387809] [0.34476796]\n",
    "loss on epoch  140  :  1.95561428699 [0.31517509] [0.35802826]\n",
    "loss on epoch  150  :  1.86308251487 [0.31776914] [0.36754107]\n",
    "loss on epoch  160  :  1.81187883516 [0.31258106] [0.37590083]\n",
    "loss on epoch  170  :  1.78083807672 [0.30869001] [0.38887286]\n",
    "loss on epoch  180  :  1.75611915246 [0.31258106] [0.3979533]\n",
    "loss on epoch  190  :  1.73424496474 [0.31517509] [0.41164601]\n",
    "loss on epoch  200  :  1.71701375809 [0.31776914] [0.42332083]\n",
    "loss on epoch  210  :  1.6997012634 [0.30739298] [0.43441916]\n",
    "loss on epoch  220  :  1.68647382436 [0.28145266] [0.43773422]\n",
    "loss on epoch  230  :  1.67338317412 [0.28404668] [0.44767946]\n",
    "loss on epoch  240  :  1.66195621259 [0.27885863] [0.45503026]\n",
    "loss on epoch  250  :  1.65262653982 [0.28664073] [0.46036321]\n",
    "loss on epoch  260  :  1.64566449987 [0.28404668] [0.4628135]\n",
    "loss on epoch  270  :  1.63894850071 [0.29053178] [0.48255983]\n",
    "loss on epoch  280  :  1.6328404325 [0.2775616] [0.47852406]\n",
    "loss on epoch  290  :  1.62758091368 [0.27885863] [0.48385701]\n",
    "loss on epoch  300  :  1.62190970337 [0.28404668] [0.48861343]\n",
    "\n",
    "0.75\n",
    "loss on epoch  0  :  2977.35766489 [0.16990921] [0.14946671]\n",
    "loss on epoch  10  :  2217.23882152 [0.22568093] [0.21230902]\n",
    "loss on epoch  20  :  1420.06968067 [0.23605707] [0.24185644]\n",
    "loss on epoch  30  :  815.662684123 [0.24773023] [0.24646872]\n",
    "loss on epoch  40  :  424.706616154 [0.25551233] [0.25295475]\n",
    "loss on epoch  50  :  201.05387391 [0.25291827] [0.24762179]\n",
    "loss on epoch  60  :  86.85074068 [0.25291827] [0.24848659]\n",
    "loss on epoch  70  :  34.9947918609 [0.25680932] [0.24661286]\n",
    "loss on epoch  80  :  13.9666180081 [0.25551233] [0.24603632]\n",
    "loss on epoch  90  :  6.26568315427 [0.25551233] [0.24589218]\n",
    "loss on epoch  100  :  3.58276599535 [0.31517509] [0.27976361]\n",
    "loss on epoch  110  :  2.52540745007 [0.31258106] [0.28596136]\n",
    "loss on epoch  120  :  2.00819005845 [0.31776914] [0.29691553]\n",
    "loss on epoch  130  :  1.82808638668 [0.31387809] [0.29965407]\n",
    "loss on epoch  140  :  1.77833122843 [0.30998704] [0.30325744]\n",
    "loss on epoch  150  :  1.75744953089 [0.31128404] [0.30844623]\n",
    "loss on epoch  160  :  1.74410498749 [0.31387809] [0.31248197]\n",
    "loss on epoch  170  :  1.73372809147 [0.31387809] [0.318968]\n",
    "loss on epoch  180  :  1.72589197865 [0.31258106] [0.32257134]\n",
    "loss on epoch  190  :  1.71831875101 [0.31128404] [0.32992217]\n",
    "loss on epoch  200  :  1.71030590214 [0.31128404] [0.33655232]\n",
    "loss on epoch  210  :  1.70321574145 [0.31258106] [0.34145287]\n",
    "loss on epoch  220  :  1.69467934249 [0.30220494] [0.34779474]\n",
    "loss on epoch  230  :  1.68479432828 [0.30479896] [0.36523494]\n",
    "loss on epoch  240  :  1.67157381994 [0.29442284] [0.38166618]\n",
    "loss on epoch  250  :  1.6562432128 [0.30220494] [0.40386277]\n",
    "loss on epoch  260  :  1.64067010195 [0.28793773] [0.42288843]\n",
    "loss on epoch  270  :  1.6255188049 [0.28404668] [0.43946382]\n",
    "loss on epoch  280  :  1.61254157347 [0.28274968] [0.45675987]\n",
    "loss on epoch  290  :  1.6003067085 [0.27626458] [0.48054194]\n",
    "loss on epoch  300  :  1.58969467106 [0.28664073] [0.50446814]\n",
    "loss on epoch  310  :  1.58066189786 [0.28534371] [0.5174402]\n",
    "loss on epoch  320  :  1.57223566704 [0.27885863] [0.53343904]\n",
    "loss on epoch  330  :  1.5658605165 [0.28145266] [0.55736524]\n",
    "loss on epoch  340  :  1.56251320188 [0.27237353] [0.58475065]\n",
    "loss on epoch  350  :  1.55729498907 [0.27107653] [0.58878642]\n",
    "loss on epoch  360  :  1.55314406201 [0.27367055] [0.59613723]\n",
    "loss on epoch  370  :  1.54791894114 [0.26848248] [0.60348803]\n",
    "loss on epoch  380  :  1.54467611125 [0.27107653] [0.61098301]\n",
    "loss on epoch  390  :  1.54223347428 [0.26070037] [0.61804557]\n",
    "loss on epoch  400  :  1.53922743323 [0.26718548] [0.62063998]\n",
    "loss on epoch  410  :  1.53627740564 [0.27237353] [0.62654942]\n",
    "loss on epoch  420  :  1.53325057526 [0.27367055] [0.62568462]\n",
    "loss on epoch  430  :  1.53148831703 [0.25940338] [0.62741423]\n",
    "loss on epoch  440  :  1.53023800419 [0.28274968] [0.62251371]\n",
    "loss on epoch  450  :  1.5295211728 [0.26718548] [0.63245893]\n",
    "loss on epoch  460  :  1.52626908101 [0.2619974] [0.63390028]\n",
    "loss on epoch  470  :  1.52444877393 [0.26070037] [0.63562989]\n",
    "loss on epoch  480  :  1.52219186999 [0.27107653] [0.63851255]\n",
    "loss on epoch  490  :  1.52172864863 [0.26848248] [0.637936]\n",
    "loss on epoch  500  :  1.52001977464 [0.26848248] [0.63851255]\n",
    "loss on epoch  510  :  1.51810660462 [0.27626458] [0.62972039]\n",
    "loss on epoch  520  :  1.51779708266 [0.27367055] [0.64053041]\n",
    "loss on epoch  530  :  1.5144958899 [0.26459143] [0.6396656]\n",
    "loss on epoch  540  :  1.51424664811 [0.26718548] [0.63735944]\n",
    "\n",
    "[ 32,32,16 ]: #neurons, regularization\n",
    "\n",
    "0.75 0.25   \n",
    "\n",
    "loss on epoch  0  :  2937.00183784 [0.13748379] [0.16431248]\n",
    "loss on epoch  10  :  1980.56166077 [0.23476005] [0.22686653]\n",
    "loss on epoch  20  :  1060.14993936 [0.22178988] [0.23133467]\n",
    "loss on epoch  30  :  469.47508932 [0.20622568] [0.2382531]\n",
    "loss on epoch  40  :  173.987235387 [0.23994812] [0.24791007]\n",
    "loss on epoch  50  :  54.8401287927 [0.25551233] [0.24661286]\n",
    "loss on epoch  60  :  15.6860910345 [0.25551233] [0.24589218]\n",
    "loss on epoch  70  :  5.05427253909 [0.25551233] [0.24589218]\n",
    "loss on epoch  80  :  2.48936366483 [0.25551233] [0.24589218]\n",
    "loss on epoch  90  :  1.92046676135 [0.25551233] [0.24589218]\n",
    "loss on epoch  100  :  1.81257838894 [0.25551233] [0.24589218]\n",
    "loss on epoch  110  :  1.79125653152 [0.2697795] [0.25626981]\n",
    "loss on epoch  120  :  1.78011440182 [0.2697795] [0.25713462]\n",
    "loss on epoch  130  :  1.77279601826 [0.26459143] [0.25843182]\n",
    "loss on epoch  140  :  1.76641532116 [0.26848248] [0.26535025]\n",
    "loss on epoch  150  :  1.76214704359 [0.26718548] [0.26924187]\n",
    "loss on epoch  160  :  1.75710989204 [0.26848248] [0.27082732]\n",
    "loss on epoch  170  :  1.75269452731 [0.26588845] [0.27443066]\n",
    "loss on epoch  180  :  1.74956474978 [0.31387809] [0.30023062]\n",
    "loss on epoch  190  :  1.74727627469 [0.31517509] [0.30556357]\n",
    "loss on epoch  200  :  1.74427712129 [0.32555124] [0.30758142]\n",
    "loss on epoch  210  :  1.74187214673 [0.31906614] [0.30988759]\n",
    "loss on epoch  220  :  1.73836385597 [0.31776914] [0.30916691]\n",
    "loss on epoch  230  :  1.73654806172 [0.32555124] [0.31377918]\n",
    "loss on epoch  240  :  1.73394887701 [0.32166019] [0.31680599]\n",
    "loss on epoch  250  :  1.7322146302 [0.33073929] [0.31954452]\n",
    "loss on epoch  260  :  1.73029606651 [0.32814527] [0.32127413]\n",
    "loss on epoch  270  :  1.72917312604 [0.33463034] [0.32386854]\n",
    "loss on epoch  280  :  1.72733631344 [0.32425421] [0.32559815]\n",
    "loss on epoch  290  :  1.72618572745 [0.32814527] [0.32603055]\n",
    "loss on epoch  300  :  1.72489549772 [0.32684824] [0.32516575]\n",
    "loss on epoch  310  :  1.72346471581 [0.32555124] [0.32747188]\n",
    "loss on epoch  320  :  1.72210429664 [0.32555124] [0.32703948]\n",
    "loss on epoch  330  :  1.7202126737 [0.32166019] [0.33366963]\n",
    "loss on epoch  340  :  1.7197007432 [0.32166019] [0.33208418]\n",
    "loss on epoch  350  :  1.71754157102 [0.31387809] [0.3377054]\n",
    "loss on epoch  360  :  1.71545168206 [0.31776914] [0.34001154]\n",
    "loss on epoch  370  :  1.7137490522 [0.31387809] [0.34505621]\n",
    "loss on epoch  380  :  1.7121563013 [0.31906614] [0.34779474]\n",
    "loss on epoch  390  :  1.70994089654 [0.31128404] [0.35413665]\n",
    "loss on epoch  400  :  1.70707785145 [0.31647211] [0.3565869]\n",
    "loss on epoch  410  :  1.70359759695 [0.31387809] [0.36710867]\n",
    "loss on epoch  420  :  1.69967268959 [0.31258106] [0.37518016]\n",
    "loss on epoch  430  :  1.69559655421 [0.29961088] [0.38267511]\n",
    "loss on epoch  440  :  1.69131073908 [0.30739298] [0.40458345]\n",
    "loss on epoch  450  :  1.68664100932 [0.29442284] [0.41669068]\n",
    "loss on epoch  460  :  1.6815467957 [0.29961088] [0.43816662]\n",
    "loss on epoch  470  :  1.67764298673 [0.29571983] [0.45575094]\n",
    "    \n",
    "0.65 0.25\n",
    "\n",
    "loss on epoch  0  :  2886.24161219 [0.19455253] [0.20351687]\n",
    "loss on epoch  10  :  1988.59360532 [0.1997406] [0.22383973]\n",
    "loss on epoch  20  :  1128.57974187 [0.24383917] [0.25381956]\n",
    "loss on epoch  30  :  552.771804244 [0.25551233] [0.26535025]\n",
    "loss on epoch  40  :  237.863297639 [0.26588845] [0.28048429]\n",
    "loss on epoch  50  :  91.2934027071 [0.25810635] [0.27961949]\n",
    "loss on epoch  60  :  32.7627919515 [0.27237353] [0.29143846]\n",
    "loss on epoch  70  :  12.5326693058 [0.28404668] [0.29806861]\n",
    "loss on epoch  80  :  6.06290647719 [0.30609599] [0.31522053]\n",
    "loss on epoch  90  :  3.9064360857 [0.30479896] [0.33309311]\n",
    "loss on epoch  100  :  2.95455121663 [0.30998704] [0.33539924]\n",
    "loss on epoch  110  :  2.42096429291 [0.31906614] [0.34808302]\n",
    "loss on epoch  120  :  2.10320025738 [0.31776914] [0.35154223]\n",
    "loss on epoch  130  :  1.92841164105 [0.31776914] [0.34678581]\n",
    "loss on epoch  140  :  1.83906269681 [0.32036316] [0.34246179]\n",
    "loss on epoch  150  :  1.79554319271 [0.32814527] [0.34058806]\n",
    "loss on epoch  160  :  1.77310961043 [0.32555124] [0.34073219]\n",
    "loss on epoch  170  :  1.76078001603 [0.32555124] [0.33799365]\n",
    "loss on epoch  180  :  1.75251507538 [0.32425421] [0.33885846]\n",
    "loss on epoch  190  :  1.74655083208 [0.31776914] [0.33511099]\n",
    "loss on epoch  200  :  1.74213193688 [0.32425421] [0.33712885]\n",
    "loss on epoch  210  :  1.73741377283 [0.32684824] [0.3368406]\n",
    "loss on epoch  220  :  1.73437432503 [0.32295719] [0.33943498]\n",
    "loss on epoch  230  :  1.73158523275 [0.31776914] [0.34001154]\n",
    "loss on epoch  240  :  1.72955057466 [0.31387809] [0.33986738]\n",
    "loss on epoch  250  :  1.72650755666 [0.31776914] [0.3428942]\n",
    "loss on epoch  260  :  1.72500768745 [0.31647211] [0.34174114]\n",
    "loss on epoch  270  :  1.72295075103 [0.31258106] [0.34303835]\n",
    "loss on epoch  280  :  1.72162079866 [0.31258106] [0.34260595]\n",
    "loss on epoch  290  :  1.71957002415 [0.31906614] [0.34505621]\n",
    "loss on epoch  300  :  1.71809021577 [0.31906614] [0.34505621]\n",
    "loss on epoch  310  :  1.71719058575 [0.32166019] [0.3446238]\n",
    "loss on epoch  320  :  1.71635875051 [0.31776914] [0.3441914]\n",
    "loss on epoch  330  :  1.71525748847 [0.31517509] [0.34520036]\n",
    "loss on epoch  340  :  1.71390254244 [0.31387809] [0.34606513]\n",
    "loss on epoch  350  :  1.71252528109 [0.31387809] [0.34606513]\n",
    "loss on epoch  360  :  1.71242676675 [0.31776914] [0.34851542]\n",
    "loss on epoch  370  :  1.71220593486 [0.31906614] [0.34779474]\n",
    "loss on epoch  380  :  1.71055379841 [0.31776914] [0.35110983]\n",
    "loss on epoch  390  :  1.71012227348 [0.31517509] [0.35110983]\n",
    "loss on epoch  400  :  1.7088378723 [0.31647211] [0.34909195]\n",
    "loss on epoch  410  :  1.70774036867 [0.31776914] [0.35096571]\n",
    "loss on epoch  420  :  1.70759297245 [0.31647211] [0.35211876]\n",
    "loss on epoch  430  :  1.70742613629 [0.31128404] [0.35110983]\n",
    "loss on epoch  440  :  1.70667638602 [0.31387809] [0.35269532]\n",
    "loss on epoch  450  :  1.70597999405 [0.31387809] [0.35154223]\n",
    "loss on epoch  460  :  1.70544632441 [0.30609599] [0.35139811]\n",
    "loss on epoch  470  :  1.70489510232 [0.30739298] [0.35298356]\n",
    "loss on epoch  480  :  1.70388288796 [0.31517509] [0.35312769]\n",
    "loss on epoch  490  :  1.70463823444 [0.30739298] [0.35211876]\n",
    "loss on epoch  500  :  1.70427047489 [0.30869001] [0.35327184]\n",
    "loss on epoch  510  :  1.70289952943 [0.30998704] [0.35471317]\n",
    "loss on epoch  520  :  1.70337097678 [0.31387809] [0.35442489]\n",
    "loss on epoch  530  :  1.70273655598 [0.31647211] [0.3552897]\n",
    "loss on epoch  540  :  1.70253641407 [0.31776914] [0.3548573]\n",
    "loss on epoch  550  :  1.70112308418 [0.31387809] [0.3557221]\n",
    "loss on epoch  560  :  1.70103886889 [0.31258106] [0.3552897]\n",
    "loss on epoch  570  :  1.70150572282 [0.31647211] [0.35586625]\n",
    "loss on epoch  580  :  1.7008207926 [0.30739298] [0.35773998]\n",
    "loss on epoch  590  :  1.6999411307 [0.30869001] [0.35759586]\n",
    "loss on epoch  600  :  1.69985376298 [0.30869001] [0.35817239]\n",
    "loss on epoch  610  :  1.6987061197 [0.31258106] [0.36134332]\n",
    "loss on epoch  620  :  1.69947070601 [0.30869001] [0.36091092]\n",
    "loss on epoch  630  :  1.69928016872 [0.30998704] [0.35990199]\n",
    "loss on epoch  640  :  1.69800751794 [0.31517509] [0.35961372]\n",
    "loss on epoch  650  :  1.698172896 [0.31517509] [0.3616316]\n",
    "loss on epoch  660  :  1.69777368727 [0.31128404] [0.36278465]\n",
    "loss on epoch  670  :  1.69761315982 [0.30350193] [0.3603344]\n",
    "loss on epoch  680  :  1.69764046261 [0.31647211] [0.36408186]\n",
    "loss on epoch  690  :  1.69709036085 [0.30869001] [0.36220813]\n",
    "loss on epoch  700  :  1.69737868232 [0.30739298] [0.36321706]\n",
    "loss on epoch  710  :  1.69611418026 [0.31258106] [0.36408186]\n",
    "loss on epoch  720  :  1.69624953745 [0.30739298] [0.36148745]\n",
    "loss on epoch  730  :  1.69619914613 [0.30739298] [0.36696455]\n",
    "loss on epoch  740  :  1.69532852868 [0.31128404] [0.36393774]\n",
    "loss on epoch  750  :  1.69535909262 [0.30609599] [0.36595562]\n",
    "loss on epoch  760  :  1.69589098515 [0.30998704] [0.36739695]\n",
    "loss on epoch  770  :  1.69526974139 [0.30350193] [0.36754107]\n",
    "loss on epoch  780  :  1.69432959788 [0.31128404] [0.36710867]\n",
    "loss on epoch  790  :  1.69514299819 [0.31128404] [0.36883828]\n",
    "loss on epoch  800  :  1.69410144013 [0.31128404] [0.37013549]\n",
    "loss on epoch  810  :  1.69474113319 [0.30739298] [0.36826175]\n",
    "loss on epoch  820  :  1.6939667811 [0.30479896] [0.36782935]\n",
    "loss on epoch  830  :  1.69408524588 [0.31387809] [0.36883828]\n",
    "loss on epoch  840  :  1.69354515992 [0.31387809] [0.37186509]\n",
    "loss on epoch \n",
    "    \n",
    "0.55 no dropout\n",
    "\n",
    "loss on epoch  0  :  2874.94279197 [0.1608301] [0.16070914]\n",
    "loss on epoch  10  :  2083.56454524 [0.23605707] [0.22902854]\n",
    "loss on epoch  20  :  1281.37241618 [0.25162128] [0.24531566]\n",
    "loss on epoch  30  :  698.150010851 [0.24902724] [0.25036034]\n",
    "loss on epoch  40  :  341.334552765 [0.27107653] [0.2660709]\n",
    "loss on epoch  50  :  150.811489388 [0.28664073] [0.27486306]\n",
    "loss on epoch  60  :  60.9060257982 [0.28015563] [0.28524071]\n",
    "loss on epoch  70  :  23.8454049075 [0.27626458] [0.28610551]\n",
    "loss on epoch  80  :  10.3388724415 [0.28534371] [0.29201499]\n",
    "loss on epoch  90  :  5.56707937188 [0.29053178] [0.29259151]\n",
    "loss on epoch  100  :  3.68248138935 [0.27367055] [0.28855577]\n",
    "loss on epoch  110  :  2.76654098762 [0.25551233] [0.28624964]\n",
    "loss on epoch  120  :  2.27717612849 [0.25291827] [0.27889881]\n",
    "loss on epoch  130  :  2.00933238864 [0.24254215] [0.26347652]\n",
    "loss on epoch  140  :  1.89256767542 [0.24124514] [0.26318824]\n",
    "loss on epoch  150  :  1.84241652875 [0.2386511] [0.25900835]\n",
    "loss on epoch  160  :  1.81760840835 [0.2386511] [0.2523782]\n",
    "loss on epoch  170  :  1.8034652168 [0.24124514] [0.2493514]\n",
    "loss on epoch  180  :  1.79319385191 [0.25032425] [0.24329779]\n",
    "loss on epoch  190  :  1.7860051239 [0.23994812] [0.24430671]\n",
    "loss on epoch  200  :  1.7799572183 [0.23605707] [0.23969443]\n",
    "loss on epoch  210  :  1.77461905777 [0.24773023] [0.24041511]\n",
    "loss on epoch  220  :  1.77011702955 [0.24513619] [0.24430671]\n",
    "loss on epoch  230  :  1.76598942335 [0.25551233] [0.24589218]\n",
    "loss on epoch  240  :  1.76272176316 [0.25551233] [0.24589218]\n",
    "loss on epoch  250  :  1.75966795837 [0.25551233] [0.24589218]\n",
    "loss on epoch  260  :  1.75665102126 [0.25551233] [0.24589218]\n",
    "loss on epoch  270  :  1.75511636006 [0.25551233] [0.24589218]\n",
    "loss on epoch  280  :  1.75301427532 [0.25551233] [0.24589218]\n",
    "loss on epoch  290  :  1.75122346646 [0.25551233] [0.24589218]\n",
    "loss on epoch  300  :  1.74980408229 [0.25551233] [0.24589218]\n",
    "loss on epoch  310  :  1.74853351381 [0.29961088] [0.27515134]\n",
    "loss on epoch  320  :  1.74744045679 [0.30609599] [0.27716923]\n",
    "loss on epoch  330  :  1.74594371849 [0.30869001] [0.28192562]\n",
    "loss on epoch  340  :  1.74501032299 [0.31128404] [0.28452003]\n",
    "loss on epoch  350  :  1.74359217728 [0.31517509] [0.28725857]\n",
    "loss on epoch  360  :  1.74254825822 [0.32166019] [0.28999713]\n",
    "loss on epoch  370  :  1.7404457281 [0.32036316] [0.29388872]\n",
    "loss on epoch  380  :  1.73958398126 [0.32684824] [0.29662728]\n",
    "loss on epoch  390  :  1.73755385368 [0.31776914] [0.29878926]\n",
    "loss on epoch  400  :  1.73575710643 [0.31906614] [0.30196023]\n",
    "loss on epoch  410  :  1.73401501775 [0.32814527] [0.30628422]\n",
    "loss on epoch  420  :  1.73316884703 [0.32425421] [0.31003171]\n",
    "loss on epoch  430  :  1.73088525291 [0.32295719] [0.31406745]\n",
    "loss on epoch  440  :  1.72995901494 [0.33073929] [0.31666186]\n",
    "loss on epoch  450  :  1.72873910599 [0.32814527] [0.31666186]\n",
    "loss on epoch  460  :  1.72727826348 [0.32814527] [0.31579706]\n",
    "loss on epoch  470  :  1.7261250858 [0.32814527] [0.32069761]\n",
    "loss on epoch  480  :  1.7253982938 [0.32684824] [0.32069761]\n",
    "loss on epoch  490  :  1.72430204573 [0.32684824] [0.32213894]\n",
    "loss on epoch  500  :  1.72306088496 [0.32814527] [0.3253099]\n",
    "loss on epoch  510  :  1.72182659418 [0.32944229] [0.32689536]\n",
    "loss on epoch  520  :  1.7212175026 [0.32814527] [0.32703948]\n",
    "loss on epoch  530  :  1.72045994578 [0.33203632] [0.32833669]\n",
    "loss on epoch  540  :  1.71956506096 [0.32555124] [0.33237243]\n",
    "loss on epoch  550  :  1.7187097917 [0.32814527] [0.3322283]\n",
    "loss on epoch  560  :  1.7172617101 [0.32555124] [0.3359758]\n",
    "loss on epoch  570  :  1.71600874927 [0.31258106] [0.33669645]\n",
    "loss on epoch  580  :  1.71367786438 [0.31517509] [0.34015566]\n",
    "loss on epoch  590  :  1.71271143026 [0.32295719] [0.33756125]\n",
    "loss on epoch  600  :  1.71196182734 [0.31387809] [0.34087634]\n",
    "loss on epoch  610  :  1.71009510038 [0.31387809] [0.34808302]\n",
    "loss on epoch  620  :  1.70871090337 [0.30998704] [0.34981263]\n",
    "loss on epoch  630  :  1.70718104806 [0.31776914] [0.35384837]\n",
    "loss on epoch  640  :  1.7056134023 [0.31776914] [0.3552897]\n",
    "loss on epoch  650  :  1.7038799149 [0.31647211] [0.36278465]\n",
    "loss on epoch  660  :  1.70147576045 [0.30998704] [0.36465842]\n",
    "loss on epoch  670  :  1.69909681214 [0.31517509] [0.37013549]\n",
    "loss on epoch  680  :  1.69627010602 [0.31128404] [0.37806284]\n",
    "loss on epoch  690  :  1.69379743547 [0.31387809] [0.38685501]\n",
    "loss on epoch  700  :  1.69017124949 [0.30998704] [0.38901702]\n",
    "loss on epoch  710  :  1.68774709989 [0.31906614] [0.40357453]\n",
    "loss on epoch  720  :  1.68479633607 [0.30998704] [0.40847507]\n",
    "loss on epoch  730  :  1.68057860876 [0.29701686] [0.42476219]\n",
    "loss on epoch  740  :  1.67775793042 [0.29571983] [0.43081579]\n",
    "loss on epoch  750  :  1.67421361583 [0.29312581] [0.44292304]\n",
    "loss on epoch  760  :  1.66988071192 [0.29571983] [0.44854426]\n",
    "\n",
    "\n",
    "    \n",
    "0.45 no dropout\n",
    "\n",
    "loss on epoch  0  :  2904.77315267 [0.19714656] [0.18895936]\n",
    "loss on epoch  10  :  2239.2465583 [0.21141374] [0.21533583]\n",
    "loss on epoch  20  :  1507.24788694 [0.230869] [0.24113578]\n",
    "loss on epoch  30  :  919.092349017 [0.24513619] [0.2541078]\n",
    "loss on epoch  40  :  513.960989917 [0.25551233] [0.25799942]\n",
    "loss on epoch  50  :  264.626757163 [0.2542153] [0.25425196]\n",
    "loss on epoch  60  :  126.197195512 [0.24773023] [0.25309888]\n",
    "loss on epoch  70  :  56.272663452 [0.24773023] [0.25468436]\n",
    "loss on epoch  80  :  24.2569229161 [0.24513619] [0.26261172]\n",
    "loss on epoch  90  :  10.8478422783 [0.2464332] [0.26679158]\n",
    "loss on epoch  100  :  5.65829826947 [0.2464332] [0.27226865]\n",
    "loss on epoch  110  :  3.68620026774 [0.25162128] [0.27543962]\n",
    "loss on epoch  120  :  2.85285171535 [0.25940338] [0.27659267]\n",
    "loss on epoch  130  :  2.42323571554 [0.26329443] [0.27270105]\n",
    "loss on epoch  140  :  2.17573743231 [0.26588845] [0.27284521]\n",
    "loss on epoch  150  :  2.0301572537 [0.26718548] [0.2702508]\n",
    "loss on epoch  160  :  1.94258512888 [0.26459143] [0.27342173]\n",
    "loss on epoch  170  :  1.88481268949 [0.2697795] [0.27716923]\n",
    "loss on epoch  180  :  1.84617088845 [0.27107653] [0.28106081]\n",
    "loss on epoch  190  :  1.81930811924 [0.2697795] [0.28178149]\n",
    "loss on epoch  200  :  1.80086447188 [0.26718548] [0.28423178]\n",
    "loss on epoch  210  :  1.7875992137 [0.32036316] [0.30859038]\n",
    "loss on epoch  220  :  1.77780386474 [0.32684824] [0.31565294]\n",
    "loss on epoch  230  :  1.76974088947 [0.32555124] [0.32040933]\n",
    "loss on epoch  240  :  1.7632859283 [0.32295719] [0.32170653]\n",
    "loss on epoch  250  :  1.75851491149 [0.32036316] [0.32213894]\n",
    "loss on epoch  260  :  1.75423782402 [0.32295719] [0.32372442]\n",
    "loss on epoch  270  :  1.75089333234 [0.32944229] [0.32473335]\n",
    "loss on epoch  280  :  1.74837678836 [0.32684824] [0.32516575]\n",
    "loss on epoch  290  :  1.74513709545 [0.32814527] [0.32516575]\n",
    "loss on epoch  300  :  1.74362069755 [0.33073929] [0.32631883]\n",
    "loss on epoch  310  :  1.74140824046 [0.32036316] [0.32660708]\n",
    "loss on epoch  320  :  1.73898471064 [0.31906614] [0.32732776]\n",
    "loss on epoch  330  :  1.73712137341 [0.32166019] [0.32992217]\n",
    "loss on epoch  340  :  1.7356525207 [0.32425421] [0.33237243]\n",
    "loss on epoch  350  :  1.73420486185 [0.32555124] [0.33208418]\n",
    "loss on epoch  360  :  1.73237812188 [0.32555124] [0.3317959]\n",
    "loss on epoch  370  :  1.7308102366 [0.32425421] [0.33309311]\n",
    "loss on epoch  380  :  1.72947029383 [0.32814527] [0.33712885]\n",
    "loss on epoch  390  :  1.72876107362 [0.32944229] [0.33611992]\n",
    "loss on epoch  400  :  1.72705303133 [0.32684824] [0.337273]\n",
    "loss on epoch  410  :  1.72517795254 [0.32166019] [0.33828193]\n",
    "loss on epoch  420  :  1.72473206951 [0.31906614] [0.34231767]\n",
    "loss on epoch  430  :  1.72311593703 [0.32295719] [0.34260595]\n",
    "loss on epoch  440  :  1.72156693207 [0.31906614] [0.34447968]\n",
    "loss on epoch  450  :  1.72039219958 [0.31776914] [0.34851542]\n",
    "loss on epoch  460  :  1.71828975887 [0.32295719] [0.35125396]\n",
    "loss on epoch  470  :  1.7153291465 [0.32295719] [0.35269532]\n",
    "loss on epoch  480  :  1.71169379354 [0.31776914] [0.35932547]\n",
    "loss on epoch  490  :  1.70989083913 [0.32036316] [0.36739695]\n",
    "loss on epoch  500  :  1.7066930996 [0.31776914] [0.3727299]\n",
    "loss on epoch  510  :  1.70284286583 [0.31776914] [0.38353992]\n",
    "loss on epoch  520  :  1.69862202583 [0.32036316] [0.39680022]\n",
    "loss on epoch  530  :  1.69522329889 [0.32295719] [0.40386277]\n",
    "loss on epoch  540  :  1.69030124115 [0.30350193] [0.41741136]\n",
    "loss on epoch  550  :  1.68574391637 [0.30479896] [0.42375323]\n",
    "loss on epoch  560  :  1.68108665722 [0.29701686] [0.42923033]\n",
    "loss on epoch  570  :  1.6782276647 [0.29053178] [0.44739118]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "0.45 dropout    \n",
    "loss on epoch  0  :  2337.22378088 [0.15953307] [0.15162872]\n",
    "loss on epoch  10  :  1889.34066999 [0.22438392] [0.19789565]\n",
    "loss on epoch  20  :  1397.70430897 [0.22697794] [0.21029115]\n",
    "loss on epoch  30  :  975.793204696 [0.21271077] [0.2164889]\n",
    "loss on epoch  40  :  657.499851086 [0.22827497] [0.23234361]\n",
    "loss on epoch  50  :  433.534762488 [0.25032425] [0.24387431]\n",
    "loss on epoch  60  :  283.06589282 [0.23994812] [0.23609109]\n",
    "loss on epoch  70  :  186.267238617 [0.25291827] [0.23508215]\n",
    "loss on epoch  80  :  125.115274783 [0.23735408] [0.23464975]\n",
    "loss on epoch  90  :  86.2245288425 [0.25940338] [0.23796484]\n",
    "loss on epoch  100  :  60.8256910289 [0.21789883] [0.20683195]\n",
    "loss on epoch  110  :  43.7591062299 [0.26070037] [0.24300951]\n",
    "loss on epoch  120  :  31.7362976163 [0.22438392] [0.22614586]\n",
    "loss on epoch  130  :  22.9795110579 [0.22568093] [0.23046988]\n",
    "loss on epoch  140  :  16.7677883131 [0.21789883] [0.22917268]\n",
    "loss on epoch  150  :  12.3121599665 [0.23476005] [0.23392908]\n",
    "loss on epoch  160  :  9.14334391223 [0.26070037] [0.25540501]\n",
    "loss on epoch  170  :  6.85844402622 [0.27496758] [0.26203516]\n",
    "loss on epoch  180  :  5.16672201731 [0.28534371] [0.27125973]\n",
    "loss on epoch  190  :  4.00481683568 [0.29831389] [0.27601615]\n",
    "loss on epoch  200  :  3.21034482011 [0.29571983] [0.2822139]\n",
    "loss on epoch  210  :  2.70275750204 [0.29961088] [0.28927645]\n",
    "loss on epoch  220  :  2.37622975972 [0.30479896] [0.2946094]\n",
    "loss on epoch  230  :  2.1668948487 [0.30350193] [0.30988759]\n",
    "loss on epoch  240  :  2.03203844527 [0.30739298] [0.31392333]\n",
    "loss on epoch  250  :  1.94201788472 [0.31128404] [0.32761604]\n",
    "loss on epoch  260  :  1.88699813353 [0.31647211] [0.33309311]\n",
    "loss on epoch  270  :  1.84561692509 [0.31776914] [0.34001154]\n",
    "loss on epoch  280  :  1.81618379553 [0.31387809] [0.34894782]\n",
    "loss on epoch  290  :  1.79266019552 [0.31128404] [0.35788411]\n",
    "loss on epoch  300  :  1.77263346977 [0.31906614] [0.36638802]\n",
    "loss on epoch  310  :  1.7528932939 [0.31906614] [0.37172097]\n",
    "loss on epoch  320  :  1.73470119746 [0.32166019] [0.39391756]\n",
    "loss on epoch  330  :  1.71242918129 [0.31517509] [0.42173538]\n",
    "loss on epoch  340  :  1.69382170781 [0.31647211] [0.43456328]\n",
    "loss on epoch  350  :  1.67667607301 [0.30479896] [0.46007496]\n",
    "loss on epoch  360  :  1.66357970734 [0.29831389] [0.48962238]\n",
    "loss on epoch  370  :  1.65094748581 [0.29961088] [0.50763911]\n",
    "loss on epoch  380  :  1.64171585827 [0.29571983] [0.5289709]\n",
    "loss on epoch  390  :  1.63375818564 [0.29312581] [0.53963679]\n",
    "loss on epoch  400  :  1.62783267542 [0.28793773] [0.5478524]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[128,64]\n",
    "\n",
    "loss on epoch  0  :  1210.32822616 [0.14526589] [0.15321419]\n",
    "loss on epoch  10  :  975.476586801 [0.19844358] [0.20798501]\n",
    "loss on epoch  20  :  710.190851847 [0.230869] [0.24286538]\n",
    "loss on epoch  30  :  478.653611501 [0.24254215] [0.24401845]\n",
    "loss on epoch  40  :  303.751033218 [0.24902724] [0.2541078]\n",
    "loss on epoch  50  :  184.012676451 [0.2542153] [0.26333237]\n",
    "loss on epoch  60  :  108.341687273 [0.26329443] [0.27053908]\n",
    "loss on epoch  70  :  64.2979677518 [0.26718548] [0.27817816]\n",
    "loss on epoch  80  :  40.3065272084 [0.26848248] [0.28091669]\n",
    "loss on epoch  90  :  27.2785535565 [0.27107653] [0.29792449]\n",
    "loss on epoch  100  :  19.8817367465 [0.27885863] [0.30513117]\n",
    "loss on epoch  110  :  15.599368559 [0.29571983] [0.31882387]\n",
    "loss on epoch  120  :  12.8068944878 [0.29701686] [0.32473335]\n",
    "loss on epoch  130  :  10.7874289486 [0.30220494] [0.33107525]\n",
    "loss on epoch  140  :  9.32465522819 [0.30220494] [0.33323723]\n",
    "loss on epoch  150  :  8.17446415733 [0.30350193] [0.33554339]\n",
    "loss on epoch  160  :  7.2405699933 [0.30220494] [0.33698472]\n",
    "loss on epoch  170  :  6.48965020312 [0.30090791] [0.34001154]\n",
    "loss on epoch  180  :  5.83686744284 [0.30090791] [0.33986738]\n",
    "loss on epoch  190  :  5.27779822438 [0.30998704] [0.33929086]\n",
    "loss on epoch  200  :  4.79931404414 [0.31128404] [0.34102046]\n",
    "loss on epoch  210  :  4.38942148288 [0.30479896] [0.3428942]\n",
    "loss on epoch  220  :  4.03806058897 [0.30739298] [0.34217355]\n",
    "loss on epoch  230  :  3.73076107105 [0.30220494] [0.34505621]\n",
    "loss on epoch  240  :  3.46827077866 [0.30350193] [0.35500145]\n",
    "loss on epoch  250  :  3.23870945529 [0.30739298] [0.35644278]\n",
    "loss on epoch  260  :  3.04132128424 [0.31128404] [0.37388295]\n",
    "loss on epoch  270  :  2.86460503936 [0.30350193] [0.38829634]\n",
    "loss on epoch  280  :  2.70675845941 [0.29961088] [0.41251081]\n",
    "loss on epoch  290  :  2.56500498785 [0.30350193] [0.42461804]\n",
    "loss on epoch  300  :  2.44148617983 [0.29571983] [0.43917555]\n",
    "loss on epoch  310  :  2.33327854232 [0.28923476] [0.44998559]\n",
    "loss on epoch  320  :  2.24139836927 [0.28793773] [0.46021909]\n",
    "loss on epoch  330  :  2.15919722396 [0.28923476] [0.46886712]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "64 64\n",
    "\n",
    "loss on epoch  0  :  1195.55497798 [0.18028535] [0.16791582]\n",
    "loss on epoch  10  :  1013.02485487 [0.21660182] [0.21331796]\n",
    "loss on epoch  20  :  789.418885973 [0.23605707] [0.22081292]\n",
    "loss on epoch  30  :  567.339089853 [0.2464332] [0.23219948]\n",
    "loss on epoch  40  :  384.794134069 [0.24124514] [0.23003748]\n",
    "loss on epoch  50  :  249.932189235 [0.23735408] [0.24257711]\n",
    "loss on epoch  60  :  158.152248948 [0.26459143] [0.26664746]\n",
    "loss on epoch  70  :  99.6646442767 [0.26459143] [0.27385414]\n",
    "loss on epoch  80  :  64.1486431051 [0.26459143] [0.27875468]\n",
    "loss on epoch  90  :  43.2139298298 [0.2775616] [0.28797925]\n",
    "loss on epoch  100  :  30.8514907978 [0.28664073] [0.2959066]\n",
    "loss on epoch  110  :  23.2330012763 [0.27626458] [0.29922166]\n",
    "loss on epoch  120  :  18.3880767646 [0.27496758] [0.30859038]\n",
    "loss on epoch  130  :  15.0871106739 [0.28274968] [0.31233785]\n",
    "loss on epoch  140  :  12.7901680072 [0.2775616] [0.31925628]\n",
    "loss on epoch  150  :  11.0702089513 [0.2775616] [0.32732776]\n",
    "loss on epoch  160  :  9.72318348178 [0.27885863] [0.3326607]\n",
    "loss on epoch  170  :  8.56857403561 [0.28793773] [0.33453444]\n",
    "loss on epoch  180  :  7.60758964441 [0.28534371] [0.33611992]\n",
    "loss on epoch  190  :  6.80784432976 [0.28015563] [0.34102046]\n",
    "loss on epoch  200  :  6.13846226975 [0.29182878] [0.34577689]\n",
    "loss on epoch  210  :  5.55261748367 [0.28793773] [0.34505621]\n",
    "loss on epoch  220  :  5.04354822636 [0.28664073] [0.34664169]\n",
    "loss on epoch  230  :  4.60903858476 [0.28793773] [0.34808302]\n",
    "loss on epoch  240  :  4.2378280516 [0.28274968] [0.35197464]\n",
    "loss on epoch  250  :  3.92214061817 [0.29312581] [0.35586625]\n",
    "loss on epoch  260  :  3.64353918588 [0.29831389] [0.35975784]\n",
    "loss on epoch  270  :  3.40245219513 [0.29053178] [0.36307293]\n",
    "loss on epoch  280  :  3.18737982379 [0.29053178] [0.36638802]\n",
    "loss on epoch  290  :  2.9966332791 [0.30350193] [0.37157682]\n",
    "loss on epoch  300  :  2.82588767233 [0.30479896] [0.3805131]\n",
    "loss on epoch  310  :  2.67088949791 [0.30609599] [0.39132315]\n",
    "loss on epoch  320  :  2.53125948597 [0.31517509] [0.39737678]\n",
    "loss on epoch  330  :  2.40231606033 [0.31258106] [0.41193429]\n",
    "loss on epoch  340  :  2.28232390903 [0.30350193] [0.42663592]\n",
    "loss on epoch  350  :  2.1780065723 [0.30090791] [0.434275]\n",
    "loss on epoch  360  :  2.0889446029 [0.29831389] [0.44465265]\n",
    "loss on epoch  370  :  2.01131260947 [0.30350193] [0.45258]\n",
    "loss on epoch  380  :  1.93970542815 [0.30998704] [0.4628135]\n",
    "loss on epoch  390  :  1.87546846326 [0.30739298] [0.49048716]\n",
    "loss on epoch  400  :  1.81851998082 [0.29442284] [0.50951284]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BACK-UP\n",
    "\n",
    "\n",
    "# wide, embedding, co-occurrence and feature interaction\n",
    "# version4: interaction on individual hidden neurons\n",
    "\n",
    "class InterNN_fuse_full():\n",
    "    \n",
    "    def __init__(self, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, session,n_embedding, n_hidden_list, lr, l2, batch_size):\n",
    "        \n",
    "#       build the network graph \n",
    "        self.LEARNING_RATE = lr\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC  = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.N_EMBED = n_embedding\n",
    "        self.N_BATCH = batch_size\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list)\n",
    "        self.N_TOTAL = self.N_CONTI+ self.N_DISC*self.N_EMBED\n",
    "        self.N_TOTAL_EMBED = self.N_DISC*self.N_EMBED\n",
    "\n",
    "        self.sess = session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y  = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "#       wide part on categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "    \n",
    "    \n",
    "#         for i in range(self.N_DISC):\n",
    "            \n",
    "#             with tf.variable_scope(\"wide\"+str(i)):\n",
    "#                 w= tf.Variable(\\\n",
    "#                 tf.random_uniform([self.N_DISC_VOCA[i], self.N_CLASS],-1.0, 1.0) )\n",
    "# #                         tf.random_normal([self.N_DISC_VOCA, self.N_CLASS],\\\n",
    "# #                         stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "#                 if i==0:\n",
    "#                     dx_wsum = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                    \n",
    "# #                   L2\n",
    "#                     self.regularizer_wide = tf.nn.l2_loss(w)\n",
    "                    \n",
    "#                 else:\n",
    "#                     dx_wsum = dx_wsum + tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                    \n",
    "# #                   L2  \n",
    "#                     self.regularizer_wide = self.regularizer_wide + tf.nn.l2_loss(w)\n",
    "               \n",
    "    \n",
    "#       co-occurrence of categorical features\n",
    "        for i in range(20):\n",
    "            \n",
    "            tmpvoca1 = self.N_DISC_VOCA[i] \n",
    "            \n",
    "            for j in range(i+1,20):\n",
    "                \n",
    "                tmpvoca2 = self.N_DISC_VOCA[j]\n",
    "                \n",
    "                with tf.variable_scope(\"cooccur\"+str(i)+str(j)):\n",
    "                    \n",
    "                    \n",
    "#                   vector approximate of co-occurrence matrix \n",
    "                    w1 = tf.Variable(tf.random_normal([self.N_CLASS, tmpvoca1, 1],\\\n",
    "                                            stddev=math.sqrt(2.0/float( tmpvoca1 ))) )\n",
    "                    \n",
    "                    w2 = tf.Variable(tf.random_normal([self.N_CLASS, 1, tmpvoca2],\\\n",
    "                                            stddev=math.sqrt(2.0/float( tmpvoca2 ))) )\n",
    "            \n",
    "                    b  = tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "                    \n",
    "                    cooc_mat = []\n",
    "                    for k in range(self.N_CLASS):\n",
    "                        cooc_mat.append( tf.reshape( tf.matmul(w1[k],w2[k]), [-1,1] ) )\n",
    "                        \n",
    "#                   n_class by voca1*voca2  \n",
    "                    cooc_vec = tf.stack(cooc_mat)\n",
    "                    cooc_vec = tf.reshape(cooc_vec, [self.N_CLASS, -1] )\n",
    "                    cooc_vec = tf.transpose(cooc_vec,[1,0])\n",
    "                    \n",
    "#                   build the idx of co-occurrence features\n",
    "                    cooc_idx = self.dx_trans[i] * tmpvoca2 + self.dx_trans[j]\n",
    "    \n",
    "                    if i==0 and j==1:\n",
    "#                       co-occurrence weighted sum\n",
    "                        cooc_wsum  = tf.nn.embedding_lookup( cooc_vec, cooc_idx ) + b\n",
    "#                       L2 \n",
    "                        self.regularizer_cooc   = tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2) \n",
    "                    else:\n",
    "                        cooc_wsum += tf.nn.embedding_lookup( cooc_vec, cooc_idx ) + b\n",
    "#                       L2\n",
    "                        self.regularizer_cooc += (tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2))\n",
    "    \n",
    "#       non-linear on co-occurrence weighted sum\n",
    "        cooc_wsum = tf.nn.relu( cooc_wsum )\n",
    "        \n",
    "        \n",
    "#       embedding of categorical features        \n",
    "        tmp_embeded = []                \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"embedding\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA[i], self.N_EMBED],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "                              \n",
    "                tmp_embeded.append( tf.nn.embedding_lookup( w, self.dx_trans[i] ) )\n",
    "                \n",
    "        dx_embeded = tf.concat(tmp_embeded, 1)\n",
    "        x_concate =  tf.concat( [self.cx,dx_embeded], 1 )\n",
    "        \n",
    "    \n",
    "    \n",
    "#       feature interaction\n",
    "        expend_cx = tf.reshape( self.cx, [-1,self.N_CONTI,1])  \n",
    "        expend_cx = tf.transpose( expend_cx, [0,2,1])\n",
    "        \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope( \"interact\"+str(i) ):\n",
    "                \n",
    "                w = tf.Variable(\\\n",
    "                tf.random_normal([self.N_DISC_VOCA[i], n_hidden_list[0], self.N_CONTI],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "                \n",
    "                tmp_w = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                \n",
    "                if i==0:\n",
    "                    tmp_h = tf.reduce_sum( tmp_w * expend_cx, 2 )\n",
    "                    \n",
    "#                   L2\n",
    "                    self.regularizer_inter = tf.nn.l2_loss(w) \n",
    "                    \n",
    "                else:\n",
    "                    tmp_h+= tf.reduce_sum( tmp_w * expend_cx, 2 )\n",
    "                    \n",
    "#                   L2 \n",
    "                    self.regularizer_inter += tf.nn.l2_loss(w)\n",
    "\n",
    "\n",
    "#         tmp_h = []\n",
    "#         for k in range(n_hidden_list[0]):\n",
    "            \n",
    "#             for i in range(self.N_DISC):\n",
    "                \n",
    "#                 with tf.variable_scope(\"interact\"+str(i)+\"h\"+str(k)):\n",
    "                    \n",
    "#                     w1 = tf.Variable(\\\n",
    "#                         tf.random_normal([self.N_DISC_VOCA[i], 1],\\\n",
    "#                         stddev=math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "                    \n",
    "#                     w2 = tf.Variable(\\\n",
    "#                         tf.random_normal([1, self.N_CONTI],\\\n",
    "#                         stddev=math.sqrt(2.0/float(self.N_CONTI))) )\n",
    "                    \n",
    "#                     w = tf.matmul(w1,w2)\n",
    "                    \n",
    "# #                     w = tf.Variable(\\\n",
    "# #                         tf.random_normal([self.N_DISC_VOCA[i], self.N_CONTI],\\\n",
    "# #                         stddev=math.sqrt(2.0/float(self.N_DISC_VOCA[i]))) )\n",
    "\n",
    "#                     if i==0:\n",
    "#                         inter_wsum  = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "#                     else:\n",
    "#                         inter_wsum += tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "\n",
    "#                     if k==0 and i==0:\n",
    "# #                       L2  \n",
    "#                         self.regularizer_inter  = tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2) \n",
    "#                     else:\n",
    "# #                       L2\n",
    "#                         self.regularizer_inter += ( tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2) )\n",
    "    \n",
    "#             tmp_h.append( tf.reduce_sum( self.cx * inter_wsum, 1 ) )\n",
    "            \n",
    "#         tmp_h = tf.stack( tmp_h )\n",
    "#         tmp_h = tf.transpose( tmp_h, [1,0] )\n",
    "        \n",
    "    \n",
    "        \n",
    "#       interaction hidden layers\n",
    "\n",
    "\n",
    "#         tmp_h = tf.reduce_sum( self.cx * inter_wsum, 2   )\n",
    "        \n",
    "#         tmp_h = [ ]\n",
    "#         for i in range( n_hidden_list[0] ):\n",
    "#             tmp_h.append( singleCol_h )\n",
    "        \n",
    "#         tmp_h = tf.stack( tmp_h )\n",
    "#         tmp_h = tf.transpose( tmp_h, [1,0])\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope(\"inter_h0\"):\n",
    "            \n",
    "            w= tf.Variable(tf.random_normal([self.N_TOTAL, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_CONTI)))) \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            \n",
    "            h_inter = tf.matmul(x_concate, w)\n",
    "            h_inter = h_inter + tmp_h\n",
    "            h_inter = tf.nn.relu( tf.add( h_inter, b ) )\n",
    "            \n",
    "#           L2   \n",
    "            self.regularizer = tf.nn.l2_loss(w)\n",
    "            \n",
    "#       dropout\n",
    "#         h_inter = tf.nn.dropout(h_inter, self.keep_prob)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"inter_h\"+str(i)):\n",
    "                w = tf.Variable(\\\n",
    "                        tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b = tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                h_inter = tf.nn.relu( tf.add( tf.matmul(h_inter, w),b) )\n",
    "        \n",
    "#               L2\n",
    "                self.regularizer += tf.nn.l2_loss(w)\n",
    "        \n",
    "        \n",
    "#       output layer  \n",
    "        h = h_inter\n",
    "#         h = tf.concat([h_embed, h_inter], 1)\n",
    "        \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w = tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1],\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "            b = tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            \n",
    "            h = tf.matmul(h, w)\n",
    "#             h = tf.add( h, dx_wsum )\n",
    "            h = tf.add( h, cooc_wsum )\n",
    "            h = tf.add( h, b )\n",
    "        \n",
    "            self.logit = h\n",
    "            \n",
    "#           L2  \n",
    "            self.regularizer += tf.nn.l2_loss(w) \n",
    "    \n",
    "#       overall L2\n",
    "        self.regularizer += (self.regularizer_cooc + self.regularizer_inter)\n",
    "#       self.regularizer_wide + \n",
    "         \n",
    "        \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y) \\\n",
    "                                  + self.L2*self.regularizer )\n",
    "        self.optimizer = \\\n",
    "        tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#       tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                       feed_dict={self.dx:dx_batch, \\\n",
    "                                  self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                  self.lr:lr,\\\n",
    "                                  self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "        \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                  self.cx:cx_test, self.y:y_test,\\\n",
    "                                                  self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        return sess.run( [self.tmp1], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "\n",
    "#   representation ability\n",
    "para_n_hidden_list = [ 16,32,32 ]\n",
    "#     128,16,8\n",
    "para_lr = 0.045\n",
    "para_n_embedding = 2\n",
    "#   regularization\n",
    "para_batch_size = 32\n",
    "para_keep_prob = 0.8\n",
    "para_l2 = 0.2\n",
    "\n",
    "# fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = [8]*para_n_disc\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 10\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    clf = InterNN_fuse_full( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                      para_n_disc_voca, sess, para_n_embedding, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2, para_batch_size)\n",
    "    clf.train_ini()\n",
    "    clf.inference_ini()\n",
    "    \n",
    "    para_cur_lr=para_lr\n",
    "    \n",
    "    total_cnt= np.shape(cxtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "    \n",
    "    total_idx=range(total_cnt)\n",
    "    \n",
    "    \n",
    "#   training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc=0.0\n",
    "        \n",
    "#       shuffle traning instances each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "    \n",
    "#       Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                     para_cur_lr )\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "    \n",
    "        tmp_test_acc  = clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.56679606]\n"
     ]
    }
   ],
   "source": [
    "# for test\n",
    "\n",
    "w1 = tf.Variable( tf.random_normal([10,2, 3],\\\n",
    "                        stddev=math.sqrt(2.0/float(2))) )\n",
    "w2 = tf.Variable( tf.random_normal([10,1, 3],\\\n",
    "                        stddev=math.sqrt(2.0/float(2))) )\n",
    "\n",
    "tmpmul = w1 * w2\n",
    "\n",
    "w3 = tf.Variable( tf.random_normal( [], stddev=math.sqrt(2.0/float(2))) )\n",
    "\n",
    "# w2 = tf.Variable( tf.random_normal([10, 5, 6],\\\n",
    "#                         stddev=math.sqrt(2.0/float(2))) )\n",
    "\n",
    "\n",
    "# idx = tf.constant( [ [1,2],[2,3],[3,4],[9,8] ] )\n",
    "\n",
    "# tmp = tf.nn.embedding_lookup( [ w2 ], idx )\n",
    "tmpshape = tf.shape( tmpmul )\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run( tf.global_variables_initializer() )\n",
    "    print sess.run( [ w3 ], feed_dict={ })\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
