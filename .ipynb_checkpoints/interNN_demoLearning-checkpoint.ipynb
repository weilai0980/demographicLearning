{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wide and embedding, \n",
    "# wide co-occurrence\n",
    "# wide co-occurrence & interaction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from six.moves import urllib\n",
    "# from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# continuous and categorical feature split\n",
    "\n",
    "# features on columns [0:split_idx] are continuous\n",
    "# the rest are categorical\n",
    "def conti_cate_split(dta_df, split_idx):\n",
    "    \n",
    "    col_cnt= dta_df.shape[1]\n",
    "    conti_cols= range(split_idx)\n",
    "    dis_cols=range(split_idx, col_cnt)\n",
    "    \n",
    "    return dta_df[conti_cols], dta_df[dis_cols]\n",
    "\n",
    "def conti_normalization_train_dta(dta_df):\n",
    "    \n",
    "    return preprocessing.scale(dta_df)\n",
    "\n",
    "def conti_normalization_test_dta(dta_df, train_df):\n",
    "    \n",
    "    mean_dim = np.mean(train_df, axis=0)\n",
    "    std_dim = np.std(train_df, axis=0)\n",
    "        \n",
    "    df=pd.DataFrame()\n",
    "    cols = train_df.columns\n",
    "    idx=0\n",
    "    \n",
    "    for i in cols:\n",
    "        df[i] = (dta_df[i]- mean_dim[idx])*1.0/std_dim[idx]\n",
    "        idx=idx+1\n",
    "        \n",
    "    return df\n",
    "\n",
    "files_list=[\"../dataset/dataset_demo/xtrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/xtest.csv\",\\\n",
    "            \"../dataset/dataset_demo/ytrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/ytest.csv\"]\n",
    "\n",
    "xtrain_df=pd.read_csv( files_list[0] ,sep=',', header=None)\n",
    "xtest_df=pd.read_csv( files_list[1] ,sep=',', header=None)\n",
    "ytrain_df=pd.read_csv( files_list[2] ,sep=',', header=None)\n",
    "ytest_df=pd.read_csv( files_list[3] ,sep=',', header=None)\n",
    "\n",
    "print xtrain_df.shape, xtest_df.shape, ytrain_df.shape, ytest_df.shape\n",
    "\n",
    "\n",
    "# get training and testing data prepared\n",
    "\n",
    "cxtrain, dxtrain = conti_cate_split(xtrain_df, 106)\n",
    "cxtest, dxtest = conti_cate_split(xtest_df, 106)\n",
    "\n",
    "cxtest = conti_normalization_test_dta(cxtest, cxtrain)\n",
    "cxtrain = conti_normalization_train_dta(cxtrain)\n",
    "\n",
    "print cxtrain.shape, dxtrain.shape\n",
    "print cxtest.shape, dxtest.shape\n",
    "\n",
    "# cxtrain = cxtrain.as_matrix() \n",
    "dxtrain = dxtrain.as_matrix().astype(int)\n",
    "cxtest = cxtest.as_matrix()\n",
    "dxtest = dxtest.as_matrix().astype(int)\n",
    "\n",
    "ytest = np_utils.to_categorical( ytest_df.as_matrix() )\n",
    "ytrain =np_utils.to_categorical( ytrain_df.as_matrix() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wide and embedding\n",
    "# co-occurrence features\n",
    "\n",
    "class wide_embed_coocc_NN():\n",
    "    \n",
    "    def __init__(self, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, session,n_embedding, n_hidden_list, lr, l2):\n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.N_EMBED = n_embedding\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list)\n",
    "        self.N_TOTAL = self.N_CONTI+ self.N_DISC*self.N_EMBED\n",
    "\n",
    "        self.sess=session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "#       wide part on categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "        \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"wide\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_uniform([self.N_DISC_VOCA[i], self.N_CLASS],-1.0, 1.0) )\n",
    "#                         tf.random_normal([self.N_DISC_VOCA, self.N_CLASS],\\\n",
    "#                         stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "                if i==0:\n",
    "                    dx_wsum = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                    \n",
    "#                   L2\n",
    "                    self.regularizer_wide = tf.nn.l2_loss(w)\n",
    "                    \n",
    "                else:\n",
    "                    dx_wsum = dx_wsum + tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                    \n",
    "#                   L2  \n",
    "                    self.regularizer_wide = self.regularizer_wide + tf.nn.l2_loss(w)\n",
    "                    \n",
    "            \n",
    "#       embedding categorical features        \n",
    "        tmp_embeded = []                \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"embedding\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA[i], self.N_EMBED],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "                              \n",
    "                tmp_embeded.append( tf.nn.embedding_lookup( w, self.dx_trans[i] ) )\n",
    "                \n",
    "        dx_embeded = tf.concat(tmp_embeded, 1)\n",
    "        x_concate =  tf.concat( [self.cx,dx_embeded], 1 )\n",
    "\n",
    "        \n",
    "#      co-occurrence of categorical features\n",
    "        \n",
    "       for i in range(self.N_DISC):\n",
    "            for j in range(i+1,self.N_DISC):\n",
    "                \n",
    "                \n",
    "        \n",
    "        \n",
    "#----unit test---------------------------------------------------------\n",
    "#         print self.N_TOTAL\n",
    "#         self.tmpshape= tf.shape(x_concate)\n",
    "        \n",
    "#         self.tmpshape1= tf.shape(tmp_embeded[0])\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# #     hidden layers\n",
    "        with tf.variable_scope(\"h0\"):\n",
    "            w= tf.Variable(tf.random_normal([self.N_TOTAL, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_CONTI)))) \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            h = tf.nn.relu( tf.add( tf.matmul(x_concate, w),b) )\n",
    "        \n",
    "                \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"layer\"+str(i)):\n",
    "                w= tf.Variable(tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b= tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                h = tf.nn.relu( tf.add( tf.matmul(h, w),b) )\n",
    "                \n",
    "                \n",
    "#       Regularization\n",
    "\n",
    "#       dropout\n",
    "#         if not (abs(self.keep_prob - 1.0) <= 1e-4):\n",
    "#             h = tf.nn.dropout(h, self.keep_prob)\n",
    "#       L2\n",
    "        self.regularizer = tf.nn.l2_loss(w)\n",
    "        \n",
    "        \n",
    "#       output layer  \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w= tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1],\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            \n",
    "            \n",
    "#             h = tf.add( tf.matmul(h, w), b )\n",
    "            \n",
    "            h = tf.add( tf.matmul(h, w), dx_wsum )\n",
    "            h = tf.add( h, b )\n",
    "#             \n",
    "            self.logit = h\n",
    "     \n",
    "    \n",
    "#           L2  \n",
    "            self.regularizer = self.regularizer + tf.nn.l2_loss(w)\n",
    "    \n",
    "#       overall L2\n",
    "        self.regularizer= self.regularizer + self.regularizer_wide\n",
    "         \n",
    "    \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y) \\\n",
    "                                  + self.L2*self.regularizer )\n",
    "        self.optimizer = \\\n",
    "        tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#       tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                       feed_dict={self.dx:dx_batch, \\\n",
    "                                  self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                  self.lr:lr,\\\n",
    "                                  self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "    \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                  self.cx:cx_test, self.y:y_test,\\\n",
    "                                                  self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        return sess.run( [self.tmpshape], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "#   representation ability\n",
    "para_n_hidden_list = [ 128,64,8 ]\n",
    "#     128,16,8 ]\n",
    "para_lr = 0.05\n",
    "para_n_embedding = 2\n",
    "#   regularization\n",
    "para_batch_size = 32\n",
    "para_keep_prob = 1.0\n",
    "para_l2 = 0.15\n",
    "\n",
    "# fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = [8,8,8,8,8,8,8,8]\n",
    "\n",
    "tot_cnt= np.shape(cxtrain)[0]\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    wide_mlp_clf = wide_embed_NN( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                      para_n_disc_voca, sess, para_n_embedding, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2 )\n",
    "    wide_mlp_clf.train_ini()\n",
    "    wide_mlp_clf.inference_ini()\n",
    "    \n",
    "    para_cur_lr=para_lr\n",
    "    total_batch = int(tot_cnt/para_batch_size)\n",
    "\n",
    "    tot_idx=range(tot_cnt)\n",
    "\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc=0.0\n",
    "        \n",
    "#       shuffle traning dataset each epoch  \n",
    "        np.random.shuffle(tot_idx)\n",
    "   \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = tot_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += wide_mlp_clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                       para_cur_lr)\n",
    "        \n",
    "#       learning rate decaying\n",
    "#         para_cur_lr = para_cur_lr*0.98\n",
    "        \n",
    "        tmp_test_acc = wide_mlp_clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = wide_mlp_clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
