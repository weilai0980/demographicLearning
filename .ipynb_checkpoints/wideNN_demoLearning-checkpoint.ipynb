{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TO DO:\n",
    "\n",
    "# ordinal \n",
    "# categorical+continuous\n",
    "\n",
    "# feature interaction\n",
    "\n",
    "# batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from six.moves import urllib\n",
    "# from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6938, 186) (771, 186) (6938, 1) (771, 1)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "files_list=[\"../dataset/dataset_demo/xtrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/xtest.csv\",\\\n",
    "            \"../dataset/dataset_demo/ytrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/ytest.csv\"]\n",
    "\n",
    "xtrain_df=pd.read_csv( files_list[0] ,sep=',', header=None)\n",
    "xtest_df=pd.read_csv( files_list[1] ,sep=',', header=None)\n",
    "ytrain_df=pd.read_csv( files_list[2] ,sep=',', header=None)\n",
    "ytest_df=pd.read_csv( files_list[3] ,sep=',', header=None)\n",
    "\n",
    "print xtrain_df.shape, xtest_df.shape, ytrain_df.shape, ytest_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# continuous and categorical feature split\n",
    "\n",
    "# features on columns [0:split_idx] are continuous\n",
    "# the rest are categorical\n",
    "def conti_cate_split(dta_df, split_idx):\n",
    "    \n",
    "    col_cnt= dta_df.shape[1]\n",
    "    conti_cols= range(split_idx)\n",
    "    dis_cols=range(split_idx, col_cnt)\n",
    "    \n",
    "    return dta_df[conti_cols], dta_df[dis_cols]\n",
    "\n",
    "def conti_normalization_train_dta(dta_df):\n",
    "    \n",
    "    return preprocessing.scale(dta_df)\n",
    "\n",
    "def conti_normalization_test_dta(dta_df, train_df):\n",
    "    \n",
    "    mean_dim = np.mean(train_df, axis=0)\n",
    "    std_dim = np.std(train_df, axis=0)\n",
    "        \n",
    "    df=pd.DataFrame()\n",
    "    cols = train_df.columns\n",
    "    idx=0\n",
    "    \n",
    "    for i in cols:\n",
    "        df[i] = (dta_df[i]- mean_dim[idx])*1.0/std_dim[idx]\n",
    "        idx=idx+1\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6938, 106) (6938, 80)\n",
      "(771, 106) (771, 80)\n"
     ]
    }
   ],
   "source": [
    "# get training and testing data prepared\n",
    "\n",
    "cxtrain, dxtrain = conti_cate_split(xtrain_df, 106)\n",
    "cxtest, dxtest = conti_cate_split(xtest_df, 106)\n",
    "\n",
    "cxtest = conti_normalization_test_dta(cxtest, cxtrain)\n",
    "cxtrain = conti_normalization_train_dta(cxtrain)\n",
    "\n",
    "print cxtrain.shape, dxtrain.shape\n",
    "print cxtest.shape, dxtest.shape\n",
    "\n",
    "# cxtrain = cxtrain.as_matrix() \n",
    "dxtrain = dxtrain.as_matrix().astype(int)\n",
    "cxtest = cxtest.as_matrix()\n",
    "dxtest = dxtest.as_matrix().astype(int)\n",
    "\n",
    "ytest = np_utils.to_categorical( ytest_df.as_matrix() )\n",
    "ytrain =np_utils.to_categorical( ytrain_df.as_matrix() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only embedding\n",
    "\n",
    "class mlp_demo():\n",
    "    \n",
    "    def __init__(self, batch_size, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, n_embedding, session, n_hidden_list, lr, l2):\n",
    "        \n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.N_EMBED = n_embedding\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list) \n",
    "        self.N_TOTAL = self.N_CONTI+ self.N_DISC*self.N_EMBED\n",
    "        \n",
    "        self.sess=session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "#       embedding categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "        \n",
    "        tmp_embeded = []                \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"disc\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA, self.N_EMBED],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "                              \n",
    "                tmp_embeded.append( tf.nn.embedding_lookup( w, self.dx_trans[i] ) )\n",
    "                \n",
    "        dx_embeded = tf.concat(tmp_embeded, 1)\n",
    "        x_concate =  tf.concat( [self.cx,dx_embeded], 1 )\n",
    "\n",
    "#----unit test---------------------------------------------------------\n",
    "#         print self.N_TOTAL\n",
    "#         self.tmpshape= tf.shape(x_concate)\n",
    "        \n",
    "#         self.tmpshape1= tf.shape(tmp_embeded[0])\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# #     hidden layers\n",
    "        with tf.variable_scope(\"h0\"):\n",
    "            w= tf.Variable(tf.random_normal([self.N_TOTAL, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_TOTAL)))) \n",
    "            \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            \n",
    "            h = tf.nn.relu( tf.add( tf.matmul(x_concate, w),b) )\n",
    "        \n",
    "        \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"layer\"+str(i)):\n",
    "                \n",
    "                w= tf.Variable(tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                \n",
    "                b= tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                \n",
    "                h = tf.nn.relu( tf.add( tf.matmul(h, w),b) )\n",
    "        \n",
    "#   Regularization  \n",
    "#       dropout\n",
    "        h = tf.nn.dropout(h, self.keep_prob)\n",
    "\n",
    "#       L2\n",
    "        self.regularizer = tf.nn.l2_loss(w)\n",
    "    \n",
    "        \n",
    "#       output layer  \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w= tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1],\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "                \n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            \n",
    "            h = tf.add( tf.matmul(h, w),b)\n",
    "            \n",
    "#           L2  \n",
    "            self.regularizer = self.regularizer + tf.nn.l2_loss(w)\n",
    "            \n",
    "            \n",
    "            self.logit = h\n",
    "    \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        \n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y) \\\n",
    "                                   )\n",
    "#                                   + self.L2*self.regularizer)\n",
    "        \n",
    "        self.optimizer = \\\n",
    "        tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                       feed_dict={self.dx:dx_batch, \\\n",
    "                                  self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                  self.lr:lr,\\\n",
    "                                  self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "    \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        \n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                  self.cx:cx_test, self.y:y_test,\\\n",
    "                                                  self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "        return sess.run( [self.tmpshape], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch  0  :  2.61422722582 [0.08690013] [0.084462382]\n",
      "loss on epoch  1  :  2.49386830606 [0.10246433] [0.087633327]\n",
      "loss on epoch  2  :  2.40369938249 [0.10505836] [0.097002015]\n",
      "loss on epoch  3  :  2.32592620193 [0.12581064] [0.11790141]\n",
      "loss on epoch  4  :  2.26486413375 [0.13618676] [0.13058518]\n",
      "loss on epoch  5  :  2.21472775159 [0.15953307] [0.14153935]\n",
      "loss on epoch  6  :  2.17865108234 [0.16990921] [0.16215047]\n",
      "loss on epoch  7  :  2.14756187321 [0.17509727] [0.17541078]\n",
      "loss on epoch  8  :  2.10958231878 [0.18677042] [0.18261747]\n",
      "loss on epoch  9  :  2.08552519567 [0.18028535] [0.18535601]\n",
      "loss on epoch  10  :  2.06685491403 [0.19844358] [0.19991352]\n",
      "loss on epoch  11  :  2.04117549243 [0.19584955] [0.20265207]\n",
      "loss on epoch  12  :  2.02940414263 [0.20492867] [0.21274142]\n",
      "loss on epoch  13  :  2.01158284709 [0.1919585] [0.21202075]\n",
      "loss on epoch  14  :  1.99941817574 [0.21919584] [0.22138944]\n",
      "loss on epoch  15  :  1.98532693282 [0.21400778] [0.22268665]\n",
      "loss on epoch  16  :  1.970428364 [0.21530479] [0.22643413]\n",
      "loss on epoch  17  :  1.95943478657 [0.21271077] [0.22556932]\n",
      "loss on epoch  18  :  1.95040682606 [0.19844358] [0.23147881]\n",
      "loss on epoch  19  :  1.94116616422 [0.21400778] [0.23364082]\n",
      "loss on epoch  20  :  1.93450188205 [0.21919584] [0.23681176]\n",
      "loss on epoch  21  :  1.92922003459 [0.21530479] [0.24272124]\n",
      "loss on epoch  22  :  1.91551972645 [0.21400778] [0.23839723]\n",
      "loss on epoch  23  :  1.90681860499 [0.21660182] [0.24416259]\n",
      "loss on epoch  24  :  1.90527622164 [0.22438392] [0.24373019]\n",
      "loss on epoch  25  :  1.89479471981 [0.23476005] [0.24286538]\n",
      "loss on epoch  26  :  1.88681508493 [0.22178988] [0.24445085]\n",
      "loss on epoch  27  :  1.88398860244 [0.21660182] [0.24762179]\n",
      "loss on epoch  28  :  1.87418638796 [0.22568093] [0.25093687]\n",
      "loss on epoch  29  :  1.86902366687 [0.23994812] [0.24690112]\n",
      "loss on epoch  30  :  1.85822703527 [0.21789883] [0.2528106]\n",
      "loss on epoch  31  :  1.8534215585 [0.23346303] [0.25468436]\n",
      "loss on epoch  32  :  1.83902623774 [0.23735408] [0.25136927]\n",
      "loss on epoch  33  :  1.84152160866 [0.21530479] [0.24949554]\n",
      "loss on epoch  34  :  1.83938114453 [0.24513619] [0.25180167]\n",
      "loss on epoch  35  :  1.83359429715 [0.25291827] [0.25612569]\n",
      "loss on epoch  36  :  1.8241494488 [0.23994812] [0.25554916]\n",
      "loss on epoch  37  :  1.81742702014 [0.23735408] [0.25886422]\n",
      "loss on epoch  38  :  1.81930239995 [0.22957198] [0.25886422]\n",
      "loss on epoch  39  :  1.81010535748 [0.24513619] [0.26635918]\n",
      "loss on epoch  40  :  1.80726657916 [0.23735408] [0.26246756]\n",
      "loss on epoch  41  :  1.80655316166 [0.25162128] [0.26030555]\n",
      "loss on epoch  42  :  1.8053623697 [0.24254215] [0.26405305]\n",
      "loss on epoch  43  :  1.8007925913 [0.24254215] [0.25944075]\n",
      "loss on epoch  44  :  1.80164214494 [0.24773023] [0.26434129]\n",
      "loss on epoch  45  :  1.78981894082 [0.25032425] [0.25929663]\n",
      "loss on epoch  46  :  1.78740447673 [0.25680932] [0.26145864]\n",
      "loss on epoch  47  :  1.78715371308 [0.22308689] [0.26535025]\n",
      "loss on epoch  48  :  1.77901055847 [0.23735408] [0.26808879]\n",
      "loss on epoch  49  :  1.77977057989 [0.23735408] [0.2702508]\n",
      "loss on epoch  50  :  1.77956776861 [0.23216602] [0.26722398]\n",
      "loss on epoch  51  :  1.76444555106 [0.22308689] [0.26333237]\n",
      "loss on epoch  52  :  1.77577356757 [0.24513619] [0.27125973]\n",
      "loss on epoch  53  :  1.76994730424 [0.23476005] [0.26318824]\n",
      "loss on epoch  54  :  1.76955749418 [0.230869] [0.26909772]\n",
      "loss on epoch  55  :  1.76536952153 [0.24773023] [0.27140385]\n",
      "loss on epoch  56  :  1.76179415506 [0.2542153] [0.2711156]\n",
      "loss on epoch  57  :  1.76048218426 [0.24513619] [0.27082732]\n",
      "loss on epoch  58  :  1.75456513097 [0.230869] [0.27688095]\n",
      "loss on epoch  59  :  1.75259230707 [0.24902724] [0.27486306]\n",
      "loss on epoch  60  :  1.75556410568 [0.24902724] [0.26679158]\n",
      "loss on epoch  61  :  1.75497958366 [0.24902724] [0.26722398]\n",
      "loss on epoch  62  :  1.75122288085 [0.25032425] [0.27183625]\n",
      "loss on epoch  63  :  1.74525252632 [0.24124514] [0.27515134]\n",
      "loss on epoch  64  :  1.74325222122 [0.22697794] [0.27688095]\n",
      "loss on epoch  65  :  1.74560496773 [0.24383917] [0.27587202]\n",
      "loss on epoch  66  :  1.73978167168 [0.25291827] [0.27284521]\n",
      "loss on epoch  67  :  1.7391307665 [0.25162128] [0.27414241]\n",
      "loss on epoch  68  :  1.73621884032 [0.24254215] [0.2826463]\n",
      "loss on epoch  69  :  1.73697915716 [0.25551233] [0.27832228]\n",
      "loss on epoch  70  :  1.73305163522 [0.25810635] [0.28091669]\n",
      "loss on epoch  71  :  1.73230051908 [0.24383917] [0.27486306]\n",
      "loss on epoch  72  :  1.72892993343 [0.25291827] [0.27601615]\n",
      "loss on epoch  73  :  1.72756560125 [0.23994812] [0.27702507]\n",
      "loss on epoch  74  :  1.72633773693 [0.22568093] [0.2776016]\n",
      "loss on epoch  75  :  1.7250714233 [0.2386511] [0.28524071]\n",
      "loss on epoch  76  :  1.7245498386 [0.2464332] [0.28120497]\n",
      "loss on epoch  77  :  1.7253584283 [0.25032425] [0.28192562]\n",
      "loss on epoch  78  :  1.72253868632 [0.24383917] [0.28423178]\n",
      "loss on epoch  79  :  1.71978227643 [0.24902724] [0.28178149]\n",
      "loss on epoch  80  :  1.72009952777 [0.25291827] [0.28668204]\n",
      "loss on epoch  81  :  1.72050525313 [0.25162128] [0.28423178]\n",
      "loss on epoch  82  :  1.71664421714 [0.25680932] [0.28480831]\n",
      "loss on epoch  83  :  1.71465025777 [0.24902724] [0.27918708]\n",
      "loss on epoch  84  :  1.71126086556 [0.22697794] [0.28206977]\n",
      "loss on epoch  85  :  1.71045970917 [0.22438392] [0.28769097]\n",
      "loss on epoch  86  :  1.71390926406 [0.25162128] [0.28697032]\n",
      "loss on epoch  87  :  1.70894393109 [0.26588845] [0.29028538]\n",
      "loss on epoch  88  :  1.71021868958 [0.24773023] [0.28495243]\n",
      "loss on epoch  89  :  1.71148903733 [0.24124514] [0.28668204]\n",
      "loss on epoch  90  :  1.70101175792 [0.24773023] [0.28466415]\n",
      "loss on epoch  91  :  1.6974319185 [0.25291827] [0.28942057]\n",
      "loss on epoch  92  :  1.70477889586 [0.24902724] [0.29042953]\n",
      "loss on epoch  93  :  1.71027487171 [0.23735408] [0.29042953]\n",
      "loss on epoch  94  :  1.70083088806 [0.24513619] [0.29071778]\n",
      "loss on epoch  95  :  1.69831745434 [0.24254215] [0.29028538]\n",
      "loss on epoch  96  :  1.70295568048 [0.23994812] [0.28985298]\n",
      "loss on epoch  97  :  1.70005551715 [0.25680932] [0.2830787]\n",
      "loss on epoch  98  :  1.69905696313 [0.27107653] [0.29273567]\n",
      "loss on epoch  99  :  1.68922085952 [0.24254215] [0.28970885]\n",
      "loss on epoch  100  :  1.69521312696 [0.24383917] [0.29475352]\n",
      "loss on epoch  101  :  1.6901545516 [0.25940338] [0.29042953]\n",
      "loss on epoch  102  :  1.68992905945 [0.22827497] [0.29331219]\n",
      "loss on epoch  103  :  1.68812953994 [0.26070037] [0.28624964]\n",
      "loss on epoch  104  :  1.69065090625 [0.25291827] [0.29230326]\n",
      "loss on epoch  105  :  1.68259632069 [0.26070037] [0.29345632]\n",
      "loss on epoch  106  :  1.68658695446 [0.26459143] [0.29158258]\n",
      "loss on epoch  107  :  1.68642134943 [0.24513619] [0.29749209]\n",
      "loss on epoch  108  :  1.68449893205 [0.2697795] [0.30123955]\n",
      "loss on epoch  109  :  1.68571563303 [0.24513619] [0.29619488]\n",
      "loss on epoch  110  :  1.68117628754 [0.25291827] [0.29100606]\n",
      "loss on epoch  111  :  1.68321034183 [0.26329443] [0.29763621]\n",
      "loss on epoch  112  :  1.67956350679 [0.2619974] [0.29806861]\n",
      "loss on epoch  113  :  1.68410028668 [0.26459143] [0.29936582]\n",
      "loss on epoch  114  :  1.6812430672 [0.24773023] [0.29533008]\n",
      "loss on epoch  115  :  1.67432038248 [0.24773023] [0.29965407]\n",
      "loss on epoch  116  :  1.67479011546 [0.26588845] [0.30282503]\n",
      "loss on epoch  117  :  1.67291004934 [0.27626458] [0.29778033]\n",
      "loss on epoch  118  :  1.67324330669 [0.25162128] [0.29821274]\n",
      "loss on epoch  119  :  1.67615422304 [0.24902724] [0.29633901]\n",
      "loss on epoch  120  :  1.67492169315 [0.2775616] [0.30412224]\n",
      "loss on epoch  121  :  1.67083613682 [0.27496758] [0.30210435]\n",
      "loss on epoch  122  :  1.66654620913 [0.26459143] [0.29950994]\n",
      "loss on epoch  123  :  1.67006113737 [0.2542153] [0.29792449]\n",
      "loss on epoch  124  :  1.66945139913 [0.27237353] [0.30095127]\n",
      "loss on epoch  125  :  1.6678916501 [0.2464332] [0.29994234]\n",
      "loss on epoch  126  :  1.66611844474 [0.26070037] [0.30023062]\n",
      "loss on epoch  127  :  1.670025632 [0.25940338] [0.30642837]\n",
      "loss on epoch  128  :  1.66472513866 [0.25291827] [0.30441049]\n",
      "loss on epoch  129  :  1.66167618399 [0.2464332] [0.29749209]\n",
      "loss on epoch  130  :  1.66368335831 [0.26459143] [0.30282503]\n",
      "loss on epoch  131  :  1.66460313572 [0.27496758] [0.29936582]\n",
      "loss on epoch  132  :  1.66144398848 [0.29053178] [0.29878926]\n",
      "loss on epoch  133  :  1.66340365185 [0.26848248] [0.30066302]\n",
      "loss on epoch  134  :  1.66320372146 [0.27367055] [0.30628422]\n",
      "loss on epoch  135  :  1.65455690415 [0.27367055] [0.30051887]\n",
      "loss on epoch  136  :  1.65585200856 [0.2697795] [0.30729318]\n",
      "loss on epoch  137  :  1.66074991831 [0.25551233] [0.30729318]\n",
      "loss on epoch  138  :  1.65490908986 [0.27496758] [0.30642837]\n",
      "loss on epoch  139  :  1.65650673275 [0.26718548] [0.30945519]\n",
      "loss on epoch  140  :  1.66073810881 [0.25680932] [0.30513117]\n",
      "loss on epoch  141  :  1.65161540629 [0.25680932] [0.3065725]\n",
      "loss on epoch  142  :  1.65395997141 [0.24383917] [0.31305853]\n",
      "loss on epoch  143  :  1.6552448195 [0.27496758] [0.30902278]\n",
      "loss on epoch  144  :  1.65322914676 [0.2619974] [0.30570769]\n",
      "loss on epoch  145  :  1.65212599436 [0.27107653] [0.31507638]\n",
      "loss on epoch  146  :  1.65259484709 [0.2697795] [0.30729318]\n",
      "loss on epoch  147  :  1.64482027897 [0.27237353] [0.30412224]\n",
      "loss on epoch  148  :  1.64610056601 [0.27367055] [0.31277025]\n",
      "loss on epoch  149  :  1.6470984998 [0.27626458] [0.30729318]\n",
      "loss on epoch  150  :  1.64978679453 [0.25940338] [0.31132892]\n",
      "loss on epoch  151  :  1.64683401757 [0.26848248] [0.30513117]\n",
      "loss on epoch  152  :  1.64378582308 [0.2697795] [0.31363505]\n",
      "loss on epoch  153  :  1.64196273987 [0.27626458] [0.30945519]\n",
      "loss on epoch  154  :  1.65037150487 [0.27367055] [0.31449986]\n",
      "loss on epoch  155  :  1.64072564979 [0.26848248] [0.30815798]\n",
      "loss on epoch  156  :  1.6456297636 [0.26848248] [0.31219372]\n",
      "loss on epoch  157  :  1.643377571 [0.27626458] [0.30844623]\n",
      "loss on epoch  158  :  1.63993541313 [0.26848248] [0.31075239]\n",
      "loss on epoch  159  :  1.64460514507 [0.25810635] [0.31277025]\n",
      "loss on epoch  160  :  1.64305499585 [0.28145266] [0.30873451]\n",
      "loss on epoch  161  :  1.63953856627 [0.27367055] [0.31204957]\n",
      "loss on epoch  162  :  1.64047178928 [0.27107653] [0.30916691]\n",
      "loss on epoch  163  :  1.64398846592 [0.28274968] [0.30758142]\n",
      "loss on epoch  164  :  1.63921382375 [0.28274968] [0.3116172]\n",
      "loss on epoch  165  :  1.64019210356 [0.28145266] [0.31363505]\n",
      "loss on epoch  166  :  1.62776404878 [0.25680932] [0.31738254]\n",
      "loss on epoch  167  :  1.64101356613 [0.26588845] [0.31536466]\n",
      "loss on epoch  168  :  1.63252607895 [0.27367055] [0.30859038]\n",
      "loss on epoch  169  :  1.63815864273 [0.25291827] [0.3194004]\n",
      "loss on epoch  170  :  1.63270210097 [0.27367055] [0.32271549]\n",
      "loss on epoch  171  :  1.63517226603 [0.27367055] [0.31795907]\n",
      "loss on epoch  172  :  1.63880319699 [0.25940338] [0.31738254]\n",
      "loss on epoch  173  :  1.63147490958 [0.27107653] [0.31435573]\n",
      "loss on epoch  174  :  1.62989330637 [0.29701686] [0.31089652]\n",
      "loss on epoch  175  :  1.6278641388 [0.26329443] [0.3194004]\n",
      "loss on epoch  176  :  1.62908899957 [0.2697795] [0.31651774]\n",
      "loss on epoch  177  :  1.63313287928 [0.25810635] [0.31464398]\n",
      "loss on epoch  178  :  1.62522724832 [0.27496758] [0.32660708]\n",
      "loss on epoch  179  :  1.63129894025 [0.27107653] [0.31017584]\n",
      "loss on epoch  180  :  1.62656145338 [0.2619974] [0.31723839]\n",
      "loss on epoch  181  :  1.62877973785 [0.2619974] [0.31997693]\n",
      "loss on epoch  182  :  1.62233376935 [0.2697795] [0.31637359]\n",
      "loss on epoch  183  :  1.6276209985 [0.26848248] [0.32502162]\n",
      "loss on epoch  184  :  1.62201510305 [0.29701686] [0.32329202]\n",
      "loss on epoch  185  :  1.61847600125 [0.26329443] [0.31968868]\n",
      "loss on epoch  186  :  1.62231313837 [0.27496758] [0.32069761]\n",
      "loss on epoch  187  :  1.62613297113 [0.28793773] [0.32040933]\n",
      "loss on epoch  188  :  1.62880422499 [0.26459143] [0.32040933]\n",
      "loss on epoch  189  :  1.61994261327 [0.28664073] [0.32314789]\n",
      "loss on epoch  190  :  1.61482667923 [0.27626458] [0.3304987]\n",
      "loss on epoch  191  :  1.61692957688 [0.28534371] [0.32113001]\n",
      "loss on epoch  192  :  1.61877404175 [0.27496758] [0.3244451]\n",
      "loss on epoch  193  :  1.62038492897 [0.2619974] [0.31882387]\n",
      "loss on epoch  194  :  1.61936412946 [0.27107653] [0.32069761]\n",
      "loss on epoch  195  :  1.6168661394 [0.26329443] [0.32430094]\n",
      "loss on epoch  196  :  1.61880313745 [0.27626458] [0.32300374]\n",
      "loss on epoch  197  :  1.61818910854 [0.27496758] [0.32458922]\n",
      "loss on epoch  198  :  1.61886279393 [0.28923476] [0.32502162]\n",
      "loss on epoch  199  :  1.61514697472 [0.28274968] [0.32833669]\n",
      "loss on epoch  200  :  1.61196388715 [0.28534371] [0.32660708]\n",
      "loss on epoch  201  :  1.6097901118 [0.29182878] [0.32819256]\n",
      "loss on epoch  202  :  1.61432593802 [0.29312581] [0.32833669]\n",
      "loss on epoch  203  :  1.60803401643 [0.28274968] [0.32285962]\n",
      "loss on epoch  204  :  1.6140108359 [0.28274968] [0.33078697]\n",
      "loss on epoch  205  :  1.61180584327 [0.27496758] [0.32271549]\n",
      "loss on epoch  206  :  1.61233669692 [0.27107653] [0.32703948]\n",
      "loss on epoch  207  :  1.60779411724 [0.26588845] [0.32242721]\n",
      "loss on epoch  208  :  1.61105318519 [0.27496758] [0.32992217]\n",
      "loss on epoch  209  :  1.60927555267 [0.29053178] [0.3248775]\n",
      "loss on epoch  210  :  1.61038536093 [0.27107653] [0.32862496]\n",
      "loss on epoch  211  :  1.60661830591 [0.27626458] [0.32761604]\n",
      "loss on epoch  212  :  1.60598229498 [0.2775616] [0.33583164]\n",
      "loss on epoch  213  :  1.60343729413 [0.28534371] [0.32516575]\n",
      "loss on epoch  214  :  1.60293952538 [0.26718548] [0.32920149]\n",
      "loss on epoch  215  :  1.6065662175 [0.28015563] [0.33194005]\n",
      "loss on epoch  216  :  1.61023265383 [0.29442284] [0.33309311]\n",
      "loss on epoch  217  :  1.60671417523 [0.28664073] [0.33395791]\n",
      "loss on epoch  218  :  1.60029305755 [0.30350193] [0.32747188]\n",
      "loss on epoch  219  :  1.60230076227 [0.27626458] [0.32660708]\n",
      "loss on epoch  220  :  1.60310762689 [0.29571983] [0.33208418]\n",
      "loss on epoch  221  :  1.60611775042 [0.2775616] [0.33366963]\n",
      "loss on epoch  222  :  1.60767434473 [0.28274968] [0.3368406]\n",
      "loss on epoch  223  :  1.59885677825 [0.26588845] [0.33986738]\n",
      "loss on epoch  224  :  1.60215993463 [0.28145266] [0.33611992]\n",
      "loss on epoch  225  :  1.59910532789 [0.27496758] [0.32948977]\n",
      "loss on epoch  226  :  1.6067588623 [0.28923476] [0.33828193]\n",
      "loss on epoch  227  :  1.60493328433 [0.26070037] [0.33669645]\n",
      "loss on epoch  228  :  1.59537817862 [0.29961088] [0.33021045]\n",
      "loss on epoch  229  :  1.59717476368 [0.28404668] [0.33669645]\n",
      "loss on epoch  230  :  1.59583705836 [0.30090791] [0.33871433]\n",
      "loss on epoch  231  :  1.5980478888 [0.26848248] [0.33237243]\n",
      "loss on epoch  232  :  1.60065011443 [0.28404668] [0.33309311]\n",
      "loss on epoch  233  :  1.59516813703 [0.28404668] [0.33842605]\n",
      "loss on epoch  234  :  1.59394969716 [0.28534371] [0.33078697]\n",
      "loss on epoch  235  :  1.59720753587 [0.28923476] [0.33467859]\n",
      "loss on epoch  236  :  1.59560884255 [0.29701686] [0.33496684]\n",
      "loss on epoch  237  :  1.59542596945 [0.28015563] [0.33251658]\n",
      "loss on epoch  238  :  1.59220009351 [0.27367055] [0.3377054]\n",
      "loss on epoch  239  :  1.5917280731 [0.30220494] [0.33381379]\n",
      "loss on epoch  240  :  1.59273186888 [0.27626458] [0.34116459]\n",
      "loss on epoch  241  :  1.59274913003 [0.27367055] [0.337273]\n",
      "loss on epoch  242  :  1.59249239383 [0.29182878] [0.34116459]\n",
      "loss on epoch  243  :  1.59151257553 [0.30090791] [0.33669645]\n",
      "loss on epoch  244  :  1.5942455798 [0.2775616] [0.33914673]\n",
      "loss on epoch  245  :  1.58795188907 [0.29182878] [0.33395791]\n",
      "loss on epoch  246  :  1.58512260776 [0.29961088] [0.34102046]\n",
      "loss on epoch  247  :  1.58765042865 [0.29571983] [0.3368406]\n",
      "loss on epoch  248  :  1.58421827658 [0.28664073] [0.34303835]\n",
      "loss on epoch  249  :  1.58485727293 [0.28664073] [0.34447968]\n",
      "loss on epoch  250  :  1.58729444984 [0.29312581] [0.33914673]\n",
      "loss on epoch  251  :  1.58321696606 [0.26848248] [0.34217355]\n",
      "loss on epoch  252  :  1.58843778009 [0.28923476] [0.33972326]\n",
      "loss on epoch  253  :  1.5850086765 [0.29442284] [0.34116459]\n",
      "loss on epoch  254  :  1.58840860318 [0.2775616] [0.33712885]\n",
      "loss on epoch  255  :  1.58502191219 [0.28015563] [0.33943498]\n",
      "loss on epoch  256  :  1.58078161357 [0.28404668] [0.34404728]\n",
      "loss on epoch  257  :  1.58022428855 [0.28923476] [0.34520036]\n",
      "loss on epoch  258  :  1.5830756901 [0.28145266] [0.34505621]\n",
      "loss on epoch  259  :  1.58032094309 [0.28793773] [0.34246179]\n",
      "loss on epoch  260  :  1.57987393504 [0.27885863] [0.34520036]\n",
      "loss on epoch  261  :  1.58377049107 [0.28015563] [0.33929086]\n",
      "loss on epoch  262  :  1.57781318651 [0.28145266] [0.3359758]\n",
      "loss on epoch  263  :  1.57776725638 [0.29442284] [0.33525512]\n",
      "loss on epoch  264  :  1.58158685677 [0.29312581] [0.34563276]\n",
      "loss on epoch  265  :  1.57752667907 [0.28793773] [0.3441914]\n",
      "loss on epoch  266  :  1.57728689823 [0.28404668] [0.35211876]\n",
      "loss on epoch  267  :  1.57402402681 [0.28534371] [0.34217355]\n",
      "loss on epoch  268  :  1.57194823113 [0.27626458] [0.34447968]\n",
      "loss on epoch  269  :  1.57641290582 [0.29442284] [0.34981263]\n",
      "loss on epoch  270  :  1.57785174812 [0.29053178] [0.343759]\n",
      "loss on epoch  271  :  1.57768320346 [0.2697795] [0.34822714]\n",
      "loss on epoch  272  :  1.57103791271 [0.27237353] [0.35644278]\n",
      "loss on epoch  273  :  1.57321587162 [0.28793773] [0.343759]\n",
      "loss on epoch  274  :  1.5729787056 [0.29312581] [0.34707409]\n",
      "loss on epoch  275  :  1.57188535341 [0.27107653] [0.33900261]\n",
      "loss on epoch  276  :  1.56832791325 [0.28404668] [0.35211876]\n",
      "loss on epoch  277  :  1.56911313966 [0.27367055] [0.34822714]\n",
      "loss on epoch  278  :  1.57296860995 [0.25940338] [0.34563276]\n",
      "loss on epoch  279  :  1.57148840116 [0.29312581] [0.34592101]\n",
      "loss on epoch  280  :  1.57051600408 [0.28664073] [0.34231767]\n",
      "loss on epoch  281  :  1.57291408898 [0.29961088] [0.34707409]\n",
      "loss on epoch  282  :  1.565831098 [0.28145266] [0.35038915]\n",
      "loss on epoch  283  :  1.56775942834 [0.28534371] [0.34361488]\n",
      "loss on epoch  284  :  1.56816553119 [0.28664073] [0.35096571]\n",
      "loss on epoch  285  :  1.56778172911 [0.27107653] [0.3548573]\n",
      "loss on epoch  286  :  1.57114019601 [0.27626458] [0.33943498]\n",
      "loss on epoch  287  :  1.56453608689 [0.29053178] [0.34433556]\n",
      "loss on epoch  288  :  1.56929924091 [0.27885863] [0.3446238]\n",
      "loss on epoch  289  :  1.56798517531 [0.28145266] [0.34491208]\n",
      "loss on epoch  290  :  1.56731935515 [0.26848248] [0.35514557]\n",
      "loss on epoch  291  :  1.56746249873 [0.28145266] [0.34808302]\n",
      "loss on epoch  292  :  1.56237046097 [0.28793773] [0.3552897]\n",
      "loss on epoch  293  :  1.56561315146 [0.28404668] [0.35139811]\n",
      "loss on epoch  294  :  1.56622335099 [0.28534371] [0.34822714]\n",
      "loss on epoch  295  :  1.56062916051 [0.28534371] [0.34938022]\n",
      "loss on epoch  296  :  1.56249964324 [0.29571983] [0.35629866]\n",
      "loss on epoch  297  :  1.56672552098 [0.29053178] [0.35038915]\n",
      "loss on epoch  298  :  1.56542755987 [0.29312581] [0.35384837]\n",
      "loss on epoch  299  :  1.56129760155 [0.28664073] [0.35543385]\n",
      "loss on epoch  300  :  1.56401248859 [0.27885863] [0.34909195]\n",
      "loss on epoch  301  :  1.56115893264 [0.2775616] [0.35226291]\n",
      "loss on epoch  302  :  1.56184436446 [0.29701686] [0.35125396]\n",
      "loss on epoch  303  :  1.55947515325 [0.30739298] [0.35644278]\n",
      "loss on epoch  304  :  1.56075630845 [0.28534371] [0.34995675]\n",
      "loss on epoch  305  :  1.56276087744 [0.29053178] [0.35255116]\n",
      "loss on epoch  306  :  1.55964823046 [0.30998704] [0.35557798]\n",
      "loss on epoch  307  :  1.55843533727 [0.27626458] [0.34952435]\n",
      "loss on epoch  308  :  1.55898785678 [0.28534371] [0.35716346]\n",
      "loss on epoch  309  :  1.55279939676 [0.27367055] [0.36278465]\n",
      "loss on epoch  310  :  1.55777931473 [0.29053178] [0.34765062]\n",
      "loss on epoch  311  :  1.55623694779 [0.28534371] [0.35154223]\n",
      "loss on epoch  312  :  1.55949163005 [0.28274968] [0.35428077]\n",
      "loss on epoch  313  :  1.55914581254 [0.30350193] [0.35543385]\n",
      "loss on epoch  314  :  1.55631851977 [0.28274968] [0.35125396]\n",
      "loss on epoch  315  :  1.56181482388 [0.30220494] [0.35168636]\n",
      "loss on epoch  316  :  1.55464390568 [0.2775616] [0.35442489]\n",
      "loss on epoch  317  :  1.55178000702 [0.29182878] [0.35356009]\n",
      "loss on epoch  318  :  1.55952101341 [0.29053178] [0.35831651]\n",
      "loss on epoch  319  :  1.55095859282 [0.29442284] [0.35961372]\n",
      "loss on epoch  320  :  1.55107389326 [0.28664073] [0.35889307]\n",
      "loss on epoch  321  :  1.55601706107 [0.29312581] [0.34894782]\n",
      "loss on epoch  322  :  1.55168117734 [0.27496758] [0.3561545]\n",
      "loss on epoch  323  :  1.54954182843 [0.2775616] [0.35946959]\n",
      "loss on epoch  324  :  1.54744014014 [0.28793773] [0.35687518]\n",
      "loss on epoch  325  :  1.55227681606 [0.27367055] [0.36292881]\n",
      "loss on epoch  326  :  1.54635335656 [0.28793773] [0.35687518]\n",
      "loss on epoch  327  :  1.54941915861 [0.28534371] [0.3561545]\n",
      "loss on epoch  328  :  1.54976532079 [0.29442284] [0.36379361]\n",
      "loss on epoch  329  :  1.55072136806 [0.29053178] [0.36451426]\n",
      "loss on epoch  330  :  1.54385475922 [0.28534371] [0.3561545]\n",
      "loss on epoch  331  :  1.54393426664 [0.27367055] [0.36451426]\n",
      "loss on epoch  332  :  1.54236346656 [0.30479896] [0.35903719]\n",
      "loss on epoch  333  :  1.5477453142 [0.29053178] [0.36134332]\n",
      "loss on epoch  334  :  1.54560586421 [0.26848248] [0.35817239]\n",
      "loss on epoch  335  :  1.5491548267 [0.28015563] [0.36321706]\n",
      "loss on epoch  336  :  1.54694283009 [0.26329443] [0.36494666]\n",
      "loss on epoch  337  :  1.54167940478 [0.27885863] [0.35961372]\n",
      "loss on epoch  338  :  1.54421128829 [0.28015563] [0.36105505]\n",
      "loss on epoch  339  :  1.54505009323 [0.28015563] [0.36393774]\n",
      "loss on epoch  340  :  1.5457862702 [0.26070037] [0.36321706]\n",
      "loss on epoch  341  :  1.54346239653 [0.28015563] [0.36177573]\n",
      "loss on epoch  342  :  1.54649431982 [0.29961088] [0.3607668]\n",
      "loss on epoch  343  :  1.54171908506 [0.2697795] [0.362064]\n",
      "loss on epoch  344  :  1.54643293135 [0.27107653] [0.36782935]\n",
      "loss on epoch  345  :  1.540095883 [0.26588845] [0.36437014]\n",
      "loss on epoch  346  :  1.54471881839 [0.28793773] [0.362064]\n",
      "loss on epoch  347  :  1.53662310994 [0.29053178] [0.36552322]\n",
      "loss on epoch  348  :  1.53683151024 [0.27107653] [0.37071201]\n",
      "loss on epoch  349  :  1.53400001146 [0.2775616] [0.36264053]\n",
      "loss on epoch  350  :  1.54328190589 [0.29701686] [0.36782935]\n",
      "loss on epoch  351  :  1.54193830576 [0.27626458] [0.36552322]\n",
      "loss on epoch  352  :  1.53721495815 [0.30090791] [0.36364946]\n",
      "loss on epoch  353  :  1.53751960008 [0.28664073] [0.36451426]\n",
      "loss on epoch  354  :  1.53647109305 [0.27626458] [0.36105505]\n",
      "loss on epoch  355  :  1.53651830079 [0.27885863] [0.36883828]\n",
      "loss on epoch  356  :  1.5369222121 [0.2775616] [0.35773998]\n",
      "loss on epoch  357  :  1.53331676255 [0.29182878] [0.35932547]\n",
      "loss on epoch  358  :  1.53749199881 [0.29961088] [0.36609975]\n",
      "loss on epoch  359  :  1.53799132333 [0.2775616] [0.36264053]\n",
      "loss on epoch  360  :  1.5341967638 [0.28274968] [0.36509082]\n",
      "loss on epoch  361  :  1.53634656858 [0.29571983] [0.36883828]\n",
      "loss on epoch  362  :  1.53538244185 [0.29571983] [0.36336121]\n",
      "loss on epoch  363  :  1.53189126132 [0.28145266] [0.36509082]\n",
      "loss on epoch  364  :  1.53451344414 [0.29053178] [0.36609975]\n",
      "loss on epoch  365  :  1.53408959033 [0.28404668] [0.36336121]\n",
      "loss on epoch  366  :  1.53450494659 [0.30739298] [0.38036898]\n",
      "loss on epoch  367  :  1.5317440629 [0.28923476] [0.37200922]\n",
      "loss on epoch  368  :  1.5333330493 [0.27107653] [0.36984721]\n",
      "loss on epoch  369  :  1.53109200104 [0.28145266] [0.36710867]\n",
      "loss on epoch  370  :  1.52549788053 [0.28923476] [0.36264053]\n",
      "loss on epoch  371  :  1.53090870726 [0.27367055] [0.37186509]\n",
      "loss on epoch  372  :  1.52685344565 [0.29831389] [0.3722975]\n",
      "loss on epoch  373  :  1.52789336616 [0.28793773] [0.36422601]\n",
      "loss on epoch  374  :  1.52847247711 [0.27367055] [0.37676564]\n",
      "loss on epoch  375  :  1.53084224808 [0.27626458] [0.37388295]\n",
      "loss on epoch  376  :  1.52843056641 [0.27626458] [0.36984721]\n",
      "loss on epoch  377  :  1.52930054803 [0.27367055] [0.37200922]\n",
      "loss on epoch  378  :  1.52681452468 [0.28534371] [0.37532431]\n",
      "loss on epoch  379  :  1.52257849168 [0.27885863] [0.3689824]\n",
      "loss on epoch  380  :  1.52531223971 [0.28145266] [0.37460363]\n",
      "loss on epoch  381  :  1.5275093844 [0.28404668] [0.38036898]\n",
      "loss on epoch  382  :  1.52625300919 [0.30090791] [0.36739695]\n",
      "loss on epoch  383  :  1.52466591372 [0.29701686] [0.36970308]\n",
      "loss on epoch  384  :  1.52335652407 [0.27367055] [0.3731623]\n",
      "loss on epoch  385  :  1.52454504414 [0.27885863] [0.37950417]\n",
      "loss on epoch  386  :  1.52013548215 [0.28274968] [0.37503603]\n",
      "loss on epoch  387  :  1.52317686012 [0.27367055] [0.3727299]\n",
      "loss on epoch  388  :  1.5243087791 [0.28015563] [0.38022485]\n",
      "loss on epoch  389  :  1.52405286958 [0.28015563] [0.37763044]\n",
      "loss on epoch  390  :  1.51812897299 [0.28664073] [0.37417123]\n",
      "loss on epoch  391  :  1.52378608265 [0.26070037] [0.36999136]\n",
      "loss on epoch  392  :  1.52483972777 [0.28404668] [0.37532431]\n",
      "loss on epoch  393  :  1.51841678222 [0.28145266] [0.37633324]\n",
      "loss on epoch  394  :  1.51755234642 [0.28923476] [0.36754107]\n",
      "loss on epoch  395  :  1.52047690965 [0.28534371] [0.37518016]\n",
      "loss on epoch  396  :  1.51762422617 [0.2775616] [0.37806284]\n",
      "loss on epoch  397  :  1.52061503866 [0.28274968] [0.37590083]\n",
      "loss on epoch  398  :  1.51954705041 [0.28923476] [0.38137791]\n",
      "loss on epoch  399  :  1.51609612634 [0.27885863] [0.38541368]\n",
      "loss on epoch  400  :  1.51879321838 [0.28534371] [0.36970308]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2b267ccb88f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mbatch_y\u001b[0m  \u001b[1;33m=\u001b[0m  \u001b[0mytrain\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0mtmpc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmlp_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mbatch_dx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_cx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpara_keep_prob\u001b[0m\u001b[1;33m,\u001b[0m                                       \u001b[0mpara_cur_lr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;31m#       learning rate decaying\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-e5cabb94d7a7>\u001b[0m in \u001b[0;36mtrain_batch\u001b[1;34m(self, dx_batch, cx_batch, y_batch, keep_prob, lr)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdx_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m                       \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdx_batch\u001b[0m\u001b[1;33m,\u001b[0m                                   \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mkeep_prob\u001b[0m                                 \u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "para_n_epoch = 1000\n",
    "\n",
    "# tunable parameters\n",
    "# representation ability\n",
    "para_n_hidden_list = [ 256,128,32 ]\n",
    "para_n_embedding = 2\n",
    "para_lr=0.007\n",
    "# regularization\n",
    "para_batch_size = 50\n",
    "para_keep_prob=0.9\n",
    "para_l2= 0.1\n",
    "\n",
    "\n",
    "# fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = 8\n",
    "\n",
    "tot_cnt= np.shape(cxtrain)[0]\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    mlp_clf = mlp_demo( para_batch_size, para_n_conti, para_n_disc, para_n_class, \\\n",
    "                        para_n_disc_voca, para_n_embedding, sess, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2 )\n",
    "    mlp_clf.train_ini()\n",
    "    mlp_clf.inference_ini()\n",
    "    \n",
    "    para_cur_lr=para_lr\n",
    "    total_batch = int(tot_cnt/para_batch_size)\n",
    "\n",
    "    tot_idx=range(tot_cnt)\n",
    "\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc=0.0\n",
    "        \n",
    "#       shuffle traning dataset each epoch  \n",
    "        np.random.shuffle(tot_idx)\n",
    "   \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = tot_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += mlp_clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                       para_cur_lr)\n",
    "        \n",
    "#       learning rate decaying\n",
    "        para_cur_lr = para_cur_lr*0.99\n",
    "        \n",
    "        tmp_test_acc = mlp_clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = mlp_clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only wide\n",
    "class wide_mlp_demo():\n",
    "    \n",
    "    def __init__(self, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, session, n_hidden_list, lr, l2):\n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list) \n",
    "        self.sess=session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        \n",
    "#       linnear part on categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "        \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"disc\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_uniform([self.N_DISC_VOCA, self.N_CLASS],-1.0, 1.0) )\n",
    "#                         tf.random_normal([self.N_DISC_VOCA, self.N_CLASS],\\\n",
    "#                         stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "                \n",
    "                if i==0:\n",
    "                    dx_wsum = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                else:\n",
    "                    dx_wsum = dx_wsum + tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "        \n",
    "        with tf.variable_scope(\"wide\"):\n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))            \n",
    "            dx_wsum = tf.add(dx_wsum, b)\n",
    "\n",
    "#----unit test---------------------------------------------------------\n",
    "#         print self.N_TOTAL\n",
    "#         self.tmpshape= tf.shape(x_concate)\n",
    "        \n",
    "#         self.tmpshape1= tf.shape(tmp_embeded[0])\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# #     hidden layers\n",
    "        with tf.variable_scope(\"h0\"):\n",
    "            w= tf.Variable(tf.random_normal([self.N_CONTI, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_CONTI)))) \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            h = tf.nn.relu( tf.add( tf.matmul(self.cx, w),b) )\n",
    "        \n",
    "                \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"layer\"+str(i)):\n",
    "                w= tf.Variable(tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b= tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                h = tf.nn.relu( tf.add( tf.matmul(h, w),b) )\n",
    "        \n",
    "#       Regularization\n",
    "#       dropout\n",
    "#         h = tf.nn.dropout(h, self.keep_prob)\n",
    "#       L2\n",
    "        self.regularizer = tf.nn.l2_loss(w)\n",
    "        \n",
    "#       output layer  \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w= tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1],\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            h = tf.add( tf.matmul(h, w),b)\n",
    "            \n",
    "#           L2  \n",
    "            self.regularizer = self.regularizer + tf.nn.l2_loss(w)\n",
    "            \n",
    "            self.logit = h + dx_wsum\n",
    "    \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y) \\\n",
    "                                  + self.L2*self.regularizer)\n",
    "        self.optimizer = \\\n",
    "        tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#       tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                       feed_dict={self.dx:dx_batch, \\\n",
    "                                  self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                  self.lr:lr,\\\n",
    "                                  self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "    \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                  self.cx:cx_test, self.y:y_test,\\\n",
    "                                                  self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        return sess.run( [self.tmpshape], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "para_n_epoch = 800\n",
    "\n",
    "# tunable parameters\n",
    "# representation ability\n",
    "para_n_hidden_list = [ 16,8 ]\n",
    "para_lr = 0.001\n",
    "# regularization\n",
    "para_batch_size = 128\n",
    "para_keep_prob = 0.9\n",
    "para_l2 = 0.1\n",
    "\n",
    "# fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = 8\n",
    "\n",
    "\n",
    "tot_cnt= np.shape(cxtrain)[0]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    wide_mlp_clf = wide_mlp_demo( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                        para_n_disc_voca, sess, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2 )\n",
    "    wide_mlp_clf.train_ini()\n",
    "    wide_mlp_clf.inference_ini()\n",
    "    \n",
    "    para_cur_lr=para_lr\n",
    "    total_batch = int(tot_cnt/para_batch_size)\n",
    "\n",
    "    tot_idx=range(tot_cnt)\n",
    "\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc=0.0\n",
    "        \n",
    "#       shuffle traning dataset each epoch  \n",
    "        np.random.shuffle(tot_idx)\n",
    "   \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = tot_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += wide_mlp_clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                       para_cur_lr)\n",
    "        \n",
    "#       learning rate decaying\n",
    "        para_cur_lr = para_cur_lr*0.98\n",
    "        \n",
    "        tmp_test_acc = wide_mlp_clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = wide_mlp_clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wide and embedding\n",
    "class wide_embed_NN():\n",
    "    \n",
    "    def __init__(self, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, session,n_embedding, n_hidden_list, lr, l2):\n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.N_EMBED = n_embedding\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list)\n",
    "        self.N_TOTAL = self.N_CONTI+ self.N_DISC*self.N_EMBED\n",
    "\n",
    "        self.sess=session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "#       wide part on categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "        \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"wide\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_uniform([self.N_DISC_VOCA, self.N_CLASS],-1.0, 1.0) )\n",
    "#                         tf.random_normal([self.N_DISC_VOCA, self.N_CLASS],\\\n",
    "#                         stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "                if i==0:\n",
    "                    dx_wsum = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                    \n",
    "#                   L2\n",
    "                    self.regularizer_wide = tf.nn.l2_loss(w)\n",
    "                    \n",
    "                else:\n",
    "                    dx_wsum = dx_wsum + tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                    \n",
    "#                   L2  \n",
    "                    self.regularizer_wide = self.regularizer_wide + tf.nn.l2_loss(w)\n",
    "                    \n",
    "#       nonlinear    \n",
    "#       dx_wsum = tf.nn.relu( dx_wsum )\n",
    "\n",
    "            \n",
    "#       embedding categorical features\n",
    "#         self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "        \n",
    "        tmp_embeded = []                \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"embedding\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA, self.N_EMBED],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "                              \n",
    "                tmp_embeded.append( tf.nn.embedding_lookup( w, self.dx_trans[i] ) )\n",
    "                \n",
    "        dx_embeded = tf.concat(tmp_embeded, 1)\n",
    "        x_concate =  tf.concat( [self.cx,dx_embeded], 1 )\n",
    "\n",
    "#----unit test---------------------------------------------------------\n",
    "#         print self.N_TOTAL\n",
    "#         self.tmpshape= tf.shape(x_concate)\n",
    "        \n",
    "#         self.tmpshape1= tf.shape(tmp_embeded[0])\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# #     hidden layers\n",
    "        with tf.variable_scope(\"h0\"):\n",
    "            w= tf.Variable(tf.random_normal([self.N_TOTAL, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_CONTI)))) \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            h = tf.nn.relu( tf.add( tf.matmul(x_concate, w),b) )\n",
    "        \n",
    "                \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"layer\"+str(i)):\n",
    "                w= tf.Variable(tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b= tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                h = tf.nn.relu( tf.add( tf.matmul(h, w),b) )\n",
    "                \n",
    "                \n",
    "#       Regularization\n",
    "\n",
    "#       dropout\n",
    "#         if not (abs(self.keep_prob - 1.0) <= 1e-4):\n",
    "#             h = tf.nn.dropout(h, self.keep_prob)\n",
    "#       L2\n",
    "        self.regularizer = tf.nn.l2_loss(w)\n",
    "        \n",
    "        \n",
    "#       output layer  \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w= tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1],\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            \n",
    "            \n",
    "#             h = tf.add( tf.matmul(h, w), b )\n",
    "            \n",
    "            h = tf.add( tf.matmul(h, w), dx_wsum )\n",
    "            h = tf.add( h, b )\n",
    "#             \n",
    "            self.logit = h\n",
    "     \n",
    "    \n",
    "#           L2  \n",
    "            self.regularizer = self.regularizer + tf.nn.l2_loss(w)\n",
    "    \n",
    "#       overall L2\n",
    "        self.regularizer= self.regularizer + self.regularizer_wide\n",
    "         \n",
    "    \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y) \\\n",
    "                                  + self.L2*self.regularizer )\n",
    "        self.optimizer = \\\n",
    "        tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#       tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                       feed_dict={self.dx:dx_batch, \\\n",
    "                                  self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                  self.lr:lr,\\\n",
    "                                  self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "    \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                  self.cx:cx_test, self.y:y_test,\\\n",
    "                                                  self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        return sess.run( [self.tmpshape], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch  0  :  94.4937369382 [0.1465629] [0.14153935]\n",
      "loss on epoch  1  :  92.3291044942 [0.17120622] [0.17137504]\n",
      "loss on epoch  2  :  90.432362839 [0.19714656] [0.18578841]\n",
      "loss on epoch  3  :  88.5252320678 [0.20752271] [0.19112135]\n",
      "loss on epoch  4  :  86.5589682614 [0.21660182] [0.19530123]\n",
      "loss on epoch  5  :  84.5368006318 [0.22049287] [0.19746324]\n",
      "loss on epoch  6  :  82.4663367095 [0.21789883] [0.19789565]\n",
      "loss on epoch  7  :  80.3735050978 [0.21789883] [0.19731911]\n",
      "loss on epoch  8  :  78.2713521675 [0.21660182] [0.19616604]\n",
      "loss on epoch  9  :  76.1661681069 [0.21271077] [0.19558951]\n",
      "loss on epoch  10  :  74.0831482146 [0.21530479] [0.19717498]\n",
      "loss on epoch  11  :  72.0111204783 [0.21660182] [0.19717498]\n",
      "loss on epoch  12  :  69.9527416582 [0.21271077] [0.19731911]\n",
      "loss on epoch  13  :  67.9132997372 [0.21530479] [0.19674258]\n",
      "loss on epoch  14  :  65.9043848603 [0.21660182] [0.19616604]\n",
      "loss on epoch  15  :  63.9216221527 [0.21660182] [0.19659844]\n",
      "loss on epoch  16  :  61.9589118074 [0.21660182] [0.19861631]\n",
      "loss on epoch  17  :  60.039198275 [0.21919584] [0.19861631]\n",
      "loss on epoch  18  :  58.1500600532 [0.21789883] [0.19991352]\n",
      "loss on epoch  19  :  56.2980681349 [0.21400778] [0.20178726]\n",
      "loss on epoch  20  :  54.4833497648 [0.21271077] [0.2045258]\n",
      "loss on epoch  21  :  52.7203920152 [0.21141374] [0.20654367]\n",
      "loss on epoch  22  :  50.9889692377 [0.21400778] [0.20971462]\n",
      "loss on epoch  23  :  49.3117629334 [0.21660182] [0.21115595]\n",
      "loss on epoch  24  :  47.6687175786 [0.22049287] [0.21130009]\n",
      "loss on epoch  25  :  46.0732638041 [0.21919584] [0.21274142]\n",
      "loss on epoch  26  :  44.5216157348 [0.22178988] [0.21519169]\n",
      "loss on epoch  27  :  43.0128811024 [0.22438392] [0.21576823]\n",
      "loss on epoch  28  :  41.5477971324 [0.22308689] [0.21720958]\n",
      "loss on epoch  29  :  40.1242208481 [0.22178988] [0.21836264]\n",
      "loss on epoch  30  :  38.7565874877 [0.22178988] [0.21821851]\n",
      "loss on epoch  31  :  37.4214583326 [0.22568093] [0.21865091]\n",
      "loss on epoch  32  :  36.1320730315 [0.22827497] [0.22124532]\n",
      "loss on epoch  33  :  34.884803825 [0.22697794] [0.22225425]\n",
      "loss on epoch  34  :  33.6801138454 [0.22697794] [0.22441626]\n",
      "loss on epoch  35  :  32.5135397558 [0.22568093] [0.22701067]\n",
      "loss on epoch  36  :  31.3933576743 [0.22697794] [0.22701067]\n",
      "loss on epoch  37  :  30.3134647299 [0.22049287] [0.22830787]\n",
      "loss on epoch  38  :  29.2721963635 [0.22178988] [0.23133467]\n",
      "loss on epoch  39  :  28.2673095067 [0.22438392] [0.23176707]\n",
      "loss on epoch  40  :  27.29977885 [0.22438392] [0.23364082]\n",
      "loss on epoch  41  :  26.3738222387 [0.22697794] [0.23522629]\n",
      "loss on epoch  42  :  25.4856768184 [0.22957198] [0.23609109]\n",
      "loss on epoch  43  :  24.6261152073 [0.22697794] [0.23594695]\n",
      "loss on epoch  44  :  23.8072291215 [0.22957198] [0.23666763]\n",
      "loss on epoch  45  :  23.0173719812 [0.23476005] [0.23810896]\n",
      "loss on epoch  46  :  22.2628408715 [0.23346303] [0.23926203]\n",
      "loss on epoch  47  :  21.5368046319 [0.23346303] [0.24012683]\n",
      "loss on epoch  48  :  20.8466231646 [0.23346303] [0.24127991]\n",
      "loss on epoch  49  :  20.1821368624 [0.23605707] [0.24257711]\n",
      "loss on epoch  50  :  19.545376813 [0.2386511] [0.24502738]\n",
      "loss on epoch  51  :  18.9385997896 [0.23735408] [0.24574806]\n",
      "loss on epoch  52  :  18.3588685548 [0.23605707] [0.24747767]\n",
      "loss on epoch  53  :  17.8026619222 [0.24124514] [0.24805419]\n",
      "loss on epoch  54  :  17.2709426615 [0.23735408] [0.2497838]\n",
      "loss on epoch  55  :  16.7626037465 [0.23994812] [0.25122514]\n",
      "loss on epoch  56  :  16.2790865324 [0.23216602] [0.24877486]\n",
      "loss on epoch  57  :  15.8164870651 [0.23994812] [0.25064859]\n",
      "loss on epoch  58  :  15.3761857571 [0.23476005] [0.25050446]\n",
      "loss on epoch  59  :  14.9526313985 [0.2386511] [0.25295475]\n",
      "loss on epoch  60  :  14.5488698262 [0.24124514] [0.25425196]\n",
      "loss on epoch  61  :  14.1623575776 [0.24124514] [0.25396368]\n",
      "loss on epoch  62  :  13.7947438779 [0.23994812] [0.25569329]\n",
      "loss on epoch  63  :  13.4403655838 [0.2386511] [0.25756702]\n",
      "loss on epoch  64  :  13.1031158324 [0.2386511] [0.25799942]\n",
      "loss on epoch  65  :  12.781834567 [0.2386511] [0.2591525]\n",
      "loss on epoch  66  :  12.4708448958 [0.23735408] [0.25972903]\n",
      "loss on epoch  67  :  12.1755059913 [0.23994812] [0.26073796]\n",
      "loss on epoch  68  :  11.8885832495 [0.24254215] [0.26246756]\n",
      "loss on epoch  69  :  11.6202846986 [0.24383917] [0.26261172]\n",
      "loss on epoch  70  :  11.3592250303 [0.2386511] [0.26289997]\n",
      "loss on epoch  71  :  11.1098081933 [0.24254215] [0.26434129]\n",
      "loss on epoch  72  :  10.8698294251 [0.23994812] [0.2647737]\n",
      "loss on epoch  73  :  10.6345109101 [0.23994812] [0.2665033]\n",
      "loss on epoch  74  :  10.4164619181 [0.24124514] [0.26751226]\n",
      "loss on epoch  75  :  10.1996693081 [0.24254215] [0.26880947]\n",
      "loss on epoch  76  :  9.99119448662 [0.24513619] [0.26852119]\n",
      "loss on epoch  77  :  9.79404150557 [0.24773023] [0.26953012]\n",
      "loss on epoch  78  :  9.60028846176 [0.25032425] [0.27082732]\n",
      "loss on epoch  79  :  9.41470189448 [0.2542153] [0.27284521]\n",
      "loss on epoch  80  :  9.23525032291 [0.24902724] [0.27313346]\n",
      "loss on epoch  81  :  9.06450676918 [0.25032425] [0.27500722]\n",
      "loss on epoch  82  :  8.897771098 [0.25291827] [0.27500722]\n",
      "loss on epoch  83  :  8.73324926253 [0.24902724] [0.27443066]\n",
      "loss on epoch  84  :  8.57601340612 [0.24902724] [0.27716923]\n",
      "loss on epoch  85  :  8.4212093265 [0.25032425] [0.27774575]\n",
      "loss on epoch  86  :  8.27546403805 [0.25162128] [0.27875468]\n",
      "loss on epoch  87  :  8.13179096469 [0.25291827] [0.28019601]\n",
      "loss on epoch  88  :  7.9907086315 [0.25291827] [0.28134909]\n",
      "loss on epoch  89  :  7.85458613987 [0.24773023] [0.28235802]\n",
      "loss on epoch  90  :  7.72347540988 [0.25291827] [0.2822139]\n",
      "loss on epoch  91  :  7.59308988077 [0.25162128] [0.28408763]\n",
      "loss on epoch  92  :  7.46955953704 [0.2542153] [0.28480831]\n",
      "loss on epoch  93  :  7.3480774694 [0.25551233] [0.2839435]\n",
      "loss on epoch  94  :  7.22721408694 [0.25551233] [0.28437591]\n",
      "loss on epoch  95  :  7.11226772379 [0.25680932] [0.28552896]\n",
      "loss on epoch  96  :  7.00243532658 [0.25551233] [0.28466415]\n",
      "loss on epoch  97  :  6.89290839213 [0.25680932] [0.28653792]\n",
      "loss on epoch  98  :  6.78989406868 [0.25551233] [0.28711444]\n",
      "loss on epoch  99  :  6.68691913508 [0.25291827] [0.28711444]\n",
      "loss on epoch  100  :  6.5849838875 [0.25940338] [0.28682616]\n",
      "loss on epoch  101  :  6.48574826673 [0.25810635] [0.28884405]\n",
      "loss on epoch  102  :  6.39212633504 [0.25940338] [0.28783512]\n",
      "loss on epoch  103  :  6.29898968449 [0.2619974] [0.28999713]\n",
      "loss on epoch  104  :  6.2070237928 [0.26588845] [0.29129431]\n",
      "loss on epoch  105  :  6.11861745516 [0.26459143] [0.29230326]\n",
      "loss on epoch  106  :  6.03265711776 [0.26459143] [0.2946094]\n",
      "loss on epoch  107  :  5.94617096142 [0.26459143] [0.2950418]\n",
      "loss on epoch  108  :  5.86421720628 [0.26588845] [0.29605073]\n",
      "loss on epoch  109  :  5.78397808914 [0.26588845] [0.29720381]\n",
      "loss on epoch  110  :  5.70730102283 [0.26848248] [0.29763621]\n",
      "loss on epoch  111  :  5.6283824797 [0.27107653] [0.29835686]\n",
      "loss on epoch  112  :  5.55595803923 [0.27107653] [0.29965407]\n",
      "loss on epoch  113  :  5.48294224342 [0.26718548] [0.29994234]\n",
      "loss on epoch  114  :  5.41228245806 [0.27107653] [0.30210435]\n",
      "loss on epoch  115  :  5.3435081398 [0.27237353] [0.30311328]\n",
      "loss on epoch  116  :  5.27515331463 [0.27237353] [0.30340156]\n",
      "loss on epoch  117  :  5.20736632524 [0.27237353] [0.30354568]\n",
      "loss on epoch  118  :  5.14458852786 [0.27107653] [0.30325744]\n",
      "loss on epoch  119  :  5.08242688135 [0.27107653] [0.30282503]\n",
      "loss on epoch  120  :  5.02035080724 [0.27107653] [0.30541942]\n",
      "loss on epoch  121  :  4.95830894179 [0.27107653] [0.30570769]\n",
      "loss on epoch  122  :  4.89916391505 [0.26848248] [0.30642837]\n",
      "loss on epoch  123  :  4.84158392306 [0.2697795] [0.30815798]\n",
      "loss on epoch  124  :  4.78372862162 [0.27626458] [0.30801383]\n",
      "loss on epoch  125  :  4.72806891468 [0.27367055] [0.30859038]\n",
      "loss on epoch  126  :  4.67295415313 [0.27496758] [0.3083021]\n",
      "loss on epoch  127  :  4.61953446821 [0.27626458] [0.3116172]\n",
      "loss on epoch  128  :  4.56958765233 [0.27626458] [0.30959931]\n",
      "loss on epoch  129  :  4.51682622124 [0.2775616] [0.31204957]\n",
      "loss on epoch  130  :  4.4660634553 [0.28015563] [0.31176132]\n",
      "loss on epoch  131  :  4.41760200262 [0.28145266] [0.31320265]\n",
      "loss on epoch  132  :  4.3712131668 [0.27885863] [0.31493226]\n",
      "loss on epoch  133  :  4.32218551084 [0.28145266] [0.31680599]\n",
      "loss on epoch  134  :  4.27805671096 [0.28145266] [0.31752667]\n",
      "loss on epoch  135  :  4.23086421688 [0.28404668] [0.3194004]\n",
      "loss on epoch  136  :  4.18816401009 [0.28404668] [0.31968868]\n",
      "loss on epoch  137  :  4.14409664494 [0.28404668] [0.32141829]\n",
      "loss on epoch  138  :  4.10188903742 [0.28664073] [0.32098588]\n",
      "loss on epoch  139  :  4.06044249733 [0.28664073] [0.32242721]\n",
      "loss on epoch  140  :  4.01936358103 [0.28664073] [0.32386854]\n",
      "loss on epoch  141  :  3.97978386173 [0.28793773] [0.32358029]\n",
      "loss on epoch  142  :  3.93970244019 [0.28793773] [0.32415682]\n",
      "loss on epoch  143  :  3.90205098744 [0.28534371] [0.3253099]\n",
      "loss on epoch  144  :  3.86507606506 [0.28274968] [0.32646295]\n",
      "loss on epoch  145  :  3.82823021655 [0.28664073] [0.32819256]\n",
      "loss on epoch  146  :  3.78961248641 [0.28664073] [0.32718363]\n",
      "loss on epoch  147  :  3.75484512581 [0.28274968] [0.33006629]\n",
      "loss on epoch  148  :  3.72024091968 [0.28274968] [0.33121938]\n",
      "loss on epoch  149  :  3.68659115941 [0.28534371] [0.33107525]\n",
      "loss on epoch  150  :  3.653901141 [0.29053178] [0.33381379]\n",
      "loss on epoch  151  :  3.62173907293 [0.29182878] [0.33424619]\n",
      "loss on epoch  152  :  3.58779732166 [0.29701686] [0.33583164]\n",
      "loss on epoch  153  :  3.55668419048 [0.29312581] [0.3377054]\n",
      "loss on epoch  154  :  3.52534885429 [0.29053178] [0.33828193]\n",
      "loss on epoch  155  :  3.49528187844 [0.29053178] [0.33756125]\n",
      "loss on epoch  156  :  3.46516374857 [0.29182878] [0.33943498]\n",
      "loss on epoch  157  :  3.43534280636 [0.29182878] [0.33986738]\n",
      "loss on epoch  158  :  3.40771220569 [0.29442284] [0.33986738]\n",
      "loss on epoch  159  :  3.37879769007 [0.29053178] [0.34044394]\n",
      "loss on epoch  160  :  3.35120772322 [0.29312581] [0.34404728]\n",
      "loss on epoch  161  :  3.32249669124 [0.29053178] [0.3446238]\n",
      "loss on epoch  162  :  3.29680163993 [0.28793773] [0.34447968]\n",
      "loss on epoch  163  :  3.27006847218 [0.29442284] [0.34577689]\n",
      "loss on epoch  164  :  3.24298680822 [0.29312581] [0.3479389]\n",
      "loss on epoch  165  :  3.21676200959 [0.29312581] [0.34822714]\n",
      "loss on epoch  166  :  3.19207580001 [0.29442284] [0.34909195]\n",
      "loss on epoch  167  :  3.1680117117 [0.29312581] [0.3492361]\n",
      "loss on epoch  168  :  3.14488595945 [0.29442284] [0.3496685]\n",
      "loss on epoch  169  :  3.12030939261 [0.29182878] [0.35110983]\n",
      "loss on epoch  170  :  3.097040353 [0.29701686] [0.35197464]\n",
      "loss on epoch  171  :  3.07464027184 [0.29442284] [0.35399249]\n",
      "loss on epoch  172  :  3.05293730564 [0.30090791] [0.3557221]\n",
      "loss on epoch  173  :  3.02956266205 [0.29312581] [0.35701931]\n",
      "loss on epoch  174  :  3.00858436911 [0.29571983] [0.35961372]\n",
      "loss on epoch  175  :  2.98762723252 [0.30220494] [0.3565869]\n",
      "loss on epoch  176  :  2.96759591169 [0.29701686] [0.362064]\n",
      "loss on epoch  177  :  2.94638851175 [0.29182878] [0.35990199]\n",
      "loss on epoch  178  :  2.92755277731 [0.29571983] [0.36191985]\n",
      "loss on epoch  179  :  2.9060808067 [0.29053178] [0.36249641]\n",
      "loss on epoch  180  :  2.88628901155 [0.29312581] [0.36480254]\n",
      "loss on epoch  181  :  2.86656610171 [0.29831389] [0.36480254]\n",
      "loss on epoch  182  :  2.84756138369 [0.29961088] [0.36581147]\n",
      "loss on epoch  183  :  2.83026967998 [0.30090791] [0.36754107]\n",
      "loss on epoch  184  :  2.81123883746 [0.30479896] [0.36869416]\n",
      "loss on epoch  185  :  2.79391805772 [0.30350193] [0.36927068]\n",
      "loss on epoch  186  :  2.77638360085 [0.30350193] [0.3676852]\n",
      "loss on epoch  187  :  2.75921643994 [0.29701686] [0.37042376]\n",
      "loss on epoch  188  :  2.74116091817 [0.29701686] [0.37013549]\n",
      "loss on epoch  189  :  2.72412948034 [0.29571983] [0.37287402]\n",
      "loss on epoch  190  :  2.70763989952 [0.29961088] [0.37186509]\n",
      "loss on epoch  191  :  2.69171860152 [0.29961088] [0.37417123]\n",
      "loss on epoch  192  :  2.6756508836 [0.29442284] [0.37330642]\n",
      "loss on epoch  193  :  2.65903365722 [0.29312581] [0.37460363]\n",
      "loss on epoch  194  :  2.64340220778 [0.29831389] [0.37200922]\n",
      "loss on epoch  195  :  2.62767108723 [0.29831389] [0.37431535]\n",
      "loss on epoch  196  :  2.61219119363 [0.29831389] [0.37445951]\n",
      "loss on epoch  197  :  2.59769054364 [0.29701686] [0.37618911]\n",
      "loss on epoch  198  :  2.58307788438 [0.29831389] [0.37590083]\n",
      "loss on epoch  199  :  2.56794085105 [0.30350193] [0.37806284]\n",
      "loss on epoch  200  :  2.55406132892 [0.29831389] [0.37950417]\n",
      "loss on epoch  201  :  2.54055325742 [0.30220494] [0.38123378]\n",
      "loss on epoch  202  :  2.52673220745 [0.30479896] [0.38137791]\n",
      "loss on epoch  203  :  2.51301821514 [0.30739298] [0.38238686]\n",
      "loss on epoch  204  :  2.49937707296 [0.29831389] [0.38325167]\n",
      "loss on epoch  205  :  2.48647152163 [0.30739298] [0.38526952]\n",
      "loss on epoch  206  :  2.47340644951 [0.31517509] [0.38685501]\n",
      "loss on epoch  207  :  2.46039350276 [0.30869001] [0.38815221]\n",
      "loss on epoch  208  :  2.44759987681 [0.30350193] [0.38786393]\n",
      "loss on epoch  209  :  2.43480395277 [0.31387809] [0.38916114]\n",
      "loss on epoch  210  :  2.42452725768 [0.30869001] [0.38916114]\n",
      "loss on epoch  211  :  2.41167894116 [0.30998704] [0.38916114]\n",
      "loss on epoch  212  :  2.39918148076 [0.30479896] [0.38829634]\n",
      "loss on epoch  213  :  2.38764254252 [0.30998704] [0.39146727]\n",
      "loss on epoch  214  :  2.37547834604 [0.31128404] [0.38973767]\n",
      "loss on epoch  215  :  2.36529857914 [0.31128404] [0.39247623]\n",
      "loss on epoch  216  :  2.35337942176 [0.30609599] [0.39319688]\n",
      "loss on epoch  217  :  2.34239394245 [0.31128404] [0.39233208]\n",
      "loss on epoch  218  :  2.33224716562 [0.31128404] [0.39622369]\n",
      "loss on epoch  219  :  2.32052140324 [0.31387809] [0.39636782]\n",
      "loss on epoch  220  :  2.31133999869 [0.31128404] [0.39449409]\n",
      "loss on epoch  221  :  2.30028442321 [0.31128404] [0.39521477]\n",
      "loss on epoch  222  :  2.29025526676 [0.31258106] [0.39723262]\n",
      "loss on epoch  223  :  2.27986424278 [0.30998704] [0.39852983]\n",
      "loss on epoch  224  :  2.27087910087 [0.31128404] [0.39737678]\n",
      "loss on epoch  225  :  2.25979267116 [0.31128404] [0.39997119]\n",
      "loss on epoch  226  :  2.251401218 [0.30998704] [0.39867398]\n",
      "loss on epoch  227  :  2.2424614286 [0.31517509] [0.39939463]\n",
      "loss on epoch  228  :  2.23226678095 [0.31517509] [0.39867398]\n",
      "loss on epoch  229  :  2.22304082524 [0.31128404] [0.39910638]\n",
      "loss on epoch  230  :  2.21486625131 [0.31387809] [0.39953879]\n",
      "loss on epoch  231  :  2.20435879628 [0.31517509] [0.40083596]\n",
      "loss on epoch  232  :  2.19644305717 [0.31387809] [0.40054771]\n",
      "loss on epoch  233  :  2.18735276659 [0.31517509] [0.40155664]\n",
      "loss on epoch  234  :  2.17905307423 [0.31517509] [0.40285385]\n",
      "loss on epoch  235  :  2.17105096965 [0.31258106] [0.40328625]\n",
      "loss on epoch  236  :  2.16241900016 [0.31647211] [0.40400693]\n",
      "loss on epoch  237  :  2.15424863608 [0.31128404] [0.40285385]\n",
      "loss on epoch  238  :  2.14624053461 [0.31517509] [0.40443933]\n",
      "loss on epoch  239  :  2.13767550113 [0.31647211] [0.40703374]\n",
      "loss on epoch  240  :  2.13027279134 [0.31517509] [0.40631306]\n",
      "loss on epoch  241  :  2.12195364855 [0.31387809] [0.40703374]\n",
      "loss on epoch  242  :  2.11434825261 [0.31258106] [0.40717787]\n",
      "loss on epoch  243  :  2.10629076318 [0.31258106] [0.40717787]\n",
      "loss on epoch  244  :  2.09901970349 [0.31517509] [0.40660134]\n",
      "loss on epoch  245  :  2.09175575331 [0.31647211] [0.40876332]\n",
      "loss on epoch  246  :  2.08381771048 [0.31387809] [0.4086192]\n",
      "loss on epoch  247  :  2.07554246264 [0.31387809] [0.4090516]\n",
      "loss on epoch  248  :  2.06945769213 [0.31647211] [0.40818679]\n",
      "loss on epoch  249  :  2.06311147412 [0.32295719] [0.40919572]\n",
      "loss on epoch  250  :  2.05449651734 [0.31776914] [0.4086192]\n",
      "loss on epoch  251  :  2.04813010052 [0.32425421] [0.41251081]\n",
      "loss on epoch  252  :  2.04170541796 [0.32166019] [0.41092533]\n",
      "loss on epoch  253  :  2.03490534149 [0.32036316] [0.4107812]\n",
      "loss on epoch  254  :  2.02813856359 [0.32555124] [0.4107812]\n",
      "loss on epoch  255  :  2.02244405393 [0.32166019] [0.41308734]\n",
      "loss on epoch  256  :  2.01632688774 [0.32295719] [0.41063708]\n",
      "loss on epoch  257  :  2.00931505528 [0.32944229] [0.41481695]\n",
      "loss on epoch  258  :  2.00378062659 [0.32555124] [0.41553763]\n",
      "loss on epoch  259  :  1.99637307889 [0.32425421] [0.41366389]\n",
      "loss on epoch  260  :  1.99109990784 [0.32295719] [0.41467282]\n",
      "loss on epoch  261  :  1.9850727546 [0.31776914] [0.41539347]\n",
      "loss on epoch  262  :  1.97924959273 [0.32684824] [0.41640243]\n",
      "loss on epoch  263  :  1.97407714195 [0.32425421] [0.41741136]\n",
      "loss on epoch  264  :  1.96756388137 [0.32166019] [0.41712308]\n",
      "loss on epoch  265  :  1.96192571024 [0.32425421] [0.41885269]\n",
      "loss on epoch  266  :  1.95676076302 [0.32425421] [0.4210147]\n",
      "loss on epoch  267  :  1.95080249343 [0.32425421] [0.41870856]\n",
      "loss on epoch  268  :  1.94452604541 [0.33333334] [0.42043817]\n",
      "loss on epoch  269  :  1.93978881229 [0.32944229] [0.42173538]\n",
      "loss on epoch  270  :  1.93458827061 [0.32684824] [0.42389739]\n",
      "loss on epoch  271  :  1.92809495164 [0.32814527] [0.4214471]\n",
      "loss on epoch  272  :  1.92379298972 [0.32814527] [0.42332083]\n",
      "loss on epoch  273  :  1.91982908271 [0.32944229] [0.42577112]\n",
      "loss on epoch  274  :  1.91499463828 [0.32555124] [0.4264918]\n",
      "loss on epoch  275  :  1.90925785182 [0.33203632] [0.42461804]\n",
      "loss on epoch  276  :  1.90432762989 [0.32944229] [0.42577112]\n",
      "loss on epoch  277  :  1.89906985947 [0.32555124] [0.42721245]\n",
      "loss on epoch  278  :  1.89493006026 [0.32295719] [0.42548284]\n",
      "loss on epoch  279  :  1.8906108588 [0.32555124] [0.42750072]\n",
      "loss on epoch  280  :  1.88541840127 [0.32425421] [0.42634764]\n",
      "loss on epoch  281  :  1.88026708861 [0.32166019] [0.42966273]\n",
      "loss on epoch  282  :  1.87583712902 [0.32425421] [0.42706832]\n",
      "loss on epoch  283  :  1.87125184801 [0.32814527] [0.43023926]\n",
      "loss on epoch  284  :  1.86565808952 [0.32684824] [0.43211299]\n",
      "loss on epoch  285  :  1.86120500995 [0.32814527] [0.42951858]\n",
      "loss on epoch  286  :  1.85741420918 [0.32425421] [0.43110406]\n",
      "loss on epoch  287  :  1.85324029293 [0.32814527] [0.43182474]\n",
      "loss on epoch  288  :  1.84941080544 [0.32684824] [0.43182474]\n",
      "loss on epoch  289  :  1.84500144532 [0.32944229] [0.43268955]\n",
      "loss on epoch  290  :  1.84086220794 [0.32425421] [0.43398675]\n",
      "loss on epoch  291  :  1.83641671638 [0.33073929] [0.43182474]\n",
      "loss on epoch  292  :  1.83277281401 [0.32944229] [0.43254539]\n",
      "loss on epoch  293  :  1.82715596479 [0.32166019] [0.43542808]\n",
      "loss on epoch  294  :  1.82394846612 [0.33463034] [0.43542808]\n",
      "loss on epoch  295  :  1.82072582234 [0.32684824] [0.43211299]\n",
      "loss on epoch  296  :  1.8166465097 [0.33203632] [0.43643701]\n",
      "loss on epoch  297  :  1.81238035195 [0.32036316] [0.43629289]\n",
      "loss on epoch  298  :  1.80939723882 [0.33073929] [0.43730181]\n",
      "loss on epoch  299  :  1.80604813883 [0.33981842] [0.43658113]\n",
      "loss on epoch  300  :  1.80221569207 [0.32684824] [0.43759009]\n",
      "loss on epoch  301  :  1.79795306976 [0.33592737] [0.43816662]\n",
      "loss on epoch  302  :  1.79454519407 [0.32814527] [0.43773422]\n",
      "loss on epoch  303  :  1.79161442927 [0.33333334] [0.4393197]\n",
      "loss on epoch  304  :  1.78812323676 [0.33592737] [0.4397521]\n",
      "loss on epoch  305  :  1.78443230247 [0.33722439] [0.44004035]\n",
      "loss on epoch  306  :  1.7808151052 [0.33203632] [0.44104931]\n",
      "loss on epoch  307  :  1.77773834655 [0.33203632] [0.44047275]\n",
      "loss on epoch  308  :  1.77383182777 [0.33073929] [0.44263476]\n",
      "loss on epoch  309  :  1.77039167947 [0.33592737] [0.44292304]\n",
      "loss on epoch  310  :  1.76731598929 [0.33203632] [0.4401845]\n",
      "loss on epoch  311  :  1.76450578261 [0.33592737] [0.44306716]\n",
      "loss on epoch  312  :  1.76151370009 [0.33981842] [0.44263476]\n",
      "loss on epoch  313  :  1.75835605021 [0.34111544] [0.44162583]\n",
      "loss on epoch  314  :  1.75528414382 [0.33722439] [0.44176996]\n",
      "loss on epoch  315  :  1.75273565838 [0.34241244] [0.44220236]\n",
      "loss on epoch  316  :  1.74884888971 [0.33592737] [0.44292304]\n",
      "loss on epoch  317  :  1.74568944618 [0.33852139] [0.44349957]\n",
      "loss on epoch  318  :  1.7414748321 [0.33722439] [0.44335544]\n",
      "loss on epoch  319  :  1.73993199015 [0.33592737] [0.44436437]\n",
      "loss on epoch  320  :  1.73730400149 [0.33981842] [0.44494089]\n",
      "loss on epoch  321  :  1.73419008156 [0.33852139] [0.44494089]\n",
      "loss on epoch  322  :  1.73101975741 [0.33722439] [0.44508505]\n",
      "loss on epoch  323  :  1.72812047435 [0.33852139] [0.44393197]\n",
      "loss on epoch  324  :  1.72545222 [0.33722439] [0.44479677]\n",
      "loss on epoch  325  :  1.72197805676 [0.34111544] [0.4462381]\n",
      "loss on epoch  326  :  1.7198825622 [0.33852139] [0.44724706]\n",
      "loss on epoch  327  :  1.71645400149 [0.32944229] [0.4458057]\n",
      "loss on epoch  328  :  1.71509467231 [0.33981842] [0.4466705]\n",
      "loss on epoch  329  :  1.71159779032 [0.3450065] [0.44796771]\n",
      "loss on epoch  330  :  1.70940228248 [0.34111544] [0.44825599]\n",
      "loss on epoch  331  :  1.70609358339 [0.34241244] [0.44825599]\n",
      "loss on epoch  332  :  1.70423837392 [0.33981842] [0.44926491]\n",
      "loss on epoch  333  :  1.70069034839 [0.33852139] [0.45070624]\n",
      "loss on epoch  334  :  1.69781808445 [0.33722439] [0.44984144]\n",
      "loss on epoch  335  :  1.69652261613 [0.33463034] [0.45142692]\n",
      "loss on epoch  336  :  1.69373513648 [0.33981842] [0.45229173]\n",
      "loss on epoch  337  :  1.69135646412 [0.34241244] [0.45272413]\n",
      "loss on epoch  338  :  1.68880100824 [0.33981842] [0.45330065]\n",
      "loss on epoch  339  :  1.68706926593 [0.33333334] [0.45258]\n",
      "loss on epoch  340  :  1.68422426283 [0.33981842] [0.45315653]\n",
      "loss on epoch  341  :  1.68213395774 [0.33852139] [0.45474201]\n",
      "loss on epoch  342  :  1.67881525501 [0.33073929] [0.45358893]\n",
      "loss on epoch  343  :  1.67775055601 [0.33592737] [0.45301241]\n",
      "loss on epoch  344  :  1.67378309866 [0.33722439] [0.45503026]\n",
      "loss on epoch  345  :  1.67258938595 [0.33463034] [0.45748055]\n",
      "loss on epoch  346  :  1.67036321097 [0.32425421] [0.45531854]\n",
      "loss on epoch  347  :  1.66882040434 [0.33073929] [0.45575094]\n",
      "loss on epoch  348  :  1.66596087537 [0.32684824] [0.45258]\n",
      "loss on epoch  349  :  1.66464182238 [0.32295719] [0.45315653]\n",
      "loss on epoch  350  :  1.66166419895 [0.33722439] [0.45603919]\n",
      "loss on epoch  351  :  1.66000309587 [0.32814527] [0.45531854]\n",
      "loss on epoch  352  :  1.65762382139 [0.33463034] [0.45704815]\n",
      "loss on epoch  353  :  1.65490904947 [0.32166019] [0.45993081]\n",
      "loss on epoch  354  :  1.65282759733 [0.33463034] [0.46021909]\n",
      "loss on epoch  355  :  1.65054569863 [0.33073929] [0.45993081]\n",
      "loss on epoch  356  :  1.64834071751 [0.32814527] [0.45949841]\n",
      "loss on epoch  357  :  1.64701004878 [0.33333334] [0.46093976]\n",
      "loss on epoch  358  :  1.64571634542 [0.33203632] [0.4632459]\n",
      "loss on epoch  359  :  1.64324456122 [0.33073929] [0.4628135]\n",
      "loss on epoch  360  :  1.64079171254 [0.32555124] [0.4641107]\n",
      "loss on epoch  361  :  1.63951899553 [0.33203632] [0.46353415]\n",
      "loss on epoch  362  :  1.63713303595 [0.32814527] [0.46310174]\n",
      "loss on epoch  363  :  1.63446433897 [0.33203632] [0.4645431]\n",
      "loss on epoch  364  :  1.63230339079 [0.32944229] [0.46511963]\n",
      "loss on epoch  365  :  1.63084078994 [0.32036316] [0.4636783]\n",
      "loss on epoch  366  :  1.63040175372 [0.32166019] [0.46584031]\n",
      "loss on epoch  367  :  1.6273695098 [0.31906614] [0.46439895]\n",
      "loss on epoch  368  :  1.62611433422 [0.33333334] [0.46483135]\n",
      "loss on epoch  369  :  1.62364801047 [0.32295719] [0.46627271]\n",
      "loss on epoch  370  :  1.62258308464 [0.32166019] [0.46439895]\n",
      "loss on epoch  371  :  1.62004279097 [0.32555124] [0.4692995]\n",
      "loss on epoch  372  :  1.61799377093 [0.31906614] [0.46252522]\n",
      "loss on epoch  373  :  1.6159345458 [0.32425421] [0.46886712]\n",
      "loss on epoch  374  :  1.61413554682 [0.32425421] [0.46627271]\n",
      "loss on epoch  375  :  1.6134826276 [0.32166019] [0.47030845]\n",
      "loss on epoch  376  :  1.6115203853 [0.32425421] [0.47002017]\n",
      "loss on epoch  377  :  1.60936283072 [0.32425421] [0.47088498]\n",
      "loss on epoch  378  :  1.60782221622 [0.32166019] [0.4710291]\n",
      "loss on epoch  379  :  1.60699492417 [0.32555124] [0.46901125]\n",
      "loss on epoch  380  :  1.60390686051 [0.32036316] [0.46886712]\n",
      "loss on epoch  381  :  1.60378159472 [0.32166019] [0.47232631]\n",
      "loss on epoch  382  :  1.60167578635 [0.32036316] [0.47232631]\n",
      "loss on epoch  383  :  1.59957604055 [0.32555124] [0.4710291]\n",
      "loss on epoch  384  :  1.59826849677 [0.32425421] [0.47232631]\n",
      "loss on epoch  385  :  1.59678287031 [0.31776914] [0.46915537]\n",
      "loss on epoch  386  :  1.59496464994 [0.32684824] [0.47347939]\n",
      "loss on epoch  387  :  1.59312129738 [0.32944229] [0.47319111]\n",
      "loss on epoch  388  :  1.59190445862 [0.31776914] [0.47189391]\n",
      "loss on epoch  389  :  1.59094813025 [0.32036316] [0.47045258]\n",
      "loss on epoch  390  :  1.58758304185 [0.32036316] [0.47189391]\n",
      "loss on epoch  391  :  1.58763212904 [0.33073929] [0.47376767]\n",
      "loss on epoch  392  :  1.58532668374 [0.31906614] [0.47275871]\n",
      "loss on epoch  393  :  1.58333402099 [0.32814527] [0.47405592]\n",
      "loss on epoch  394  :  1.58248247317 [0.33073929] [0.47780341]\n",
      "loss on epoch  395  :  1.5810183662 [0.32036316] [0.47492072]\n",
      "loss on epoch  396  :  1.57852987062 [0.32036316] [0.4765062]\n",
      "loss on epoch  397  :  1.57776630421 [0.31906614] [0.47506487]\n",
      "loss on epoch  398  :  1.57674587877 [0.32036316] [0.4756414]\n",
      "loss on epoch  399  :  1.57497515667 [0.32425421] [0.4765062]\n",
      "loss on epoch  400  :  1.57258259036 [0.32166019] [0.4747766]\n",
      "loss on epoch  401  :  1.57225413896 [0.32036316] [0.47376767]\n",
      "loss on epoch  402  :  1.57026547801 [0.32555124] [0.47910061]\n",
      "loss on epoch  403  :  1.56870693024 [0.32036316] [0.47535312]\n",
      "loss on epoch  404  :  1.56895802529 [0.32166019] [0.47895646]\n",
      "loss on epoch  405  :  1.56660045352 [0.32166019] [0.47866821]\n",
      "loss on epoch  406  :  1.56581609448 [0.32166019] [0.48126262]\n",
      "loss on epoch  407  :  1.56424539398 [0.32166019] [0.47837994]\n",
      "loss on epoch  408  :  1.56280976479 [0.32036316] [0.48140675]\n",
      "loss on epoch  409  :  1.5609113414 [0.32166019] [0.48054194]\n",
      "loss on epoch  410  :  1.55950927734 [0.31776914] [0.48140675]\n",
      "loss on epoch  411  :  1.55891501186 [0.32944229] [0.48198327]\n",
      "loss on epoch  412  :  1.55784671285 [0.31906614] [0.48140675]\n",
      "loss on epoch  413  :  1.55604671935 [0.32036316] [0.48255983]\n",
      "loss on epoch  414  :  1.5545757435 [0.32036316] [0.48529837]\n",
      "loss on epoch  415  :  1.55454390082 [0.32166019] [0.48428941]\n",
      "loss on epoch  416  :  1.55098889934 [0.32425421] [0.48645142]\n",
      "loss on epoch  417  :  1.5508046956 [0.31517509] [0.48457769]\n",
      "loss on epoch  418  :  1.54930949211 [0.31906614] [0.48501009]\n",
      "loss on epoch  419  :  1.54723380009 [0.32944229] [0.48573077]\n",
      "loss on epoch  420  :  1.54762646446 [0.32166019] [0.4867397]\n",
      "loss on epoch  421  :  1.54588373502 [0.32295719] [0.48601902]\n",
      "loss on epoch  422  :  1.54446121609 [0.32295719] [0.4884693]\n",
      "loss on epoch  423  :  1.54252775345 [0.32036316] [0.48746037]\n",
      "loss on epoch  424  :  1.54177720127 [0.33073929] [0.4880369]\n",
      "loss on epoch  425  :  1.54117601779 [0.32944229] [0.4884693]\n",
      "loss on epoch  426  :  1.53936908146 [0.32295719] [0.48702797]\n",
      "loss on epoch  427  :  1.53858129404 [0.31776914] [0.4876045]\n",
      "loss on epoch  428  :  1.53698675886 [0.32944229] [0.4880369]\n",
      "loss on epoch  429  :  1.5358720488 [0.32555124] [0.48832518]\n",
      "loss on epoch  430  :  1.53484253365 [0.32944229] [0.48904583]\n",
      "loss on epoch  431  :  1.5340697942 [0.32295719] [0.4884693]\n",
      "loss on epoch  432  :  1.53286256669 [0.32555124] [0.49149612]\n",
      "loss on epoch  433  :  1.53077636714 [0.32814527] [0.49019891]\n",
      "loss on epoch  434  :  1.52960212142 [0.31776914] [0.4884693]\n",
      "loss on epoch  435  :  1.52949869909 [0.32166019] [0.49063131]\n",
      "loss on epoch  436  :  1.52750867109 [0.32295719] [0.49091956]\n",
      "loss on epoch  437  :  1.52656730402 [0.32036316] [0.49135196]\n",
      "loss on epoch  438  :  1.52557366866 [0.32684824] [0.49192852]\n",
      "loss on epoch  439  :  1.52365994288 [0.31776914] [0.49135196]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 1118, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 300, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 1044, in getinnerframes\n",
      "    framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python2.7/inspect.py\", line 1004, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 454, in getsourcefile\n",
      "    if hasattr(getmodule(object, filename), '__loader__'):\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 490, in getmodule\n",
      "    for modname, module in sys.modules.items():\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unfortunately, your original traceback can not be constructed.\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[1;34m(self, code_obj, result)\u001b[0m\n\u001b[0;32m   2900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2902\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2903\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2904\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[0;32m   1828\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1829\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[1;32m-> 1830\u001b[1;33m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[0;32m   1831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1832\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1390\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[1;32m-> 1392\u001b[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1298\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[1;32m-> 1300\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1301\u001b[0m             )\n\u001b[0;32m   1302\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 \u001b[0mstructured_traceback_parts\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m             \u001b[0mstructured_traceback_parts\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstructured_traceback_parts\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:Uncaught exception, closing connection.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 396, in execute_request\n",
      "    self._abort_queues()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 614, in _abort_queues\n",
      "    self._abort_queue(stream)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 620, in _abort_queue\n",
      "    idents,msg = self.session.recv(stream, zmq.NOBLOCK, content=True)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/jupyter_client/session.py\", line 731, in recv\n",
      "    msg_list = socket.recv_multipart(mode, copy=copy)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/sugar/socket.py\", line 358, in recv_multipart\n",
      "    parts = [self.recv(flags, copy=copy, track=track)]\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 674, in zmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:6971)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 708, in zmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:6763)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 145, in zmq.backend.cython.socket._recv_copy (zmq/backend/cython/socket.c:1931)\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 19, in zmq.backend.cython.checkrc._check_rc (zmq/backend/cython/socket.c:7367)\n",
      "    raise Again(errno)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/error.py\", line 26, in __init__\n",
      "    def __init__(self, errno=None, msg=None):\n",
      "KeyboardInterrupt\n",
      "ERROR:tornado.general:Uncaught exception, closing connection.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 396, in execute_request\n",
      "    self._abort_queues()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 614, in _abort_queues\n",
      "    self._abort_queue(stream)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 620, in _abort_queue\n",
      "    idents,msg = self.session.recv(stream, zmq.NOBLOCK, content=True)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/jupyter_client/session.py\", line 731, in recv\n",
      "    msg_list = socket.recv_multipart(mode, copy=copy)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/sugar/socket.py\", line 358, in recv_multipart\n",
      "    parts = [self.recv(flags, copy=copy, track=track)]\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 674, in zmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:6971)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 708, in zmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:6763)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 145, in zmq.backend.cython.socket._recv_copy (zmq/backend/cython/socket.c:1931)\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 19, in zmq.backend.cython.checkrc._check_rc (zmq/backend/cython/socket.c:7367)\n",
      "    raise Again(errno)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/error.py\", line 26, in __init__\n",
      "    def __init__(self, errno=None, msg=None):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "#   representation ability\n",
    "para_n_hidden_list = [ 128,64,8 ]\n",
    "#     128,16,8 ]\n",
    "para_lr = 0.05\n",
    "para_n_embedding = 2\n",
    "#   regularization\n",
    "para_batch_size = 32\n",
    "para_keep_prob = 1.0\n",
    "para_l2 = 0.15\n",
    "\n",
    "# fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = 8\n",
    "\n",
    "tot_cnt= np.shape(cxtrain)[0]\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    wide_mlp_clf = wide_embed_NN( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                      para_n_disc_voca, sess, para_n_embedding, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2 )\n",
    "    wide_mlp_clf.train_ini()\n",
    "    wide_mlp_clf.inference_ini()\n",
    "    \n",
    "    para_cur_lr=para_lr\n",
    "    total_batch = int(tot_cnt/para_batch_size)\n",
    "\n",
    "    tot_idx=range(tot_cnt)\n",
    "\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc=0.0\n",
    "        \n",
    "#       shuffle traning dataset each epoch  \n",
    "        np.random.shuffle(tot_idx)\n",
    "   \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = tot_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += wide_mlp_clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                       para_cur_lr)\n",
    "        \n",
    "#       learning rate decaying\n",
    "#         para_cur_lr = para_cur_lr*0.98\n",
    "        \n",
    "        tmp_test_acc = wide_mlp_clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = wide_mlp_clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
