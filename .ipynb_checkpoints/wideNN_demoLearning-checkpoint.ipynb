{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TO DO:\n",
    "\n",
    "# ordinal \n",
    "# categorical+continuous\n",
    "\n",
    "# feature interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from six.moves import urllib\n",
    "# from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6938, 186) (771, 186) (6938, 1) (771, 1)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "files_list=[\"../dataset/dataset_demo/xtrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/xtest.csv\",\\\n",
    "            \"../dataset/dataset_demo/ytrain.csv\", \\\n",
    "            \"../dataset/dataset_demo/ytest.csv\"]\n",
    "\n",
    "xtrain_df=pd.read_csv( files_list[0] ,sep=',', header=None)\n",
    "xtest_df=pd.read_csv( files_list[1] ,sep=',', header=None)\n",
    "ytrain_df=pd.read_csv( files_list[2] ,sep=',', header=None)\n",
    "ytest_df=pd.read_csv( files_list[3] ,sep=',', header=None)\n",
    "\n",
    "print xtrain_df.shape, xtest_df.shape, ytrain_df.shape, ytest_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# continuous and categorical feature split\n",
    "\n",
    "# features on columns [0:split_idx] are continuous\n",
    "# the rest are categorical\n",
    "def conti_cate_split(dta_df, split_idx):\n",
    "    \n",
    "    col_cnt= dta_df.shape[1]\n",
    "    conti_cols= range(split_idx)\n",
    "    dis_cols=range(split_idx, col_cnt)\n",
    "    \n",
    "    return dta_df[conti_cols], dta_df[dis_cols]\n",
    "\n",
    "def conti_normalization_train_dta(dta_df):\n",
    "    \n",
    "    return preprocessing.scale(dta_df)\n",
    "\n",
    "def conti_normalization_test_dta(dta_df, train_df):\n",
    "    \n",
    "    mean_dim = np.mean(train_df, axis=0)\n",
    "    std_dim = np.std(train_df, axis=0)\n",
    "        \n",
    "    df=pd.DataFrame()\n",
    "    cols = train_df.columns\n",
    "    idx=0\n",
    "    \n",
    "    for i in cols:\n",
    "        df[i] = (dta_df[i]- mean_dim[idx])*1.0/std_dim[idx]\n",
    "        idx=idx+1\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6938, 106) (6938, 80)\n",
      "(771, 106) (771, 80)\n"
     ]
    }
   ],
   "source": [
    "# get training and testing data prepared\n",
    "\n",
    "cxtrain, dxtrain = conti_cate_split(xtrain_df, 106)\n",
    "cxtest, dxtest = conti_cate_split(xtest_df, 106)\n",
    "\n",
    "cxtest = conti_normalization_test_dta(cxtest, cxtrain)\n",
    "cxtrain = conti_normalization_train_dta(cxtrain)\n",
    "\n",
    "print cxtrain.shape, dxtrain.shape\n",
    "print cxtest.shape, dxtest.shape\n",
    "\n",
    "# cxtrain = cxtrain.as_matrix() \n",
    "dxtrain = dxtrain.as_matrix().astype(int)\n",
    "cxtest = cxtest.as_matrix()\n",
    "dxtest = dxtest.as_matrix().astype(int)\n",
    "\n",
    "ytest = np_utils.to_categorical( ytest_df.as_matrix() )\n",
    "ytrain =np_utils.to_categorical( ytrain_df.as_matrix() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# demoMLP with embedding\n",
    "\n",
    "class mlp_demo():\n",
    "    \n",
    "    def __init__(self, batch_size, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, n_embedding, session, n_hidden_list, lr, l2):\n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "        self.BATCH_SIZE = batch_size\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.N_EMBED = n_embedding\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list) \n",
    "        self.N_TOTAL = self.N_CONTI+ self.N_DISC*self.N_EMBED\n",
    "        \n",
    "        self.sess=session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "#       embedding categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "        \n",
    "        tmp_embeded = []                \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"disc\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_normal([self.N_DISC_VOCA, self.N_EMBED],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "#                     tf.random_uniform([self.N_DISC_VOCA, self.N_EMBED],-1.0, 1.0 )\n",
    "                              \n",
    "                tmp_embeded.append( tf.nn.embedding_lookup( w, self.dx_trans[i] ) )\n",
    "                \n",
    "        dx_embeded = tf.concat(tmp_embeded, 1)\n",
    "        x_concate =  tf.concat( [self.cx,dx_embeded], 1 )\n",
    "\n",
    "#----unit test---------------------------------------------------------\n",
    "#         print self.N_TOTAL\n",
    "#         self.tmpshape= tf.shape(x_concate)\n",
    "        \n",
    "#         self.tmpshape1= tf.shape(tmp_embeded[0])\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# #     hidden layers\n",
    "        with tf.variable_scope(\"h0\"):\n",
    "            w= tf.Variable(tf.random_normal([self.N_TOTAL, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_TOTAL)))) \n",
    "            \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            \n",
    "            h = tf.nn.relu( tf.add( tf.matmul(x_concate, w),b) )\n",
    "        \n",
    "        \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"layer\"+str(i)):\n",
    "                \n",
    "                w= tf.Variable(tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                \n",
    "                b= tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                \n",
    "                h = tf.nn.relu( tf.add( tf.matmul(h, w),b) )\n",
    "        \n",
    "#   Regularization  \n",
    "#       dropout\n",
    "#         h = tf.nn.dropout(h, self.keep_prob)\n",
    "\n",
    "#       L2\n",
    "        self.regularizer = tf.nn.l2_loss(w)\n",
    "    \n",
    "        \n",
    "#       output layer  \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w= tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1],\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "                \n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            \n",
    "            h = tf.add( tf.matmul(h, w),b)\n",
    "            \n",
    "#           L2  \n",
    "            self.regularizer = self.regularizer + tf.nn.l2_loss(w)\n",
    "            \n",
    "            \n",
    "            self.logit = h\n",
    "    \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        \n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y) \\\n",
    "                                   )\n",
    "#                                   + self.L2*self.regularizer)\n",
    "        \n",
    "        self.optimizer = \\\n",
    "        tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                       feed_dict={self.dx:dx_batch, \\\n",
    "                                  self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                  self.lr:lr,\\\n",
    "                                  self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "    \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        \n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                  self.cx:cx_test, self.y:y_test,\\\n",
    "                                                  self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "        return sess.run( [self.tmpshape], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch  0  :  2.38683597247 [0.21141374] [0.21432689]\n",
      "loss on epoch  1  :  2.33484052287 [0.20752271] [0.21187662]\n",
      "loss on epoch  2  :  2.29102125874 [0.20492867] [0.20899394]\n",
      "loss on epoch  3  :  2.25421280773 [0.20881972] [0.20942634]\n",
      "loss on epoch  4  :  2.21984007623 [0.20622568] [0.20957048]\n",
      "loss on epoch  5  :  2.19144645444 [0.20492867] [0.20769674]\n",
      "loss on epoch  6  :  2.16408797105 [0.19714656] [0.20668781]\n",
      "loss on epoch  7  :  2.13968978988 [0.19844358] [0.20870568]\n",
      "loss on epoch  8  :  2.1175829415 [0.19844358] [0.21014702]\n",
      "loss on epoch  9  :  2.09760991953 [0.19455253] [0.21245316]\n",
      "loss on epoch  10  :  2.07961383352 [0.20622568] [0.21115595]\n",
      "loss on epoch  11  :  2.06185996974 [0.20363165] [0.21144421]\n",
      "loss on epoch  12  :  2.04706894468 [0.20363165] [0.2125973]\n",
      "loss on epoch  13  :  2.03176831978 [0.20492867] [0.21447103]\n",
      "loss on epoch  14  :  2.01803360603 [0.20622568] [0.22052465]\n",
      "loss on epoch  15  :  2.00506127764 [0.21660182] [0.22081292]\n",
      "loss on epoch  16  :  1.99285805888 [0.22308689] [0.22571346]\n",
      "loss on epoch  17  :  1.98097107808 [0.22438392] [0.22874027]\n",
      "loss on epoch  18  :  1.97000235761 [0.22308689] [0.23234361]\n",
      "loss on epoch  19  :  1.95956661304 [0.22568093] [0.23248775]\n",
      "loss on epoch  20  :  1.94970772222 [0.23346303] [0.23551455]\n",
      "loss on epoch  21  :  1.94014357417 [0.2464332] [0.23637936]\n",
      "loss on epoch  22  :  1.93075416265 [0.25032425] [0.23940617]\n",
      "loss on epoch  23  :  1.92187268425 [0.25551233] [0.23998271]\n",
      "loss on epoch  24  :  1.91356019621 [0.25162128] [0.24300951]\n",
      "loss on epoch  25  :  1.90556872333 [0.24773023] [0.24618046]\n",
      "loss on epoch  26  :  1.89803940058 [0.2542153] [0.2493514]\n",
      "loss on epoch  27  :  1.89128723409 [0.25291827] [0.2528106]\n",
      "loss on epoch  28  :  1.88271389846 [0.25291827] [0.25425196]\n",
      "loss on epoch  29  :  1.87792060773 [0.25291827] [0.25468436]\n",
      "loss on epoch  30  :  1.8713587655 [0.2542153] [0.25554916]\n",
      "loss on epoch  31  :  1.86531226282 [0.2542153] [0.25699049]\n",
      "loss on epoch  32  :  1.85902471454 [0.25291827] [0.25857595]\n",
      "loss on epoch  33  :  1.85479133217 [0.25680932] [0.25843182]\n",
      "loss on epoch  34  :  1.84936845082 [0.26070037] [0.26044971]\n",
      "loss on epoch  35  :  1.84471391307 [0.26070037] [0.26117036]\n",
      "loss on epoch  36  :  1.83904177613 [0.26718548] [0.26217932]\n",
      "loss on epoch  37  :  1.83449841208 [0.26329443] [0.26362064]\n",
      "loss on epoch  38  :  1.83021077845 [0.2619974] [0.26578265]\n",
      "loss on epoch  39  :  1.82706818978 [0.26459143] [0.26679158]\n",
      "loss on epoch  40  :  1.82270970168 [0.26848248] [0.26823291]\n",
      "loss on epoch  41  :  1.81911044871 [0.26329443] [0.2706832]\n",
      "loss on epoch  42  :  1.81535310657 [0.25940338] [0.27313346]\n",
      "loss on epoch  43  :  1.81248789143 [0.25551233] [0.27212453]\n",
      "loss on epoch  44  :  1.80924058181 [0.25551233] [0.27385414]\n",
      "loss on epoch  45  :  1.80560923506 [0.25680932] [0.27630442]\n",
      "loss on epoch  46  :  1.80248471984 [0.25940338] [0.27731335]\n",
      "loss on epoch  47  :  1.79979804269 [0.2619974] [0.2784664]\n",
      "loss on epoch  48  :  1.79620449852 [0.26070037] [0.27976361]\n",
      "loss on epoch  49  :  1.79402755587 [0.26070037] [0.28149322]\n",
      "loss on epoch  50  :  1.79001624054 [0.2619974] [0.28206977]\n",
      "loss on epoch  51  :  1.78818947518 [0.26329443] [0.28408763]\n",
      "loss on epoch  52  :  1.78621228315 [0.26329443] [0.28379938]\n",
      "loss on epoch  53  :  1.78368073481 [0.26329443] [0.2839435]\n",
      "loss on epoch  54  :  1.78073641768 [0.26459143] [0.28552896]\n",
      "loss on epoch  55  :  1.77910766116 [0.26459143] [0.28682616]\n",
      "loss on epoch  56  :  1.77511994265 [0.26588845] [0.28639376]\n",
      "loss on epoch  57  :  1.77413551896 [0.26718548] [0.28682616]\n",
      "loss on epoch  58  :  1.77157510651 [0.26848248] [0.28783512]\n",
      "loss on epoch  59  :  1.76864161977 [0.26459143] [0.28855577]\n",
      "loss on epoch  60  :  1.76701756098 [0.26459143] [0.28927645]\n",
      "loss on epoch  61  :  1.76584341791 [0.26459143] [0.28999713]\n",
      "loss on epoch  62  :  1.76299340637 [0.26588845] [0.28942057]\n",
      "loss on epoch  63  :  1.76086150275 [0.26848248] [0.29057366]\n",
      "loss on epoch  64  :  1.75998639619 [0.27107653] [0.29143846]\n",
      "loss on epoch  65  :  1.75664603269 [0.27107653] [0.29230326]\n",
      "loss on epoch  66  :  1.75567041724 [0.27367055] [0.29316807]\n",
      "loss on epoch  67  :  1.75303078802 [0.27237353] [0.29345632]\n",
      "loss on epoch  68  :  1.75200857057 [0.27367055] [0.29360047]\n",
      "loss on epoch  69  :  1.74986141479 [0.27367055] [0.29345632]\n",
      "loss on epoch  70  :  1.74827383403 [0.2775616] [0.29446527]\n",
      "loss on epoch  71  :  1.74663090485 [0.27885863] [0.2946094]\n",
      "loss on epoch  72  :  1.74494770059 [0.27496758] [0.29446527]\n",
      "loss on epoch  73  :  1.74200538353 [0.27496758] [0.294177]\n",
      "loss on epoch  74  :  1.74113344925 [0.27367055] [0.2946094]\n",
      "loss on epoch  75  :  1.73886423641 [0.27367055] [0.29518592]\n",
      "loss on epoch  76  :  1.73721796716 [0.27367055] [0.29489768]\n",
      "loss on epoch  77  :  1.7352469431 [0.27367055] [0.2950418]\n",
      "loss on epoch  78  :  1.7354215229 [0.27367055] [0.29576248]\n",
      "loss on epoch  79  :  1.73268113534 [0.27496758] [0.29691553]\n",
      "loss on epoch  80  :  1.73234554573 [0.27626458] [0.29720381]\n",
      "loss on epoch  81  :  1.72926168089 [0.27626458] [0.29734793]\n",
      "loss on epoch  82  :  1.72754912685 [0.28145266] [0.29778033]\n",
      "loss on epoch  83  :  1.72634085682 [0.28015563] [0.29850101]\n",
      "loss on epoch  84  :  1.72494656951 [0.28145266] [0.29907754]\n",
      "loss on epoch  85  :  1.72404714425 [0.28015563] [0.29994234]\n",
      "loss on epoch  86  :  1.72179291866 [0.27885863] [0.30051887]\n",
      "loss on epoch  87  :  1.72137846329 [0.28015563] [0.30066302]\n",
      "loss on epoch  88  :  1.71964679382 [0.27885863] [0.30152783]\n",
      "loss on epoch  89  :  1.71839455322 [0.2775616] [0.30224848]\n",
      "loss on epoch  90  :  1.71740949816 [0.2775616] [0.30196023]\n",
      "loss on epoch  91  :  1.7150693293 [0.28015563] [0.30181608]\n",
      "loss on epoch  92  :  1.71470895961 [0.28145266] [0.30296916]\n",
      "loss on epoch  93  :  1.71177572674 [0.28274968] [0.30354568]\n",
      "loss on epoch  94  :  1.71131008422 [0.28404668] [0.30426636]\n",
      "loss on epoch  95  :  1.71001232774 [0.28274968] [0.30527529]\n",
      "loss on epoch  96  :  1.70836965243 [0.28145266] [0.30570769]\n",
      "loss on epoch  97  :  1.70636783927 [0.28274968] [0.30671662]\n",
      "loss on epoch  98  :  1.70597664294 [0.28404668] [0.30686077]\n",
      "loss on epoch  99  :  1.70502481196 [0.28404668] [0.30671662]\n",
      "loss on epoch  100  :  1.70330654471 [0.28274968] [0.30714902]\n",
      "loss on epoch  101  :  1.70155258532 [0.28145266] [0.3078697]\n",
      "loss on epoch  102  :  1.7004651339 [0.28015563] [0.3083021]\n",
      "loss on epoch  103  :  1.69993671444 [0.28015563] [0.30873451]\n",
      "loss on epoch  104  :  1.69826369595 [0.28015563] [0.30916691]\n",
      "loss on epoch  105  :  1.69811559386 [0.28015563] [0.30873451]\n",
      "loss on epoch  106  :  1.69544496139 [0.27885863] [0.30873451]\n",
      "loss on epoch  107  :  1.69417325876 [0.27885863] [0.30859038]\n",
      "loss on epoch  108  :  1.69363672424 [0.28015563] [0.30859038]\n",
      "loss on epoch  109  :  1.69220531208 [0.28015563] [0.30916691]\n",
      "loss on epoch  110  :  1.6908987319 [0.28015563] [0.30974343]\n",
      "loss on epoch  111  :  1.68993333975 [0.28145266] [0.31003171]\n",
      "loss on epoch  112  :  1.68838139375 [0.28015563] [0.31003171]\n",
      "loss on epoch  113  :  1.68875782137 [0.28145266] [0.31060824]\n",
      "loss on epoch  114  :  1.68657445908 [0.28015563] [0.31046411]\n",
      "loss on epoch  115  :  1.68611536203 [0.28015563] [0.31075239]\n",
      "loss on epoch  116  :  1.68444969036 [0.28145266] [0.31219372]\n",
      "loss on epoch  117  :  1.6827846037 [0.28015563] [0.31305853]\n",
      "loss on epoch  118  :  1.68186343378 [0.28145266] [0.31262612]\n",
      "loss on epoch  119  :  1.68127686448 [0.28015563] [0.31305853]\n",
      "loss on epoch  120  :  1.68010564645 [0.28015563] [0.31305853]\n",
      "loss on epoch  121  :  1.67887003113 [0.28015563] [0.31277025]\n",
      "loss on epoch  122  :  1.67817162143 [0.27885863] [0.31377918]\n",
      "loss on epoch  123  :  1.67660976339 [0.27885863] [0.31406745]\n",
      "loss on epoch  124  :  1.67545907365 [0.28015563] [0.31449986]\n",
      "loss on epoch  125  :  1.67420467403 [0.28015563] [0.31493226]\n",
      "loss on epoch  126  :  1.67372649687 [0.28145266] [0.31522053]\n",
      "loss on epoch  127  :  1.67280202442 [0.28145266] [0.31507638]\n",
      "loss on epoch  128  :  1.6716675935 [0.28145266] [0.31478813]\n",
      "loss on epoch  129  :  1.67107750531 [0.27885863] [0.31622946]\n",
      "loss on epoch  130  :  1.66962887843 [0.27626458] [0.31723839]\n",
      "loss on epoch  131  :  1.66818800458 [0.27626458] [0.31738254]\n",
      "loss on epoch  132  :  1.66776977866 [0.2775616] [0.31752667]\n",
      "loss on epoch  133  :  1.66707567153 [0.2775616] [0.31810319]\n",
      "loss on epoch  134  :  1.66459621544 [0.27885863] [0.31824735]\n",
      "loss on epoch  135  :  1.66471791267 [0.28274968] [0.31882387]\n",
      "loss on epoch  136  :  1.66428028433 [0.28274968] [0.31954452]\n",
      "loss on epoch  137  :  1.66421324677 [0.28274968] [0.3198328]\n",
      "loss on epoch  138  :  1.66210926683 [0.28145266] [0.32012108]\n",
      "loss on epoch  139  :  1.66094769813 [0.28145266] [0.32055348]\n",
      "loss on epoch  140  :  1.66032295095 [0.28145266] [0.32040933]\n",
      "loss on epoch  141  :  1.66073385212 [0.28145266] [0.32113001]\n",
      "loss on epoch  142  :  1.6575738302 [0.28015563] [0.32213894]\n",
      "loss on epoch  143  :  1.65747664151 [0.28145266] [0.32213894]\n",
      "loss on epoch  144  :  1.65691734243 [0.28145266] [0.32228309]\n",
      "loss on epoch  145  :  1.65559697813 [0.28015563] [0.32213894]\n",
      "loss on epoch  146  :  1.65505203053 [0.28015563] [0.32242721]\n",
      "loss on epoch  147  :  1.65378062151 [0.28015563] [0.32300374]\n",
      "loss on epoch  148  :  1.65320034822 [0.28015563] [0.32300374]\n",
      "loss on epoch  149  :  1.65248686958 [0.28015563] [0.32314789]\n",
      "loss on epoch  150  :  1.65144641532 [0.28015563] [0.32358029]\n",
      "loss on epoch  151  :  1.6503266935 [0.28015563] [0.3244451]\n",
      "loss on epoch  152  :  1.6501450362 [0.27885863] [0.32473335]\n",
      "loss on epoch  153  :  1.64888462755 [0.28015563] [0.32458922]\n",
      "loss on epoch  154  :  1.6479745441 [0.28015563] [0.3248775]\n",
      "loss on epoch  155  :  1.64748242387 [0.28015563] [0.32473335]\n",
      "loss on epoch  156  :  1.6466493452 [0.28145266] [0.32559815]\n",
      "loss on epoch  157  :  1.6446238712 [0.28145266] [0.3253099]\n",
      "loss on epoch  158  :  1.64455991542 [0.28145266] [0.32516575]\n",
      "loss on epoch  159  :  1.64458622314 [0.28015563] [0.3253099]\n",
      "loss on epoch  160  :  1.64324535264 [0.28145266] [0.32516575]\n",
      "loss on epoch  161  :  1.64303599684 [0.28015563] [0.32458922]\n",
      "loss on epoch  162  :  1.64194594268 [0.28145266] [0.32458922]\n",
      "loss on epoch  163  :  1.64087046959 [0.28015563] [0.32516575]\n",
      "loss on epoch  164  :  1.64003618779 [0.28015563] [0.32574227]\n",
      "loss on epoch  165  :  1.63955256012 [0.28015563] [0.32603055]\n",
      "loss on epoch  166  :  1.63902243641 [0.28145266] [0.32617468]\n",
      "loss on epoch  167  :  1.63775515115 [0.28274968] [0.32617468]\n",
      "loss on epoch  168  :  1.63758386727 [0.28145266] [0.32703948]\n",
      "loss on epoch  169  :  1.63605901268 [0.28145266] [0.32703948]\n",
      "loss on epoch  170  :  1.63581969782 [0.28145266] [0.32776016]\n",
      "loss on epoch  171  :  1.63508956962 [0.28274968] [0.32790428]\n",
      "loss on epoch  172  :  1.6341154752 [0.28145266] [0.32833669]\n",
      "loss on epoch  173  :  1.63298546385 [0.28274968] [0.32848084]\n",
      "loss on epoch  174  :  1.63223267926 [0.28274968] [0.32905737]\n",
      "loss on epoch  175  :  1.6307945119 [0.28274968] [0.32934564]\n",
      "loss on epoch  176  :  1.63131087356 [0.28274968] [0.32963389]\n",
      "loss on epoch  177  :  1.6307048621 [0.28404668] [0.33006629]\n",
      "loss on epoch  178  :  1.62987942828 [0.28404668] [0.33006629]\n",
      "loss on epoch  179  :  1.62940076987 [0.28404668] [0.33035457]\n",
      "loss on epoch  180  :  1.62811690569 [0.28274968] [0.33078697]\n",
      "loss on epoch  181  :  1.62647137598 [0.28145266] [0.3309311]\n",
      "loss on epoch  182  :  1.62655591302 [0.28274968] [0.33107525]\n",
      "loss on epoch  183  :  1.62526843504 [0.28274968] [0.33121938]\n",
      "loss on epoch  184  :  1.62470070521 [0.28145266] [0.33194005]\n",
      "loss on epoch  185  :  1.62420895365 [0.28145266] [0.33165178]\n",
      "loss on epoch  186  :  1.62342488545 [0.28145266] [0.33194005]\n",
      "loss on epoch  187  :  1.62253382471 [0.28274968] [0.33194005]\n",
      "loss on epoch  188  :  1.62297470261 [0.28274968] [0.33208418]\n",
      "loss on epoch  189  :  1.62177525626 [0.28274968] [0.3326607]\n",
      "loss on epoch  190  :  1.62129816744 [0.28404668] [0.33280483]\n",
      "loss on epoch  191  :  1.6205880068 [0.28274968] [0.33323723]\n",
      "loss on epoch  192  :  1.61850104729 [0.28274968] [0.33381379]\n",
      "loss on epoch  193  :  1.61886852317 [0.28145266] [0.33410203]\n",
      "loss on epoch  194  :  1.61820501531 [0.28145266] [0.33410203]\n",
      "loss on epoch  195  :  1.6182567411 [0.28404668] [0.33467859]\n",
      "loss on epoch  196  :  1.61669557404 [0.28404668] [0.33511099]\n",
      "loss on epoch  197  :  1.61661857587 [0.28274968] [0.33568752]\n",
      "loss on epoch  198  :  1.61581369683 [0.28274968] [0.33611992]\n",
      "loss on epoch  199  :  1.61518875096 [0.28274968] [0.33655232]\n",
      "loss on epoch  200  :  1.61340737343 [0.28404668] [0.33655232]\n",
      "loss on epoch  201  :  1.61361249288 [0.28404668] [0.3368406]\n",
      "loss on epoch  202  :  1.61330256859 [0.28664073] [0.337273]\n",
      "loss on epoch  203  :  1.61189106659 [0.28534371] [0.3377054]\n",
      "loss on epoch  204  :  1.61108403736 [0.28404668] [0.33813781]\n",
      "loss on epoch  205  :  1.61084876679 [0.28534371] [0.33842605]\n",
      "loss on epoch  206  :  1.61006564785 [0.28534371] [0.33885846]\n",
      "loss on epoch  207  :  1.60950847908 [0.28534371] [0.33900261]\n",
      "loss on epoch  208  :  1.6088819261 [0.28534371] [0.33885846]\n",
      "loss on epoch  209  :  1.60902355998 [0.28534371] [0.33957914]\n",
      "loss on epoch  210  :  1.60764494207 [0.28274968] [0.33957914]\n",
      "loss on epoch  211  :  1.60736373177 [0.28404668] [0.34001154]\n",
      "loss on epoch  212  :  1.6074218176 [0.28534371] [0.34073219]\n",
      "loss on epoch  213  :  1.60662185042 [0.28404668] [0.34130874]\n",
      "loss on epoch  214  :  1.60511124796 [0.28923476] [0.34159699]\n",
      "loss on epoch  215  :  1.60409835754 [0.28534371] [0.34202939]\n",
      "loss on epoch  216  :  1.60425352167 [0.28793773] [0.34202939]\n",
      "loss on epoch  217  :  1.60372017931 [0.28793773] [0.34202939]\n",
      "loss on epoch  218  :  1.60338318127 [0.28923476] [0.34246179]\n",
      "loss on epoch  219  :  1.60198373485 [0.28534371] [0.34260595]\n",
      "loss on epoch  220  :  1.60183335675 [0.28923476] [0.34318247]\n",
      "loss on epoch  221  :  1.60120081239 [0.28923476] [0.34361488]\n",
      "loss on epoch  222  :  1.60037804091 [0.29053178] [0.34347075]\n",
      "loss on epoch  223  :  1.59938808282 [0.28664073] [0.3441914]\n",
      "loss on epoch  224  :  1.59955177925 [0.28923476] [0.34447968]\n",
      "loss on epoch  225  :  1.59932626398 [0.28923476] [0.34491208]\n",
      "loss on epoch  226  :  1.59754629268 [0.28793773] [0.34520036]\n",
      "loss on epoch  227  :  1.59770279019 [0.29053178] [0.34534448]\n",
      "loss on epoch  228  :  1.59681948247 [0.29053178] [0.34592101]\n",
      "loss on epoch  229  :  1.59563127933 [0.29053178] [0.34592101]\n",
      "loss on epoch  230  :  1.59630575003 [0.29053178] [0.34620929]\n",
      "loss on epoch  231  :  1.59513066875 [0.29053178] [0.34678581]\n",
      "loss on epoch  232  :  1.59451952908 [0.29182878] [0.34721822]\n",
      "loss on epoch  233  :  1.59324548863 [0.29182878] [0.34750649]\n",
      "loss on epoch  234  :  1.59384836532 [0.29182878] [0.3479389]\n",
      "loss on epoch  235  :  1.59266894614 [0.29571983] [0.3479389]\n",
      "loss on epoch  236  :  1.59196948564 [0.29442284] [0.34851542]\n",
      "loss on epoch  237  :  1.59168112057 [0.29571983] [0.34909195]\n",
      "loss on epoch  238  :  1.5907826534 [0.29571983] [0.34894782]\n",
      "loss on epoch  239  :  1.59059550806 [0.29571983] [0.34952435]\n",
      "loss on epoch  240  :  1.58992750336 [0.29571983] [0.34938022]\n",
      "loss on epoch  241  :  1.59018394682 [0.29571983] [0.3496685]\n",
      "loss on epoch  242  :  1.58905116276 [0.29701686] [0.34995675]\n",
      "loss on epoch  243  :  1.58811849356 [0.29571983] [0.35053331]\n",
      "loss on epoch  244  :  1.5871981868 [0.29571983] [0.35038915]\n",
      "loss on epoch  245  :  1.58700905244 [0.29571983] [0.35053331]\n",
      "loss on epoch  246  :  1.58634710091 [0.29571983] [0.35110983]\n",
      "loss on epoch  247  :  1.5864898739 [0.29701686] [0.35125396]\n",
      "loss on epoch  248  :  1.58550526919 [0.29701686] [0.35139811]\n",
      "loss on epoch  249  :  1.58363725521 [0.29701686] [0.35211876]\n",
      "loss on epoch  250  :  1.58422905427 [0.29701686] [0.35269532]\n",
      "loss on epoch  251  :  1.58377068131 [0.29701686] [0.35283944]\n",
      "loss on epoch  252  :  1.58379848798 [0.29831389] [0.35298356]\n",
      "loss on epoch  253  :  1.58209623452 [0.29831389] [0.35327184]\n",
      "loss on epoch  254  :  1.58220943257 [0.29831389] [0.35341597]\n",
      "loss on epoch  255  :  1.58162575298 [0.29831389] [0.35341597]\n",
      "loss on epoch  256  :  1.58168253192 [0.29831389] [0.35356009]\n",
      "loss on epoch  257  :  1.58096801572 [0.29831389] [0.35370424]\n",
      "loss on epoch  258  :  1.57932741995 [0.29961088] [0.35399249]\n",
      "loss on epoch  259  :  1.57995701278 [0.30090791] [0.35471317]\n",
      "loss on epoch  260  :  1.57826285451 [0.30220494] [0.3548573]\n",
      "loss on epoch  261  :  1.57864292021 [0.30220494] [0.3552897]\n",
      "loss on epoch  262  :  1.57883896872 [0.30350193] [0.35586625]\n",
      "loss on epoch  263  :  1.5773894831 [0.30350193] [0.35629866]\n",
      "loss on epoch  264  :  1.57730061699 [0.30479896] [0.35687518]\n",
      "loss on epoch  265  :  1.57660230442 [0.30479896] [0.35701931]\n",
      "loss on epoch  266  :  1.57672866627 [0.30609599] [0.35716346]\n",
      "loss on epoch  267  :  1.57615586784 [0.30609599] [0.35745171]\n",
      "loss on epoch  268  :  1.57551790608 [0.30609599] [0.35802826]\n",
      "loss on epoch  269  :  1.57457178169 [0.30739298] [0.35817239]\n",
      "loss on epoch  270  :  1.57305086984 [0.30739298] [0.35817239]\n",
      "loss on epoch  271  :  1.57397902012 [0.30479896] [0.35889307]\n",
      "loss on epoch  272  :  1.57354828163 [0.30479896] [0.35918131]\n",
      "loss on epoch  273  :  1.57230110963 [0.30479896] [0.35932547]\n",
      "loss on epoch  274  :  1.57058136551 [0.30479896] [0.36019024]\n",
      "loss on epoch  275  :  1.57194248173 [0.30479896] [0.36019024]\n",
      "loss on epoch  276  :  1.57033629771 [0.30479896] [0.36091092]\n",
      "loss on epoch  277  :  1.57024627262 [0.30479896] [0.36134332]\n",
      "loss on epoch  278  :  1.57034965798 [0.30479896] [0.36148745]\n",
      "loss on epoch  279  :  1.56943378846 [0.30479896] [0.36235225]\n",
      "loss on epoch  280  :  1.56939579381 [0.30609599] [0.36249641]\n",
      "loss on epoch  281  :  1.56817883253 [0.30609599] [0.36278465]\n",
      "loss on epoch  282  :  1.56749845434 [0.30739298] [0.36350533]\n",
      "loss on epoch  283  :  1.56747385749 [0.30739298] [0.36393774]\n",
      "loss on epoch  284  :  1.5667883224 [0.30739298] [0.36465842]\n",
      "loss on epoch  285  :  1.56509732096 [0.30609599] [0.36523494]\n",
      "loss on epoch  286  :  1.56558575895 [0.30609599] [0.36537907]\n",
      "loss on epoch  287  :  1.5659684583 [0.30609599] [0.36624387]\n",
      "loss on epoch  288  :  1.56461301777 [0.30609599] [0.36595562]\n",
      "loss on epoch  289  :  1.56533322952 [0.30479896] [0.36653215]\n",
      "loss on epoch  290  :  1.56360697084 [0.30479896] [0.36667627]\n",
      "loss on epoch  291  :  1.56309884345 [0.30609599] [0.36682039]\n",
      "loss on epoch  292  :  1.56351290809 [0.30479896] [0.36710867]\n",
      "loss on epoch  293  :  1.56301320924 [0.30609599] [0.36710867]\n",
      "loss on epoch  294  :  1.56292140263 [0.30609599] [0.36739695]\n",
      "loss on epoch  295  :  1.56177728264 [0.30609599] [0.3676852]\n",
      "loss on epoch  296  :  1.56077693348 [0.30609599] [0.3681176]\n",
      "loss on epoch  297  :  1.56121396356 [0.30609599] [0.36869416]\n",
      "loss on epoch  298  :  1.56017465062 [0.30739298] [0.36927068]\n",
      "loss on epoch  299  :  1.55935030293 [0.30739298] [0.36970308]\n",
      "loss on epoch  300  :  1.55874280797 [0.30739298] [0.36984721]\n",
      "loss on epoch  301  :  1.55853522928 [0.30739298] [0.36999136]\n",
      "loss on epoch  302  :  1.55727235918 [0.30739298] [0.37027961]\n",
      "loss on epoch  303  :  1.557539101 [0.30739298] [0.37042376]\n",
      "loss on epoch  304  :  1.55790822815 [0.30739298] [0.37128857]\n",
      "loss on epoch  305  :  1.55725246006 [0.30739298] [0.37128857]\n",
      "loss on epoch  306  :  1.5572987552 [0.30869001] [0.37186509]\n",
      "loss on epoch  307  :  1.55467542233 [0.30998704] [0.37200922]\n",
      "loss on epoch  308  :  1.55527680008 [0.31128404] [0.3722975]\n",
      "loss on epoch  309  :  1.55488554195 [0.31128404] [0.37287402]\n",
      "loss on epoch  310  :  1.55500114847 [0.31128404] [0.3731623]\n",
      "loss on epoch  311  :  1.55325473679 [0.31258106] [0.37388295]\n",
      "loss on epoch  312  :  1.55415264103 [0.31258106] [0.3740271]\n",
      "loss on epoch  313  :  1.55389113117 [0.31258106] [0.37431535]\n",
      "loss on epoch  314  :  1.55212496387 [0.31387809] [0.37460363]\n",
      "loss on epoch  315  :  1.55156711075 [0.31387809] [0.37489191]\n",
      "loss on epoch  316  :  1.55105729015 [0.31387809] [0.37546843]\n",
      "loss on epoch  317  :  1.55155337961 [0.31517509] [0.37561256]\n",
      "loss on epoch  318  :  1.55076215665 [0.31387809] [0.37604496]\n",
      "loss on epoch  319  :  1.55025467166 [0.31517509] [0.37604496]\n",
      "loss on epoch  320  :  1.5500782684 [0.31387809] [0.37676564]\n",
      "loss on epoch  321  :  1.54951512593 [0.31387809] [0.37705392]\n",
      "loss on epoch  322  :  1.54901258151 [0.31387809] [0.37719804]\n",
      "loss on epoch  323  :  1.54863518918 [0.31258106] [0.37748632]\n",
      "loss on epoch  324  :  1.5481348126 [0.31258106] [0.37763044]\n",
      "loss on epoch  325  :  1.54738514512 [0.31258106] [0.37777457]\n",
      "loss on epoch  326  :  1.54691820233 [0.31258106] [0.37835112]\n",
      "loss on epoch  327  :  1.54693756501 [0.31258106] [0.37892765]\n",
      "loss on epoch  328  :  1.54635631817 [0.31258106] [0.37907177]\n",
      "loss on epoch  329  :  1.54595815915 [0.31258106] [0.37936005]\n",
      "loss on epoch  330  :  1.54521486936 [0.31258106] [0.37950417]\n",
      "loss on epoch  331  :  1.54501970388 [0.31258106] [0.38008073]\n",
      "loss on epoch  332  :  1.54501946988 [0.31258106] [0.38036898]\n",
      "loss on epoch  333  :  1.54423332656 [0.31258106] [0.37993658]\n",
      "loss on epoch  334  :  1.54383068615 [0.31387809] [0.3809455]\n",
      "loss on epoch  335  :  1.5430788155 [0.31387809] [0.38108966]\n",
      "loss on epoch  336  :  1.54271443906 [0.31517509] [0.38123378]\n",
      "loss on epoch  337  :  1.54161002901 [0.31258106] [0.38123378]\n",
      "loss on epoch  338  :  1.54168104242 [0.31387809] [0.38123378]\n",
      "loss on epoch  339  :  1.54164301687 [0.31387809] [0.38166618]\n",
      "loss on epoch  340  :  1.54107858517 [0.31517509] [0.38166618]\n",
      "loss on epoch  341  :  1.54047969536 [0.31517509] [0.38195446]\n",
      "loss on epoch  342  :  1.53945408486 [0.31387809] [0.38267511]\n",
      "loss on epoch  343  :  1.54014862025 [0.31387809] [0.38267511]\n",
      "loss on epoch  344  :  1.53941136599 [0.31387809] [0.38281927]\n",
      "loss on epoch  345  :  1.53854193952 [0.31647211] [0.38267511]\n",
      "loss on epoch  346  :  1.53827798367 [0.31647211] [0.38339579]\n",
      "loss on epoch  347  :  1.53737365979 [0.31647211] [0.38382819]\n",
      "loss on epoch  348  :  1.53754663688 [0.31517509] [0.38411647]\n",
      "loss on epoch  349  :  1.53595925702 [0.31647211] [0.38411647]\n",
      "loss on epoch  350  :  1.53650255777 [0.31647211] [0.38411647]\n",
      "loss on epoch  351  :  1.53578709894 [0.31517509] [0.38426059]\n",
      "loss on epoch  352  :  1.53639665136 [0.31517509] [0.38498127]\n",
      "loss on epoch  353  :  1.53577768361 [0.31517509] [0.38454887]\n",
      "loss on epoch  354  :  1.53597546948 [0.31387809] [0.38498127]\n",
      "loss on epoch  355  :  1.53523662797 [0.31517509] [0.3855578]\n",
      "loss on epoch  356  :  1.5341489602 [0.31776914] [0.38526952]\n",
      "loss on epoch  357  :  1.53390421912 [0.31647211] [0.3855578]\n",
      "loss on epoch  358  :  1.53235761104 [0.31517509] [0.38570192]\n",
      "loss on epoch  359  :  1.53257660292 [0.31517509] [0.3855578]\n",
      "loss on epoch  360  :  1.53206310449 [0.31517509] [0.38613433]\n",
      "loss on epoch  361  :  1.53155185779 [0.31517509] [0.38656673]\n",
      "loss on epoch  362  :  1.53047380624 [0.31517509] [0.38699913]\n",
      "loss on epoch  363  :  1.53129855792 [0.31517509] [0.38743153]\n",
      "loss on epoch  364  :  1.53071362222 [0.31387809] [0.38771981]\n",
      "loss on epoch  365  :  1.52986091375 [0.31517509] [0.38743153]\n",
      "loss on epoch  366  :  1.53097165514 [0.31387809] [0.38800806]\n",
      "loss on epoch  367  :  1.5294770201 [0.31387809] [0.38771981]\n",
      "loss on epoch  368  :  1.52891365908 [0.31647211] [0.38757566]\n",
      "loss on epoch  369  :  1.52861573961 [0.31647211] [0.38771981]\n",
      "loss on epoch  370  :  1.52841073495 [0.31647211] [0.38771981]\n",
      "loss on epoch  371  :  1.52826157543 [0.31647211] [0.38771981]\n",
      "loss on epoch  372  :  1.52720811411 [0.31517509] [0.38829634]\n",
      "loss on epoch  373  :  1.52697017679 [0.31517509] [0.38858461]\n",
      "loss on epoch  374  :  1.52679978477 [0.31517509] [0.38887286]\n",
      "loss on epoch  375  :  1.52654002331 [0.31517509] [0.38887286]\n",
      "loss on epoch  376  :  1.5264278849 [0.31647211] [0.38944942]\n",
      "loss on epoch  377  :  1.52460934498 [0.31647211] [0.38973767]\n",
      "loss on epoch  378  :  1.52381637803 [0.31647211] [0.39002594]\n",
      "loss on epoch  379  :  1.52391538797 [0.31647211] [0.39089075]\n",
      "loss on epoch  380  :  1.52411442995 [0.31647211] [0.39074662]\n",
      "loss on epoch  381  :  1.52346845909 [0.31647211] [0.39060247]\n",
      "loss on epoch  382  :  1.52310000526 [0.31647211] [0.39132315]\n",
      "loss on epoch  383  :  1.52316748213 [0.31647211] [0.39189968]\n",
      "loss on epoch  384  :  1.52252386014 [0.31647211] [0.39175555]\n",
      "loss on epoch  385  :  1.52168986974 [0.31647211] [0.39218795]\n",
      "loss on epoch  386  :  1.52186718031 [0.31647211] [0.39189968]\n",
      "loss on epoch  387  :  1.52031860307 [0.31387809] [0.39247623]\n",
      "loss on epoch  388  :  1.52083810391 [0.31258106] [0.39348516]\n",
      "loss on epoch  389  :  1.52065128971 [0.31128404] [0.39319688]\n",
      "loss on epoch  390  :  1.51993918419 [0.31128404] [0.39334103]\n",
      "loss on epoch  391  :  1.51931139275 [0.31128404] [0.39362928]\n",
      "loss on epoch  392  :  1.51884012752 [0.31258106] [0.39434996]\n",
      "loss on epoch  393  :  1.51896869695 [0.31128404] [0.39478236]\n",
      "loss on epoch  394  :  1.51806600889 [0.31517509] [0.39507061]\n",
      "loss on epoch  395  :  1.51704448682 [0.31258106] [0.39593542]\n",
      "loss on epoch  396  :  1.51744316242 [0.31258106] [0.39607957]\n",
      "loss on epoch  397  :  1.51737739422 [0.31258106] [0.39651197]\n",
      "loss on epoch  398  :  1.51618547793 [0.31387809] [0.39651197]\n",
      "loss on epoch  399  :  1.51614182084 [0.31258106] [0.39636782]\n",
      "loss on epoch  400  :  1.51603542655 [0.31258106] [0.3966561]\n",
      "loss on epoch  401  :  1.51640509897 [0.31387809] [0.39694437]\n",
      "loss on epoch  402  :  1.51465305134 [0.31258106] [0.3975209]\n",
      "loss on epoch  403  :  1.51430331557 [0.31258106] [0.3975209]\n",
      "loss on epoch  404  :  1.51508862442 [0.31517509] [0.39766502]\n",
      "loss on epoch  405  :  1.51332321653 [0.31647211] [0.39809743]\n",
      "loss on epoch  406  :  1.51354058584 [0.31517509] [0.39867398]\n",
      "loss on epoch  407  :  1.51187832929 [0.31517509] [0.39867398]\n",
      "loss on epoch  408  :  1.51301188381 [0.31387809] [0.39910638]\n",
      "loss on epoch  409  :  1.5123082289 [0.31258106] [0.39939463]\n",
      "loss on epoch  410  :  1.51222802754 [0.31387809] [0.39953879]\n",
      "loss on epoch  411  :  1.51154421877 [0.31387809] [0.39939463]\n",
      "loss on epoch  412  :  1.51031156602 [0.31387809] [0.40040359]\n",
      "loss on epoch  413  :  1.51129679989 [0.31517509] [0.40083596]\n",
      "loss on epoch  414  :  1.51091924862 [0.31517509] [0.40141252]\n",
      "loss on epoch  415  :  1.50990089664 [0.31647211] [0.40098011]\n",
      "loss on epoch  416  :  1.50935795351 [0.31647211] [0.40155664]\n",
      "loss on epoch  417  :  1.50964990589 [0.31647211] [0.40155664]\n",
      "loss on epoch  418  :  1.50830990518 [0.31647211] [0.40184492]\n",
      "loss on epoch  419  :  1.50889032417 [0.31647211] [0.40184492]\n",
      "loss on epoch  420  :  1.50751281226 [0.31647211] [0.40198904]\n",
      "loss on epoch  421  :  1.50755096144 [0.31647211] [0.40227732]\n",
      "loss on epoch  422  :  1.50709916706 [0.31647211] [0.40256557]\n",
      "loss on epoch  423  :  1.50730291782 [0.31647211] [0.40242144]\n",
      "loss on epoch  424  :  1.50594386348 [0.31517509] [0.40314212]\n",
      "loss on epoch  425  :  1.50634532505 [0.31517509] [0.40314212]\n",
      "loss on epoch  426  :  1.5064677 [0.31517509] [0.40314212]\n",
      "loss on epoch  427  :  1.50484936105 [0.31517509] [0.40328625]\n",
      "loss on epoch  428  :  1.50407224231 [0.31517509] [0.40343037]\n",
      "loss on epoch  429  :  1.50516642685 [0.31647211] [0.40386277]\n",
      "loss on epoch  430  :  1.50438865247 [0.31647211] [0.40415105]\n",
      "loss on epoch  431  :  1.50358762564 [0.31647211] [0.40458345]\n",
      "loss on epoch  432  :  1.5037432909 [0.31387809] [0.40501586]\n",
      "loss on epoch  433  :  1.50280999917 [0.31517509] [0.40515998]\n",
      "loss on epoch  434  :  1.50198235556 [0.31517509] [0.40530413]\n",
      "loss on epoch  435  :  1.50198073078 [0.31517509] [0.40544826]\n",
      "loss on epoch  436  :  1.50215175858 [0.31387809] [0.40602478]\n",
      "loss on epoch  437  :  1.50168961728 [0.31517509] [0.40660134]\n",
      "loss on epoch  438  :  1.50130813872 [0.31517509] [0.40602478]\n",
      "loss on epoch  439  :  1.50041380414 [0.31387809] [0.40616894]\n",
      "loss on epoch  440  :  1.50037484699 [0.31387809] [0.40703374]\n",
      "loss on epoch  441  :  1.49980153861 [0.31517509] [0.40674546]\n",
      "loss on epoch  442  :  1.49988645315 [0.31517509] [0.40732199]\n",
      "loss on epoch  443  :  1.49910493692 [0.31387809] [0.40746614]\n",
      "loss on epoch  444  :  1.49855755214 [0.31387809] [0.40789852]\n",
      "loss on epoch  445  :  1.49880286279 [0.31258106] [0.40761027]\n",
      "loss on epoch  446  :  1.49788422055 [0.31387809] [0.40833092]\n",
      "loss on epoch  447  :  1.49702529333 [0.31258106] [0.40847507]\n",
      "loss on epoch  448  :  1.49704236013 [0.31258106] [0.40919572]\n",
      "loss on epoch  449  :  1.49689498213 [0.31387809] [0.4090516]\n",
      "loss on epoch  450  :  1.49718088795 [0.31128404] [0.4090516]\n",
      "loss on epoch  451  :  1.49678059198 [0.31128404] [0.40977228]\n",
      "loss on epoch  452  :  1.49571603095 [0.31258106] [0.41020468]\n",
      "loss on epoch  453  :  1.49610447221 [0.31128404] [0.41020468]\n",
      "loss on epoch  454  :  1.49492360707 [0.31258106] [0.4099164]\n",
      "loss on epoch  455  :  1.49514756379 [0.31258106] [0.4103488]\n",
      "loss on epoch  456  :  1.49351697719 [0.31258106] [0.4103488]\n",
      "loss on epoch  457  :  1.4942187998 [0.31128404] [0.4107812]\n",
      "loss on epoch  458  :  1.49417584914 [0.31258106] [0.41063708]\n",
      "loss on epoch  459  :  1.4927907564 [0.31258106] [0.41106948]\n",
      "loss on epoch  460  :  1.49282051016 [0.31387809] [0.41092533]\n",
      "loss on epoch  461  :  1.49268193598 [0.31258106] [0.41106948]\n",
      "loss on epoch  462  :  1.4920135317 [0.31128404] [0.41150188]\n",
      "loss on epoch  463  :  1.49282887688 [0.31128404] [0.41222253]\n",
      "loss on epoch  464  :  1.49154411422 [0.31387809] [0.41265494]\n",
      "loss on epoch  465  :  1.49158527012 [0.31258106] [0.41236669]\n",
      "loss on epoch  466  :  1.49056341913 [0.31258106] [0.41294321]\n",
      "loss on epoch  467  :  1.49013549752 [0.31258106] [0.41351974]\n",
      "loss on epoch  468  :  1.4894921007 [0.31258106] [0.41323149]\n",
      "loss on epoch  469  :  1.48994053514 [0.31387809] [0.41395214]\n",
      "loss on epoch  470  :  1.48903507877 [0.31387809] [0.41452867]\n",
      "loss on epoch  471  :  1.48905717002 [0.31387809] [0.41438454]\n",
      "loss on epoch  472  :  1.489827719 [0.31387809] [0.41452867]\n",
      "loss on epoch  473  :  1.48846637099 [0.31387809] [0.41481695]\n",
      "loss on epoch  474  :  1.48773180555 [0.31387809] [0.41481695]\n",
      "loss on epoch  475  :  1.48716865204 [0.31387809] [0.41496107]\n",
      "loss on epoch  476  :  1.48666261081 [0.31387809] [0.41510522]\n",
      "loss on epoch  477  :  1.48669693647 [0.31387809] [0.41510522]\n",
      "loss on epoch  478  :  1.48617070693 [0.31387809] [0.41597003]\n",
      "loss on epoch  479  :  1.48610234481 [0.31387809] [0.41597003]\n",
      "loss on epoch  480  :  1.48463251856 [0.31387809] [0.41568175]\n",
      "loss on epoch  481  :  1.48520475184 [0.31387809] [0.41568175]\n",
      "loss on epoch  482  :  1.48542016745 [0.31387809] [0.41597003]\n",
      "loss on epoch  483  :  1.48328565006 [0.31517509] [0.41640243]\n",
      "loss on epoch  484  :  1.48351881901 [0.31647211] [0.41640243]\n",
      "loss on epoch  485  :  1.48422417155 [0.31647211] [0.41640243]\n",
      "loss on epoch  486  :  1.48448287558 [0.31647211] [0.41741136]\n",
      "loss on epoch  487  :  1.48268024568 [0.31647211] [0.41798788]\n",
      "loss on epoch  488  :  1.48261509118 [0.31647211] [0.41798788]\n",
      "loss on epoch  489  :  1.48309644505 [0.31647211] [0.41827616]\n",
      "loss on epoch  490  :  1.48236889309 [0.31647211] [0.41870856]\n",
      "loss on epoch  491  :  1.48201636473 [0.31647211] [0.41885269]\n",
      "loss on epoch  492  :  1.48170470971 [0.31776914] [0.41914096]\n",
      "loss on epoch  493  :  1.48053790463 [0.31647211] [0.41899684]\n",
      "loss on epoch  494  :  1.4810068519 [0.31647211] [0.41885269]\n",
      "loss on epoch  495  :  1.48050766521 [0.31387809] [0.41971749]\n",
      "loss on epoch  496  :  1.47967723343 [0.31647211] [0.41971749]\n",
      "loss on epoch  497  :  1.48034142123 [0.31776914] [0.41986164]\n",
      "loss on epoch  498  :  1.47912574035 [0.31517509] [0.42029405]\n",
      "loss on epoch  499  :  1.478269162 [0.31776914] [0.42000577]\n",
      "loss on epoch  500  :  1.47836878565 [0.31387809] [0.42058229]\n",
      "loss on epoch  501  :  1.4783996741 [0.31387809] [0.42058229]\n",
      "loss on epoch  502  :  1.47841861954 [0.31387809] [0.4210147]\n",
      "loss on epoch  503  :  1.47767398534 [0.31387809] [0.4214471]\n",
      "loss on epoch  504  :  1.47669619543 [0.31387809] [0.42173538]\n",
      "loss on epoch  505  :  1.47735292823 [0.31387809] [0.42202362]\n",
      "loss on epoch  506  :  1.47636685327 [0.31387809] [0.42245603]\n",
      "loss on epoch  507  :  1.47602193665 [0.31387809] [0.4223119]\n",
      "loss on epoch  508  :  1.47624684705 [0.31387809] [0.42317671]\n",
      "loss on epoch  509  :  1.47444514654 [0.31647211] [0.42317671]\n",
      "loss on epoch  510  :  1.47481417877 [0.31387809] [0.42389739]\n",
      "loss on epoch  511  :  1.47493652061 [0.31387809] [0.42332083]\n",
      "loss on epoch  512  :  1.47475386107 [0.31517509] [0.42389739]\n",
      "loss on epoch  513  :  1.47349520524 [0.31387809] [0.42432979]\n",
      "loss on epoch  514  :  1.4744006881 [0.31387809] [0.42461804]\n",
      "loss on epoch  515  :  1.4732773613 [0.31647211] [0.42461804]\n",
      "loss on epoch  516  :  1.4723200489 [0.31647211] [0.42461804]\n",
      "loss on epoch  517  :  1.47227098986 [0.31776914] [0.42490631]\n",
      "loss on epoch  518  :  1.47230467752 [0.31776914] [0.42476219]\n",
      "loss on epoch  519  :  1.4717513941 [0.31776914] [0.42533872]\n",
      "loss on epoch  520  :  1.47186456124 [0.31776914] [0.42533872]\n",
      "loss on epoch  521  :  1.47064535485 [0.31776914] [0.42562699]\n",
      "loss on epoch  522  :  1.46982862994 [0.31776914] [0.42548284]\n",
      "loss on epoch  523  :  1.47038901073 [0.31776914] [0.42548284]\n",
      "loss on epoch  524  :  1.46987092716 [0.31906614] [0.42533872]\n",
      "loss on epoch  525  :  1.46988962977 [0.32036316] [0.42548284]\n",
      "loss on epoch  526  :  1.46980132659 [0.31776914] [0.4260594]\n",
      "loss on epoch  527  :  1.46991986257 [0.31906614] [0.4264918]\n",
      "loss on epoch  528  :  1.46845158162 [0.31906614] [0.42634764]\n",
      "loss on epoch  529  :  1.46783488327 [0.31906614] [0.42678005]\n",
      "loss on epoch  530  :  1.46855882141 [0.31906614] [0.42634764]\n",
      "loss on epoch  531  :  1.46789057608 [0.31906614] [0.42663592]\n",
      "loss on epoch  532  :  1.46782975285 [0.32036316] [0.42663592]\n",
      "loss on epoch  533  :  1.46626086588 [0.31906614] [0.42663592]\n",
      "loss on epoch  534  :  1.46715826679 [0.32036316] [0.42663592]\n",
      "loss on epoch  535  :  1.46676680115 [0.31906614] [0.4269242]\n",
      "loss on epoch  536  :  1.46625609751 [0.31906614] [0.4269242]\n",
      "loss on epoch  537  :  1.46618332907 [0.31906614] [0.42721245]\n",
      "loss on epoch  538  :  1.46520383932 [0.32036316] [0.42721245]\n",
      "loss on epoch  539  :  1.46486179696 [0.31906614] [0.42750072]\n",
      "loss on epoch  540  :  1.46497363956 [0.31906614] [0.427789]\n",
      "loss on epoch  541  :  1.46450511835 [0.31906614] [0.427789]\n",
      "loss on epoch  542  :  1.46390380462 [0.31776914] [0.427789]\n",
      "loss on epoch  543  :  1.4634644058 [0.31647211] [0.42822137]\n",
      "loss on epoch  544  :  1.46359714755 [0.31906614] [0.42807725]\n",
      "loss on epoch  545  :  1.46350515772 [0.31647211] [0.42822137]\n",
      "loss on epoch  546  :  1.46283644438 [0.31776914] [0.427789]\n",
      "loss on epoch  547  :  1.46177749281 [0.31517509] [0.42850965]\n",
      "loss on epoch  548  :  1.46258604085 [0.31517509] [0.42850965]\n",
      "loss on epoch  549  :  1.46086118177 [0.31517509] [0.42865378]\n",
      "loss on epoch  550  :  1.46110131122 [0.31517509] [0.42923033]\n",
      "loss on epoch  551  :  1.46191819509 [0.31387809] [0.42951858]\n",
      "loss on epoch  552  :  1.46090394479 [0.31517509] [0.42923033]\n",
      "loss on epoch  553  :  1.46006772474 [0.31517509] [0.42865378]\n",
      "loss on epoch  554  :  1.46126071612 [0.31517509] [0.42937446]\n",
      "loss on epoch  555  :  1.45925742167 [0.31387809] [0.42923033]\n",
      "loss on epoch  556  :  1.45841177305 [0.31387809] [0.42923033]\n",
      "loss on epoch  557  :  1.45837368126 [0.31387809] [0.43009514]\n",
      "loss on epoch  558  :  1.45843093925 [0.31387809] [0.42980686]\n",
      "loss on epoch  559  :  1.45842781553 [0.31387809] [0.43009514]\n",
      "loss on epoch  560  :  1.45714993168 [0.31517509] [0.43009514]\n",
      "loss on epoch  561  :  1.45727208809 [0.31258106] [0.43023926]\n",
      "loss on epoch  562  :  1.45753343017 [0.31258106] [0.43038338]\n",
      "loss on epoch  563  :  1.45721868453 [0.31258106] [0.43095994]\n",
      "loss on epoch  564  :  1.45641454944 [0.31258106] [0.43095994]\n",
      "loss on epoch  565  :  1.4564302365 [0.31258106] [0.43124819]\n",
      "loss on epoch  566  :  1.45595401305 [0.31258106] [0.43124819]\n",
      "loss on epoch  567  :  1.45533017097 [0.31258106] [0.43168059]\n",
      "loss on epoch  568  :  1.45564039327 [0.31258106] [0.43225715]\n",
      "loss on epoch  569  :  1.45495850952 [0.31258106] [0.43225715]\n",
      "loss on epoch  570  :  1.4548003806 [0.31128404] [0.43240127]\n",
      "loss on epoch  571  :  1.45377114084 [0.30998704] [0.43268955]\n",
      "loss on epoch  572  :  1.4535425504 [0.31128404] [0.43312195]\n",
      "loss on epoch  573  :  1.45312532231 [0.30998704] [0.43326607]\n",
      "loss on epoch  574  :  1.45339234228 [0.30998704] [0.4334102]\n",
      "loss on epoch  575  :  1.45183781562 [0.30998704] [0.43369848]\n",
      "loss on epoch  576  :  1.45248079741 [0.30998704] [0.4338426]\n",
      "loss on epoch  577  :  1.45238332174 [0.30998704] [0.434275]\n",
      "loss on epoch  578  :  1.45064421053 [0.31128404] [0.4338426]\n",
      "loss on epoch  579  :  1.45226457825 [0.30998704] [0.43398675]\n",
      "loss on epoch  580  :  1.45209685299 [0.30998704] [0.43441916]\n",
      "loss on epoch  581  :  1.44979044243 [0.30998704] [0.434275]\n",
      "loss on epoch  582  :  1.45128041285 [0.31128404] [0.43499568]\n",
      "loss on epoch  583  :  1.450370155 [0.30998704] [0.43528393]\n",
      "loss on epoch  584  :  1.45035035522 [0.30998704] [0.43557221]\n",
      "loss on epoch  585  :  1.4494481175 [0.31128404] [0.43614873]\n",
      "loss on epoch  586  :  1.44843904177 [0.31128404] [0.43643701]\n",
      "loss on epoch  587  :  1.44846401612 [0.30998704] [0.43643701]\n",
      "loss on epoch  588  :  1.44907675628 [0.31128404] [0.43629289]\n",
      "loss on epoch  589  :  1.44862217153 [0.31258106] [0.43672529]\n",
      "loss on epoch  590  :  1.44822158637 [0.31258106] [0.43701354]\n",
      "loss on epoch  591  :  1.44642010662 [0.31128404] [0.43686941]\n",
      "loss on epoch  592  :  1.4474261955 [0.31128404] [0.43715769]\n",
      "loss on epoch  593  :  1.44636810709 [0.31258106] [0.43744594]\n",
      "loss on epoch  594  :  1.44741190584 [0.31517509] [0.43730181]\n",
      "loss on epoch  595  :  1.44580623397 [0.31387809] [0.43744594]\n",
      "loss on epoch  596  :  1.44635909575 [0.31387809] [0.43744594]\n",
      "loss on epoch  597  :  1.44608480842 [0.31387809] [0.43715769]\n",
      "loss on epoch  598  :  1.44544563912 [0.31387809] [0.43715769]\n",
      "loss on epoch  599  :  1.44558214038 [0.31258106] [0.43730181]\n",
      "loss on epoch  600  :  1.44508066442 [0.31258106] [0.43744594]\n",
      "loss on epoch  601  :  1.44495425842 [0.31258106] [0.43773422]\n",
      "loss on epoch  602  :  1.44440688248 [0.31387809] [0.43787834]\n",
      "loss on epoch  603  :  1.44371477321 [0.31258106] [0.43787834]\n",
      "loss on epoch  604  :  1.44430448832 [0.31258106] [0.43859902]\n",
      "loss on epoch  605  :  1.44352544016 [0.31387809] [0.4384549]\n",
      "loss on epoch  606  :  1.44292841355 [0.31387809] [0.43831074]\n",
      "loss on epoch  607  :  1.44210682092 [0.31387809] [0.43831074]\n",
      "loss on epoch  608  :  1.4418846148 [0.31387809] [0.43874314]\n",
      "loss on epoch  609  :  1.44173699838 [0.31517509] [0.43917555]\n",
      "loss on epoch  610  :  1.44077771902 [0.31387809] [0.43960795]\n",
      "loss on epoch  611  :  1.44065954067 [0.31128404] [0.4393197]\n",
      "loss on epoch  612  :  1.44068209109 [0.31258106] [0.43960795]\n",
      "loss on epoch  613  :  1.44031850276 [0.31258106] [0.43960795]\n",
      "loss on epoch  614  :  1.44054330278 [0.31387809] [0.4401845]\n",
      "loss on epoch  615  :  1.43820007863 [0.31258106] [0.44004035]\n",
      "loss on epoch  616  :  1.44008791888 [0.31258106] [0.44047275]\n",
      "loss on epoch  617  :  1.43915332247 [0.31387809] [0.44076103]\n",
      "loss on epoch  618  :  1.43916587918 [0.31258106] [0.44032863]\n",
      "loss on epoch  619  :  1.43890708906 [0.31258106] [0.44061691]\n",
      "loss on epoch  620  :  1.43795304607 [0.31258106] [0.44076103]\n",
      "loss on epoch  621  :  1.43810914843 [0.31517509] [0.44047275]\n",
      "loss on epoch  622  :  1.43676779888 [0.31258106] [0.44104931]\n",
      "loss on epoch  623  :  1.4375034946 [0.31258106] [0.44162583]\n",
      "loss on epoch  624  :  1.43736877265 [0.31387809] [0.44133756]\n",
      "loss on epoch  625  :  1.43705912873 [0.31258106] [0.44191408]\n",
      "loss on epoch  626  :  1.43616571691 [0.31387809] [0.44234648]\n",
      "loss on epoch  627  :  1.43610718515 [0.31258106] [0.44191408]\n",
      "loss on epoch  628  :  1.43509829707 [0.31387809] [0.44176996]\n",
      "loss on epoch  629  :  1.43482282647 [0.31387809] [0.44234648]\n",
      "loss on epoch  630  :  1.43458485603 [0.31258106] [0.44306716]\n",
      "loss on epoch  631  :  1.43516525516 [0.31258106] [0.44349957]\n",
      "loss on epoch  632  :  1.43444336344 [0.31258106] [0.44364369]\n",
      "loss on epoch  633  :  1.43448602049 [0.31258106] [0.44393197]\n",
      "loss on epoch  634  :  1.43289209737 [0.31258106] [0.44349957]\n",
      "loss on epoch  635  :  1.43408554792 [0.31258106] [0.44335544]\n",
      "loss on epoch  636  :  1.43284159899 [0.31258106] [0.44378784]\n",
      "loss on epoch  637  :  1.43153473845 [0.31258106] [0.44335544]\n",
      "loss on epoch  638  :  1.43160271203 [0.31258106] [0.44349957]\n",
      "loss on epoch  639  :  1.43188826905 [0.31258106] [0.44450849]\n",
      "loss on epoch  640  :  1.43181601719 [0.31258106] [0.44393197]\n",
      "loss on epoch  641  :  1.4311635141 [0.31258106] [0.44465265]\n",
      "loss on epoch  642  :  1.43003845215 [0.31258106] [0.44494089]\n",
      "loss on epoch  643  :  1.43001476482 [0.31258106] [0.44551745]\n",
      "loss on epoch  644  :  1.43084074833 [0.31258106] [0.44566157]\n",
      "loss on epoch  645  :  1.42985887881 [0.31258106] [0.44609398]\n",
      "loss on epoch  646  :  1.42923768141 [0.30998704] [0.44551745]\n",
      "loss on epoch  647  :  1.42952004185 [0.31128404] [0.44609398]\n",
      "loss on epoch  648  :  1.42829166739 [0.30998704] [0.44681466]\n",
      "loss on epoch  649  :  1.42892222493 [0.31258106] [0.4466705]\n",
      "loss on epoch  650  :  1.42840064234 [0.30998704] [0.4466705]\n",
      "loss on epoch  651  :  1.42856466549 [0.30998704] [0.44681466]\n",
      "loss on epoch  652  :  1.42784086642 [0.30998704] [0.44695878]\n",
      "loss on epoch  653  :  1.42801510625 [0.30998704] [0.44695878]\n",
      "loss on epoch  654  :  1.42775394961 [0.30998704] [0.44695878]\n",
      "loss on epoch  655  :  1.42713771264 [0.30998704] [0.44767946]\n",
      "loss on epoch  656  :  1.42606241835 [0.31128404] [0.44840011]\n",
      "loss on epoch  657  :  1.42651186828 [0.31258106] [0.44883251]\n",
      "loss on epoch  658  :  1.42598504049 [0.31128404] [0.44912079]\n",
      "loss on epoch  659  :  1.42581661984 [0.30998704] [0.44883251]\n",
      "loss on epoch  660  :  1.42583960515 [0.31128404] [0.44926491]\n",
      "loss on epoch  661  :  1.4251559752 [0.31128404] [0.44955319]\n",
      "loss on epoch  662  :  1.42514363925 [0.31128404] [0.44955319]\n",
      "loss on epoch  663  :  1.42455665712 [0.31128404] [0.44984144]\n",
      "loss on epoch  664  :  1.42384801088 [0.31128404] [0.44998559]\n",
      "loss on epoch  665  :  1.42338205488 [0.31128404] [0.45027384]\n",
      "loss on epoch  666  :  1.42382919126 [0.31128404] [0.4508504]\n",
      "loss on epoch  667  :  1.42298711229 [0.31387809] [0.45099452]\n",
      "loss on epoch  668  :  1.42364007455 [0.31387809] [0.45056212]\n",
      "loss on epoch  669  :  1.42201369339 [0.31258106] [0.45157105]\n",
      "loss on epoch  670  :  1.42176123019 [0.31258106] [0.45142692]\n",
      "loss on epoch  671  :  1.42202567171 [0.31258106] [0.45185933]\n",
      "loss on epoch  672  :  1.42159904595 [0.31387809] [0.4521476]\n",
      "loss on epoch  673  :  1.42158497042 [0.31387809] [0.45258]\n",
      "loss on epoch  674  :  1.42163100508 [0.31517509] [0.45243585]\n",
      "loss on epoch  675  :  1.42031968523 [0.31258106] [0.45286825]\n",
      "loss on epoch  676  :  1.42095743506 [0.31258106] [0.45315653]\n",
      "loss on epoch  677  :  1.42011519715 [0.31387809] [0.45272413]\n",
      "loss on epoch  678  :  1.41915826665 [0.31517509] [0.45272413]\n",
      "loss on epoch  679  :  1.41983773752 [0.31387809] [0.45315653]\n",
      "loss on epoch  680  :  1.41852603577 [0.31387809] [0.45258]\n",
      "loss on epoch  681  :  1.41906288597 [0.31258106] [0.45358893]\n",
      "loss on epoch  682  :  1.41871270427 [0.31387809] [0.45358893]\n",
      "loss on epoch  683  :  1.41820227879 [0.31387809] [0.45402133]\n",
      "loss on epoch  684  :  1.41708918854 [0.31387809] [0.45373306]\n",
      "loss on epoch  685  :  1.41733461618 [0.31517509] [0.45358893]\n",
      "loss on epoch  686  :  1.4168332197 [0.31387809] [0.45416546]\n",
      "loss on epoch  687  :  1.41702497447 [0.31387809] [0.45330065]\n",
      "loss on epoch  688  :  1.41680520773 [0.31387809] [0.45430961]\n",
      "loss on epoch  689  :  1.41707122988 [0.31387809] [0.45416546]\n",
      "loss on epoch  690  :  1.41569816625 [0.31387809] [0.45503026]\n",
      "loss on epoch  691  :  1.41532437007 [0.31387809] [0.45517442]\n",
      "loss on epoch  692  :  1.41560775925 [0.31387809] [0.45560679]\n",
      "loss on epoch  693  :  1.41553205031 [0.31387809] [0.45560679]\n",
      "loss on epoch  694  :  1.41472864593 [0.31387809] [0.45560679]\n",
      "loss on epoch  695  :  1.41430419904 [0.31387809] [0.45632747]\n",
      "loss on epoch  696  :  1.41316918532 [0.31387809] [0.45618334]\n",
      "loss on epoch  697  :  1.41428651633 [0.31387809] [0.45603919]\n",
      "loss on epoch  698  :  1.41372638279 [0.31387809] [0.45618334]\n",
      "loss on epoch  699  :  1.41305523449 [0.31258106] [0.45647159]\n",
      "loss on epoch  700  :  1.41212939995 [0.31387809] [0.45690399]\n",
      "loss on epoch  701  :  1.41301855776 [0.31387809] [0.45690399]\n",
      "loss on epoch  702  :  1.41286835406 [0.31387809] [0.45748055]\n",
      "loss on epoch  703  :  1.41200432512 [0.31258106] [0.45748055]\n",
      "loss on epoch  704  :  1.41107302463 [0.31258106] [0.45748055]\n",
      "loss on epoch  705  :  1.41159373301 [0.31128404] [0.45762467]\n",
      "loss on epoch  706  :  1.41077478947 [0.31258106] [0.4582012]\n",
      "loss on epoch  707  :  1.40992052467 [0.31128404] [0.4582012]\n",
      "loss on epoch  708  :  1.41014573309 [0.30998704] [0.45834535]\n",
      "loss on epoch  709  :  1.40959043194 [0.30998704] [0.45877776]\n",
      "loss on epoch  710  :  1.4096544694 [0.31128404] [0.45892188]\n",
      "loss on epoch  711  :  1.4098935635 [0.31128404] [0.45921016]\n",
      "loss on epoch  712  :  1.40915089404 [0.31128404] [0.45978668]\n",
      "loss on epoch  713  :  1.40874540585 [0.30998704] [0.45978668]\n",
      "loss on epoch  714  :  1.4099192818 [0.31258106] [0.45978668]\n",
      "loss on epoch  715  :  1.4076511595 [0.30869001] [0.45949841]\n",
      "loss on epoch  716  :  1.4076719836 [0.30998704] [0.45993081]\n",
      "loss on epoch  717  :  1.40625771549 [0.30998704] [0.45949841]\n",
      "loss on epoch  718  :  1.407392493 [0.31128404] [0.46050736]\n",
      "loss on epoch  719  :  1.40674198336 [0.30998704] [0.46050736]\n",
      "loss on epoch  720  :  1.4063425925 [0.30869001] [0.46036321]\n",
      "loss on epoch  721  :  1.40655583364 [0.31128404] [0.46065149]\n",
      "loss on epoch  722  :  1.40612246593 [0.31128404] [0.46122801]\n",
      "loss on epoch  723  :  1.40602978954 [0.30998704] [0.46050736]\n",
      "loss on epoch  724  :  1.40524089999 [0.30998704] [0.46108389]\n",
      "loss on epoch  725  :  1.40499862918 [0.30998704] [0.46108389]\n",
      "loss on epoch  726  :  1.40471249598 [0.30869001] [0.46137217]\n",
      "loss on epoch  727  :  1.40506871762 [0.30739298] [0.46122801]\n",
      "loss on epoch  728  :  1.4042048962 [0.30739298] [0.46180457]\n",
      "loss on epoch  729  :  1.40347755618 [0.30869001] [0.46252522]\n",
      "loss on epoch  730  :  1.40302058061 [0.30739298] [0.46223697]\n",
      "loss on epoch  731  :  1.40360471054 [0.30739298] [0.46295762]\n",
      "loss on epoch  732  :  1.40179844477 [0.30739298] [0.46295762]\n",
      "loss on epoch  733  :  1.4033868953 [0.30739298] [0.4628135]\n",
      "loss on epoch  734  :  1.4029483751 [0.30739298] [0.46266934]\n",
      "loss on epoch  735  :  1.40325909853 [0.30998704] [0.46339002]\n",
      "loss on epoch  736  :  1.40153957738 [0.30869001] [0.4641107]\n",
      "loss on epoch  737  :  1.40173074272 [0.30869001] [0.4636783]\n",
      "loss on epoch  738  :  1.40133665226 [0.30869001] [0.46382242]\n",
      "loss on epoch  739  :  1.40083949875 [0.30739298] [0.46339002]\n",
      "loss on epoch  740  :  1.39914036901 [0.30739298] [0.4641107]\n",
      "loss on epoch  741  :  1.40019115916 [0.30609599] [0.46382242]\n",
      "loss on epoch  742  :  1.40037311448 [0.30739298] [0.46382242]\n",
      "loss on epoch  743  :  1.39940468249 [0.30609599] [0.4641107]\n",
      "loss on epoch  744  :  1.39978735756 [0.30609599] [0.4641107]\n",
      "loss on epoch  745  :  1.3987871276 [0.30609599] [0.4641107]\n",
      "loss on epoch  746  :  1.39899155608 [0.30739298] [0.4641107]\n",
      "loss on epoch  747  :  1.39919751883 [0.30609599] [0.46468723]\n",
      "loss on epoch  748  :  1.3985713323 [0.30609599] [0.46540791]\n",
      "loss on epoch  749  :  1.39795502468 [0.30609599] [0.46483135]\n",
      "loss on epoch  750  :  1.39695807077 [0.30609599] [0.46526375]\n",
      "loss on epoch  751  :  1.39639874079 [0.30609599] [0.46511963]\n",
      "loss on epoch  752  :  1.39778338759 [0.30739298] [0.46555203]\n",
      "loss on epoch  753  :  1.39564233356 [0.30739298] [0.46569616]\n",
      "loss on epoch  754  :  1.39603309057 [0.30609599] [0.46598443]\n",
      "loss on epoch  755  :  1.3952330108 [0.30739298] [0.46598443]\n",
      "loss on epoch  756  :  1.39516930006 [0.30609599] [0.46540791]\n",
      "loss on epoch  757  :  1.39537764479 [0.30739298] [0.46627271]\n",
      "loss on epoch  758  :  1.39512634057 [0.30739298] [0.46641684]\n",
      "loss on epoch  759  :  1.39591574227 [0.30739298] [0.46670511]\n",
      "loss on epoch  760  :  1.39495982285 [0.30739298] [0.46670511]\n",
      "loss on epoch  761  :  1.39427825698 [0.30739298] [0.46656096]\n",
      "loss on epoch  762  :  1.39369860402 [0.30869001] [0.46656096]\n",
      "loss on epoch  763  :  1.39369733245 [0.30869001] [0.46670511]\n",
      "loss on epoch  764  :  1.39275719281 [0.30869001] [0.46713752]\n",
      "loss on epoch  765  :  1.39291184699 [0.30869001] [0.46756992]\n",
      "loss on epoch  766  :  1.39268557451 [0.30739298] [0.46785817]\n",
      "loss on epoch  767  :  1.39306103962 [0.30739298] [0.46814644]\n",
      "loss on epoch  768  :  1.39200855626 [0.30739298] [0.46829057]\n",
      "loss on epoch  769  :  1.39109972009 [0.30739298] [0.46843472]\n",
      "loss on epoch  770  :  1.39013616244 [0.30739298] [0.46886712]\n",
      "loss on epoch  771  :  1.39128232885 [0.30869001] [0.46915537]\n",
      "loss on epoch  772  :  1.39076898716 [0.30739298] [0.46886712]\n",
      "loss on epoch  773  :  1.39067025317 [0.30479896] [0.46872297]\n",
      "loss on epoch  774  :  1.3902532657 [0.30479896] [0.46944365]\n",
      "loss on epoch  775  :  1.38974626859 [0.30479896] [0.46944365]\n",
      "loss on epoch  776  :  1.38973666783 [0.30739298] [0.4701643]\n",
      "loss on epoch  777  :  1.38934609625 [0.30739298] [0.4701643]\n",
      "loss on epoch  778  :  1.38929095975 [0.30609599] [0.47074085]\n",
      "loss on epoch  779  :  1.38881556414 [0.30739298] [0.47117326]\n",
      "loss on epoch  780  :  1.38927809397 [0.30739298] [0.4710291]\n",
      "loss on epoch  781  :  1.3878900762 [0.30609599] [0.47117326]\n",
      "loss on epoch  782  :  1.38858409281 [0.30609599] [0.47160566]\n",
      "loss on epoch  783  :  1.38800708453 [0.30739298] [0.47117326]\n",
      "loss on epoch  784  :  1.38728028536 [0.30609599] [0.47203806]\n",
      "loss on epoch  785  :  1.38668433825 [0.30609599] [0.47203806]\n",
      "loss on epoch  786  :  1.38647379699 [0.30739298] [0.47189391]\n",
      "loss on epoch  787  :  1.3857885268 [0.30739298] [0.47203806]\n",
      "loss on epoch  788  :  1.38611358183 [0.30609599] [0.47203806]\n",
      "loss on epoch  789  :  1.38463877969 [0.30609599] [0.47261459]\n",
      "loss on epoch  790  :  1.38533887819 [0.30739298] [0.47261459]\n",
      "loss on epoch  791  :  1.38496293403 [0.30739298] [0.47275871]\n",
      "loss on epoch  792  :  1.38460965289 [0.30739298] [0.47275871]\n",
      "loss on epoch  793  :  1.38484471374 [0.30739298] [0.47275871]\n",
      "loss on epoch  794  :  1.38436959187 [0.30609599] [0.47319111]\n",
      "loss on epoch  795  :  1.38435543908 [0.30609599] [0.47347939]\n",
      "loss on epoch  796  :  1.3835906563 [0.30739298] [0.47304699]\n",
      "loss on epoch  797  :  1.38303697551 [0.30739298] [0.47290286]\n",
      "loss on epoch  798  :  1.3826080958 [0.30869001] [0.47362351]\n",
      "loss on epoch  799  :  1.38336776142 [0.30869001] [0.47347939]\n",
      "loss on epoch  800  :  1.38194992145 [0.30609599] [0.47376767]\n",
      "loss on epoch  801  :  1.38246271345 [0.30998704] [0.47391179]\n",
      "loss on epoch  802  :  1.38200017258 [0.30869001] [0.47391179]\n",
      "loss on epoch  803  :  1.38139753651 [0.30869001] [0.47463247]\n",
      "loss on epoch  804  :  1.38066367529 [0.30869001] [0.47448832]\n",
      "loss on epoch  805  :  1.38055934067 [0.30869001] [0.47492072]\n",
      "loss on epoch  806  :  1.38132446563 [0.30739298] [0.47492072]\n",
      "loss on epoch  807  :  1.38052860454 [0.30869001] [0.47463247]\n",
      "loss on epoch  808  :  1.37928479248 [0.30869001] [0.475209]\n",
      "loss on epoch  809  :  1.37997533215 [0.30869001] [0.47506487]\n",
      "loss on epoch  810  :  1.37958774964 [0.30739298] [0.475209]\n",
      "loss on epoch  811  :  1.37932152439 [0.30998704] [0.47535312]\n",
      "loss on epoch  812  :  1.37887655143 [0.30869001] [0.47621793]\n",
      "loss on epoch  813  :  1.37891289923 [0.30739298] [0.4760738]\n",
      "loss on epoch  814  :  1.37817660526 [0.30739298] [0.47679445]\n",
      "loss on epoch  815  :  1.37855643034 [0.30869001] [0.47679445]\n",
      "loss on epoch  816  :  1.37722738142 [0.30869001] [0.47665033]\n",
      "loss on epoch  817  :  1.37712244414 [0.30609599] [0.47693861]\n",
      "loss on epoch  818  :  1.37715880518 [0.30739298] [0.47693861]\n",
      "loss on epoch  819  :  1.37729544331 [0.30739298] [0.47693861]\n",
      "loss on epoch  820  :  1.37732689027 [0.30998704] [0.47751513]\n",
      "loss on epoch  821  :  1.37606217685 [0.30998704] [0.47722685]\n",
      "loss on epoch  822  :  1.37489923504 [0.30739298] [0.47737101]\n",
      "loss on epoch  823  :  1.37543645612 [0.30739298] [0.47722685]\n",
      "loss on epoch  824  :  1.37492260889 [0.30869001] [0.47852406]\n",
      "loss on epoch  825  :  1.37481322112 [0.30998704] [0.47866821]\n",
      "loss on epoch  826  :  1.37510015346 [0.30739298] [0.47881234]\n",
      "loss on epoch  827  :  1.3755384507 [0.30998704] [0.47823581]\n",
      "loss on epoch  828  :  1.37389894989 [0.30869001] [0.47866821]\n",
      "loss on epoch  829  :  1.37412882734 [0.30869001] [0.47910061]\n",
      "loss on epoch  830  :  1.3730296029 [0.30869001] [0.47938886]\n",
      "loss on epoch  831  :  1.37407776824 [0.30869001] [0.47910061]\n",
      "loss on epoch  832  :  1.37250990117 [0.30869001] [0.47953302]\n",
      "loss on epoch  833  :  1.37192317512 [0.30998704] [0.47938886]\n",
      "loss on epoch  834  :  1.37217343295 [0.30869001] [0.47967714]\n",
      "loss on epoch  835  :  1.37143386293 [0.30998704] [0.47996542]\n",
      "loss on epoch  836  :  1.37240032576 [0.30869001] [0.47967714]\n",
      "loss on epoch  837  :  1.37045386323 [0.30869001] [0.47982126]\n",
      "loss on epoch  838  :  1.37050556916 [0.30869001] [0.48025367]\n",
      "loss on epoch  839  :  1.37002047786 [0.30998704] [0.48025367]\n",
      "loss on epoch  840  :  1.3702322929 [0.30869001] [0.48039782]\n",
      "loss on epoch  841  :  1.37034654838 [0.30869001] [0.47982126]\n",
      "loss on epoch  842  :  1.36987202918 [0.30869001] [0.48025367]\n",
      "loss on epoch  843  :  1.37016091744 [0.30998704] [0.48010954]\n",
      "loss on epoch  844  :  1.36909563895 [0.30739298] [0.48111847]\n",
      "loss on epoch  845  :  1.36894921903 [0.30869001] [0.48097435]\n",
      "loss on epoch  846  :  1.36842437144 [0.30869001] [0.48126262]\n",
      "loss on epoch  847  :  1.36839470157 [0.30869001] [0.48140675]\n",
      "loss on epoch  848  :  1.36867700683 [0.30998704] [0.48198327]\n",
      "loss on epoch  849  :  1.3684213029 [0.30869001] [0.48169503]\n",
      "loss on epoch  850  :  1.36761325598 [0.30869001] [0.48241568]\n",
      "loss on epoch  851  :  1.36778092605 [0.30739298] [0.48255983]\n",
      "loss on epoch  852  :  1.36688931783 [0.30739298] [0.48241568]\n",
      "loss on epoch  853  :  1.36738470749 [0.30998704] [0.48284808]\n",
      "loss on epoch  854  :  1.36613372741 [0.30739298] [0.48313636]\n",
      "loss on epoch  855  :  1.36587773208 [0.30869001] [0.48328048]\n",
      "loss on epoch  856  :  1.3657470478 [0.31258106] [0.4834246]\n",
      "loss on epoch  857  :  1.36503507473 [0.30739298] [0.48385701]\n",
      "loss on epoch  858  :  1.36602871727 [0.30998704] [0.48400116]\n",
      "loss on epoch  859  :  1.3654343839 [0.30869001] [0.48385701]\n",
      "loss on epoch  860  :  1.36441160131 [0.30998704] [0.48371288]\n",
      "loss on epoch  861  :  1.36453509331 [0.30998704] [0.48400116]\n",
      "loss on epoch  862  :  1.36453154352 [0.30998704] [0.48457769]\n",
      "loss on epoch  863  :  1.36382409158 [0.31128404] [0.48486596]\n",
      "loss on epoch  864  :  1.36348395657 [0.30869001] [0.48501009]\n",
      "loss on epoch  865  :  1.36266868644 [0.31128404] [0.48558661]\n",
      "loss on epoch  866  :  1.36358702845 [0.30998704] [0.48587489]\n",
      "loss on epoch  867  :  1.36293109258 [0.30998704] [0.48558661]\n",
      "loss on epoch  868  :  1.3619035063 [0.31128404] [0.48616317]\n",
      "loss on epoch  869  :  1.36278081382 [0.30739298] [0.48573077]\n",
      "loss on epoch  870  :  1.36271574762 [0.30739298] [0.48587489]\n",
      "loss on epoch  871  :  1.36228094057 [0.30739298] [0.48645142]\n",
      "loss on epoch  872  :  1.36148415451 [0.30869001] [0.48702797]\n",
      "loss on epoch  873  :  1.36054968834 [0.30998704] [0.48746037]\n",
      "loss on epoch  874  :  1.36065966995 [0.30739298] [0.48731622]\n",
      "loss on epoch  875  :  1.3596443556 [0.30739298] [0.4871721]\n",
      "loss on epoch  876  :  1.36020521544 [0.30869001] [0.48774862]\n",
      "loss on epoch  877  :  1.35845159601 [0.30998704] [0.4876045]\n",
      "loss on epoch  878  :  1.35927884005 [0.30609599] [0.48746037]\n",
      "loss on epoch  879  :  1.35913901197 [0.30609599] [0.48832518]\n",
      "loss on epoch  880  :  1.35857771944 [0.30609599] [0.48861343]\n",
      "loss on epoch  881  :  1.35880303383 [0.30739298] [0.48918998]\n",
      "loss on epoch  882  :  1.35863601499 [0.30869001] [0.48933411]\n",
      "loss on epoch  883  :  1.35800137343 [0.30869001] [0.48962238]\n",
      "loss on epoch  884  :  1.3569859774 [0.30739298] [0.48947823]\n",
      "loss on epoch  885  :  1.35737589774 [0.30869001] [0.48947823]\n",
      "loss on epoch  886  :  1.35718961557 [0.30739298] [0.48947823]\n",
      "loss on epoch  887  :  1.35642485045 [0.30869001] [0.48962238]\n",
      "loss on epoch  888  :  1.35675599178 [0.30739298] [0.48976651]\n",
      "loss on epoch  889  :  1.35635991008 [0.30609599] [0.48991063]\n",
      "loss on epoch  890  :  1.3559022877 [0.30869001] [0.49005476]\n",
      "loss on epoch  891  :  1.35525482451 [0.30739298] [0.49005476]\n",
      "loss on epoch  892  :  1.35559350031 [0.30869001] [0.49005476]\n",
      "loss on epoch  893  :  1.35525847144 [0.30739298] [0.49019891]\n",
      "loss on epoch  894  :  1.35459197009 [0.30869001] [0.49048716]\n",
      "loss on epoch  895  :  1.3546753901 [0.30998704] [0.49048716]\n",
      "loss on epoch  896  :  1.35349099945 [0.30998704] [0.49034303]\n",
      "loss on epoch  897  :  1.35440197256 [0.30869001] [0.49063131]\n",
      "loss on epoch  898  :  1.35267909809 [0.30998704] [0.49077544]\n",
      "loss on epoch  899  :  1.35319129626 [0.30869001] [0.49063131]\n",
      "loss on epoch  900  :  1.35299744871 [0.30998704] [0.49106371]\n",
      "loss on epoch  901  :  1.35267895902 [0.30998704] [0.49106371]\n",
      "loss on epoch  902  :  1.35176776736 [0.30869001] [0.49164024]\n",
      "loss on epoch  903  :  1.35219313701 [0.30998704] [0.49135196]\n",
      "loss on epoch  904  :  1.3518790117 [0.31128404] [0.49149612]\n",
      "loss on epoch  905  :  1.35146001754 [0.30998704] [0.49135196]\n",
      "loss on epoch  906  :  1.3512652119 [0.30998704] [0.49192852]\n",
      "loss on epoch  907  :  1.35065991349 [0.30869001] [0.49207264]\n",
      "loss on epoch  908  :  1.35066097754 [0.30869001] [0.49250504]\n",
      "loss on epoch  909  :  1.34971149321 [0.30869001] [0.49322572]\n",
      "loss on epoch  910  :  1.3498256648 [0.30739298] [0.49336985]\n",
      "loss on epoch  911  :  1.34932345152 [0.30739298] [0.49380225]\n",
      "loss on epoch  912  :  1.34901104371 [0.30998704] [0.49336985]\n",
      "loss on epoch  913  :  1.34875781006 [0.30739298] [0.49308157]\n",
      "loss on epoch  914  :  1.34812523921 [0.30739298] [0.49351397]\n",
      "loss on epoch  915  :  1.34926946958 [0.30869001] [0.49365813]\n",
      "loss on epoch  916  :  1.34884948201 [0.30869001] [0.49437878]\n",
      "loss on epoch  917  :  1.34705137323 [0.30998704] [0.49481118]\n",
      "loss on epoch  918  :  1.34843233117 [0.30869001] [0.49495533]\n",
      "loss on epoch  919  :  1.34748912078 [0.30998704] [0.49481118]\n",
      "loss on epoch  920  :  1.34730663564 [0.30609599] [0.49524358]\n",
      "loss on epoch  921  :  1.34699791449 [0.30998704] [0.49553186]\n",
      "loss on epoch  922  :  1.34642680707 [0.30998704] [0.49582013]\n",
      "loss on epoch  923  :  1.34648621965 [0.30998704] [0.49625254]\n",
      "loss on epoch  924  :  1.34618457379 [0.30869001] [0.49625254]\n",
      "loss on epoch  925  :  1.34560565816 [0.31128404] [0.49625254]\n",
      "loss on epoch  926  :  1.34516221947 [0.31258106] [0.49654078]\n",
      "loss on epoch  927  :  1.34551151594 [0.30998704] [0.49654078]\n",
      "loss on epoch  928  :  1.34464499465 [0.31258106] [0.49697319]\n",
      "loss on epoch  929  :  1.34491442972 [0.30998704] [0.49740559]\n",
      "loss on epoch  930  :  1.34317908022 [0.30869001] [0.49754971]\n",
      "loss on epoch  931  :  1.34297394532 [0.30869001] [0.49783799]\n",
      "loss on epoch  932  :  1.34321486067 [0.31128404] [0.49754971]\n",
      "loss on epoch  933  :  1.34289211918 [0.30869001] [0.49841452]\n",
      "loss on epoch  934  :  1.34322619217 [0.30869001] [0.49870279]\n",
      "loss on epoch  935  :  1.34257589225 [0.30869001] [0.49899107]\n",
      "loss on epoch  936  :  1.34222461559 [0.30998704] [0.49855867]\n",
      "loss on epoch  937  :  1.34247132142 [0.30869001] [0.49927932]\n",
      "loss on epoch  938  :  1.3418610979 [0.30869001] [0.49942347]\n",
      "loss on epoch  939  :  1.34122165706 [0.30869001] [0.4991352]\n",
      "loss on epoch  940  :  1.34174913168 [0.30998704] [0.49985588]\n",
      "loss on epoch  941  :  1.34039578614 [0.30869001] [0.4995676]\n",
      "loss on epoch  942  :  1.34020246621 [0.30869001] [0.49985588]\n",
      "loss on epoch  943  :  1.34032550565 [0.30869001] [0.4995676]\n",
      "loss on epoch  944  :  1.33965922064 [0.30869001] [0.50014412]\n",
      "loss on epoch  945  :  1.33946026034 [0.30869001] [0.50014412]\n",
      "loss on epoch  946  :  1.3401800946 [0.30869001] [0.50014412]\n",
      "loss on epoch  947  :  1.33914009289 [0.30739298] [0.50043237]\n",
      "loss on epoch  948  :  1.33737684179 [0.30739298] [0.50100893]\n",
      "loss on epoch  949  :  1.33844918233 [0.30739298] [0.50115305]\n",
      "loss on epoch  950  :  1.33860811922 [0.30609599] [0.50129718]\n",
      "loss on epoch  951  :  1.33902915098 [0.30739298] [0.50115305]\n",
      "loss on epoch  952  :  1.33659993498 [0.30739298] [0.50245029]\n",
      "loss on epoch  953  :  1.33802435795 [0.30739298] [0.50201786]\n",
      "loss on epoch  954  :  1.33751377353 [0.30609599] [0.50201786]\n",
      "loss on epoch  955  :  1.33619887078 [0.30739298] [0.50259441]\n",
      "loss on epoch  956  :  1.33648900633 [0.30739298] [0.50245029]\n",
      "loss on epoch  957  :  1.33597266233 [0.30869001] [0.50230616]\n",
      "loss on epoch  958  :  1.33656270636 [0.30739298] [0.50273854]\n",
      "loss on epoch  959  :  1.33574077597 [0.30739298] [0.50245029]\n",
      "loss on epoch  960  :  1.33566139804 [0.30739298] [0.50245029]\n",
      "loss on epoch  961  :  1.33597427386 [0.30739298] [0.50288266]\n",
      "loss on epoch  962  :  1.33430127082 [0.30869001] [0.50302678]\n",
      "loss on epoch  963  :  1.33492890552 [0.30739298] [0.50317097]\n",
      "loss on epoch  964  :  1.33229058319 [0.30869001] [0.50317097]\n",
      "loss on epoch  965  :  1.33489605453 [0.30609599] [0.50345922]\n",
      "loss on epoch  966  :  1.33372100415 [0.30609599] [0.50345922]\n",
      "loss on epoch  967  :  1.33379074821 [0.30998704] [0.50360334]\n",
      "loss on epoch  968  :  1.33248245495 [0.30739298] [0.50417989]\n",
      "loss on epoch  969  :  1.33198449126 [0.30869001] [0.50432402]\n",
      "loss on epoch  970  :  1.33308931633 [0.30739298] [0.50374746]\n",
      "loss on epoch  971  :  1.3320769491 [0.30739298] [0.50475639]\n",
      "loss on epoch  972  :  1.33158454189 [0.30739298] [0.50461227]\n",
      "loss on epoch  973  :  1.33186044075 [0.30869001] [0.50446814]\n",
      "loss on epoch  974  :  1.33046566336 [0.30739298] [0.50461227]\n",
      "loss on epoch  975  :  1.33096100445 [0.30739298] [0.50518882]\n",
      "loss on epoch  976  :  1.32951493616 [0.30869001] [0.50461227]\n",
      "loss on epoch  977  :  1.33050698704 [0.30739298] [0.50562119]\n",
      "loss on epoch  978  :  1.33009809918 [0.30869001] [0.5059095]\n",
      "loss on epoch  979  :  1.33022277223 [0.30739298] [0.50547707]\n",
      "loss on epoch  980  :  1.32992469161 [0.30739298] [0.50562119]\n",
      "loss on epoch  981  :  1.32939158104 [0.30869001] [0.50533295]\n",
      "loss on epoch  982  :  1.32777326637 [0.30739298] [0.50619775]\n",
      "loss on epoch  983  :  1.32851391148 [0.30609599] [0.50576538]\n",
      "loss on epoch  984  :  1.32880669611 [0.30739298] [0.50619775]\n",
      "loss on epoch  985  :  1.32895310499 [0.30739298] [0.506486]\n",
      "loss on epoch  986  :  1.32704976311 [0.30739298] [0.50691843]\n",
      "loss on epoch  987  :  1.32731910988 [0.30739298] [0.50663018]\n",
      "loss on epoch  988  :  1.32701243957 [0.30739298] [0.50706255]\n",
      "loss on epoch  989  :  1.3271800964 [0.30739298] [0.50677431]\n",
      "loss on epoch  990  :  1.32656929007 [0.30869001] [0.5073508]\n",
      "loss on epoch  991  :  1.32686992707 [0.30869001] [0.50763911]\n",
      "loss on epoch  992  :  1.32567271701 [0.30739298] [0.5073508]\n",
      "loss on epoch  993  :  1.32628668238 [0.30869001] [0.50749493]\n",
      "loss on epoch  994  :  1.3257247055 [0.30739298] [0.50749493]\n",
      "loss on epoch  995  :  1.32612908107 [0.30609599] [0.5073508]\n",
      "loss on epoch  996  :  1.32463681698 [0.30869001] [0.50792736]\n",
      "loss on epoch  997  :  1.32468309888 [0.30998704] [0.50850391]\n",
      "loss on epoch  998  :  1.32410511706 [0.30998704] [0.50835973]\n",
      "loss on epoch  999  :  1.32398317478 [0.30998704] [0.50850391]\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "para_n_epoch = 1000\n",
    "\n",
    "# tunable parameters\n",
    "para_n_hidden_list = [ 256,128,32 ]\n",
    "para_batch_size = 128\n",
    "para_n_embedding = 2\n",
    "para_lr=0.008\n",
    "para_keep_prob=0.8\n",
    "para_l2=0.05\n",
    "\n",
    "# fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = 8\n",
    "\n",
    "tot_cnt= np.shape(cxtrain)[0]\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    mlp_clf = mlp_demo( para_batch_size, para_n_conti, para_n_disc, para_n_class, \\\n",
    "                        para_n_disc_voca, para_n_embedding, sess, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2 )\n",
    "    mlp_clf.train_ini()\n",
    "    mlp_clf.inference_ini()\n",
    "    \n",
    "    para_cur_lr=para_lr\n",
    "    total_batch = int(tot_cnt/para_batch_size)\n",
    "\n",
    "    tot_idx=range(tot_cnt)\n",
    "\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc=0.0\n",
    "        \n",
    "#       shuffle traning dataset each epoch  \n",
    "        np.random.shuffle(tot_idx)\n",
    "   \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = tot_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += mlp_clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                       para_cur_lr)\n",
    "        \n",
    "#       learning rate decaying\n",
    "        para_cur_lr = para_cur_lr*0.99\n",
    "        \n",
    "        tmp_test_acc = mlp_clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = mlp_clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wide demoMLP\n",
    "## without embedding\n",
    "class wide_mlp_demo():\n",
    "    \n",
    "    def __init__(self, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, session, n_hidden_list, lr, l2):\n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list) \n",
    "        self.sess=session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "#       linnear part on categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "        \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"disc\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_uniform([self.N_DISC_VOCA, self.N_CLASS],-1.0, 1.0) )\n",
    "#                         tf.random_normal([self.N_DISC_VOCA, self.N_CLASS],\\\n",
    "#                         stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "                \n",
    "                if i==0:\n",
    "                    dx_wsum = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                else:\n",
    "                    dx_wsum = dx_wsum + tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "        \n",
    "        with tf.variable_scope(\"wide\"):\n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))            \n",
    "            dx_wsum = tf.add(dx_wsum, b)\n",
    "\n",
    "#----unit test---------------------------------------------------------\n",
    "#         print self.N_TOTAL\n",
    "#         self.tmpshape= tf.shape(x_concate)\n",
    "        \n",
    "#         self.tmpshape1= tf.shape(tmp_embeded[0])\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# #     hidden layers\n",
    "        with tf.variable_scope(\"h0\"):\n",
    "            w= tf.Variable(tf.random_normal([self.N_CONTI, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_CONTI)))) \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            h = tf.nn.relu( tf.add( tf.matmul(self.cx, w),b) )\n",
    "        \n",
    "                \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"layer\"+str(i)):\n",
    "                w= tf.Variable(tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b= tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                h = tf.nn.relu( tf.add( tf.matmul(h, w),b) )\n",
    "        \n",
    "#       Regularization\n",
    "#       dropout\n",
    "#         h = tf.nn.dropout(h, self.keep_prob)\n",
    "#       L2\n",
    "        self.regularizer = tf.nn.l2_loss(w)\n",
    "        \n",
    "#       output layer  \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w= tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1],\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            h = tf.add( tf.matmul(h, w),b)\n",
    "            \n",
    "#           L2  \n",
    "            self.regularizer = self.regularizer + tf.nn.l2_loss(w)\n",
    "            \n",
    "            self.logit = h + dx_wsum\n",
    "    \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y) \\\n",
    "                                  + self.L2*self.regularizer)\n",
    "        self.optimizer = \\\n",
    "        tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#       tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                       feed_dict={self.dx:dx_batch, \\\n",
    "                                  self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                  self.lr:lr,\\\n",
    "                                  self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "    \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                  self.cx:cx_test, self.y:y_test,\\\n",
    "                                                  self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        return sess.run( [self.tmpshape], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "para_n_epoch = 800\n",
    "\n",
    "# tunable parameters\n",
    "para_n_hidden_list = [ 16,8 ]\n",
    "para_batch_size = 128\n",
    "para_lr=0.001\n",
    "para_keep_prob=0.9\n",
    "para_l2=0.1\n",
    "\n",
    "# fixed parameters\n",
    "para_n_class = 8\n",
    "para_n_disc = 80\n",
    "para_n_conti = 106\n",
    "para_n_disc_voca = 8\n",
    "\n",
    "tot_cnt= np.shape(cxtrain)[0]\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    wide_mlp_clf = wide_mlp_demo( para_n_conti, para_n_disc, para_n_class, \\\n",
    "                        para_n_disc_voca, sess, para_n_hidden_list,\\\n",
    "                        para_lr, para_l2 )\n",
    "    wide_mlp_clf.train_ini()\n",
    "    wide_mlp_clf.inference_ini()\n",
    "    \n",
    "    para_cur_lr=para_lr\n",
    "    total_batch = int(tot_cnt/para_batch_size)\n",
    "\n",
    "    tot_idx=range(tot_cnt)\n",
    "\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc=0.0\n",
    "        \n",
    "#       shuffle traning dataset each epoch  \n",
    "        np.random.shuffle(tot_idx)\n",
    "   \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = tot_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_cx = cxtrain[ batch_idx ]\n",
    "            batch_dx = dxtrain[ batch_idx ]\n",
    "            batch_y  =  ytrain[ batch_idx ]\n",
    "            \n",
    "            tmpc += wide_mlp_clf.train_batch( batch_dx, batch_cx, batch_y, para_keep_prob,\\\n",
    "                                       para_cur_lr)\n",
    "        \n",
    "#       learning rate decaying\n",
    "        para_cur_lr = para_cur_lr*0.98\n",
    "        \n",
    "        tmp_test_acc = wide_mlp_clf.inference(dxtest, cxtest, ytest, para_keep_prob) \n",
    "        tmp_train_acc = wide_mlp_clf.inference(dxtrain, cxtrain, ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wide and interaction demoMLP\n",
    "## without embedding\n",
    "class wide_mlp_demo():\n",
    "    \n",
    "    def __init__(self, n_conti, n_disc, n_class,\\\n",
    "                 n_disc_voca, session, n_hidden_list, lr, l2):\n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "                \n",
    "        self.N_CLASS = n_class\n",
    "        self.N_CONTI = n_conti\n",
    "        self.N_DISC = n_disc\n",
    "        self.N_DISC_VOCA = n_disc_voca\n",
    "        self.L2 = l2\n",
    "        \n",
    "        self.N_HIDDEN_LAYERS = len(n_hidden_list) \n",
    "        self.sess=session\n",
    "        \n",
    "        self.dx = tf.placeholder(tf.int32, [None, self.N_DISC])\n",
    "        self.cx = tf.placeholder(tf.float32, [None, self.N_CONTI])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.N_CLASS])\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "#       linnear part on categorical features\n",
    "        self.dx_trans = tf.transpose( self.dx, [1,0] )\n",
    "        \n",
    "        for i in range(self.N_DISC):\n",
    "            \n",
    "            with tf.variable_scope(\"disc\"+str(i)):\n",
    "                w= tf.Variable(\\\n",
    "                        tf.random_uniform([self.N_DISC_VOCA, self.N_CLASS],-1.0, 1.0) )\n",
    "#                         tf.random_normal([self.N_DISC_VOCA, self.N_CLASS],\\\n",
    "#                         stddev=math.sqrt(2.0/float(self.N_DISC_VOCA))) )\n",
    "                if i==0:\n",
    "                    dx_wsum = tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "                else:\n",
    "                    dx_wsum = dx_wsum + tf.nn.embedding_lookup( w, self.dx_trans[i] )\n",
    "        \n",
    "        with tf.variable_scope(\"wide\"):\n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))            \n",
    "            dx_wsum = tf.add(dx_wsum, b)\n",
    "            \n",
    "            \n",
    "#       interaction of categorical features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----unit test---------------------------------------------------------\n",
    "#         print self.N_TOTAL\n",
    "#         self.tmpshape= tf.shape(x_concate)\n",
    "        \n",
    "#         self.tmpshape1= tf.shape(tmp_embeded[0])\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# #     hidden layers\n",
    "        with tf.variable_scope(\"h0\"):\n",
    "            w= tf.Variable(tf.random_normal([self.N_CONTI, n_hidden_list[0]],\\\n",
    "                        stddev=math.sqrt(2.0/float(self.N_CONTI)))) \n",
    "            b= tf.Variable(tf.zeros( [ n_hidden_list[0] ] ))\n",
    "            h = tf.nn.relu( tf.add( tf.matmul(self.cx, w),b) )\n",
    "        \n",
    "                \n",
    "        for i in range(1, self.N_HIDDEN_LAYERS):\n",
    "            \n",
    "            with tf.variable_scope(\"layer\"+str(i)):\n",
    "                w= tf.Variable(tf.random_normal([n_hidden_list[i-1],n_hidden_list[i]],\\\n",
    "                                stddev=math.sqrt(2.0/float(n_hidden_list[i-1])))) \n",
    "                b= tf.Variable(tf.zeros( [n_hidden_list[i]] ))\n",
    "                h = tf.nn.relu( tf.add( tf.matmul(h, w),b) )\n",
    "        \n",
    "#       Regularization\n",
    "#       dropout\n",
    "#         h = tf.nn.dropout(h, self.keep_prob)\n",
    "#       L2\n",
    "        self.regularizer = tf.nn.l2_loss(w)\n",
    "        \n",
    "#       output layer  \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w= tf.Variable(tf.random_normal([n_hidden_list[self.N_HIDDEN_LAYERS-1],\\\n",
    "                                             self.N_CLASS],\\\n",
    "            stddev=math.sqrt(2.0/float(n_hidden_list[self.N_HIDDEN_LAYERS-1])))) \n",
    "            b= tf.Variable(tf.zeros( [ self.N_CLASS ] ))\n",
    "            h = tf.add( tf.matmul(h, w),b)\n",
    "            \n",
    "#           L2  \n",
    "            self.regularizer = self.regularizer + tf.nn.l2_loss(w)\n",
    "            \n",
    "            self.logit = h + dx_wsum\n",
    "    \n",
    "#   initialize loss and optimization operations for training\n",
    "    def train_ini(self):\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                                                    logits=self.logit, labels=self.y) \\\n",
    "                                  + self.L2*self.regularizer)\n",
    "        self.optimizer = \\\n",
    "        tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#       tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "#   training on batch of data\n",
    "    def train_batch(self, dx_batch,cx_batch, y_batch, keep_prob, lr):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer,self.cost],\\\n",
    "                       feed_dict={self.dx:dx_batch, \\\n",
    "                                  self.cx:cx_batch, self.y:y_batch,\\\n",
    "                                  self.lr:lr,\\\n",
    "                                  self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "        \n",
    "        return c\n",
    "    \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit,1), tf.argmax(self.y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, \"float\"))\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, dx_test, cx_test, y_test, keep_prob):\n",
    "        return sess.run([self.accuracy], feed_dict={self.dx:dx_test,\\\n",
    "                                                  self.cx:cx_test, self.y:y_test,\\\n",
    "                                                  self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "#   unit_test\n",
    "    def test(self, dx_test, cx_test, y_test ):\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        return sess.run( [self.tmpshape], \\\n",
    "                 feed_dict={self.dx:dx_test, self.cx:cx_test, self.y:y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wide, embedding, interaction \n",
    "\n",
    "\n",
    "\n",
    "# ordinal regression enhancment \n",
    "class mlp_demo_or():\n",
    "    \n",
    "# feature interaction + ordinal regression enhancment \n",
    "class mlp_demo_or_fi():"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
